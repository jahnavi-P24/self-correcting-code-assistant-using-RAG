{"content":"def checkCommonSvnOptions(options):\n  \"\"\"Checks a common subset of command-line options.\n\n  Multiple scripts accept a subset of common command-line options.  This\n  function does some sanity checks on these flags.  These checks are collected\n  here because they were being duplicated in multiple scripts.\n\n  Args:\n    options: OptionParser.parse_args() options instance to check\n\n  Returns:\n    list of error message strings, or an empty list if no errors\n  \"\"\"\n  errors = []\n\n  if not options.repo:\n    errors.extend(\n        ['--repo must be supplied or have a settings file default'])\n\n  if not options.wc:\n    errors.extend(\n        ['--wc must be supplied or have a settings file default'])\n\n  if not options.branch:\n    if not options.user:\n      errors.extend(\n          ['at least one of --branch or --user must be supplied'])\n\n  return errors","sha1":"7a6be9e134a83b158985af0789cad06ff4b9d5d9","id":393625}
{"content":"from datetime import datetime\n\n\ndef add_date_to_filename(filename):\n    \"\"\"\n    Add current date to a filename.\n\n    Args:\n        filename (str)\n    Returns:\n        new_filename (str):\n            filename with added date.\n    \"\"\"\n    now = datetime.now()\n    # dd\/mm\/YY H:M:S\n    dt_string = now.strftime(\"%d-%m-%Y_%H:%M\")\n    new_filename = '_'.join([filename, dt_string])\n\n    return new_filename","sha1":"230af86ef7d3d5b8d13d42363c7222f7998bed58","id":561174}
{"content":"from typing import Callable\nfrom typing import Any\nimport inspect\n\n\ndef try_bind(func: Callable, *args: Any, **kwargs: Any) -> bool:\n    \"\"\"Try binding args & kwargs to a given func.\"\"\"\n    try:\n        inspect.signature(func).bind(*args, **kwargs)\n    except TypeError:\n        return False\n    else:\n        return True","sha1":"98ff56aabada2f56f53967e339465104678b1bfb","id":191333}
{"content":"def line() -> str:\n    \"\"\"\n    Add line md style.\n\n    :return: Formatted line in md style.\n    :rtype: str\n    \"\"\"\n    return f\"---\"","sha1":"c406a2fa5ea8452b10c3826bc7ac24674ca2cdf5","id":59916}
{"content":"def recursively_split_version_string(input_version: str, output_version: list = []):\n    \"\"\"\n    Splits a version\/tag string into a list with integers and strings\n        i.e. \"8.0.0.RC10\" --> [8, '.', 0, '.', 0, '.RC', 10]\n    Input:\n        input_version (str): a version or tag i.e. \"8.0.0.RC10\"\n        output_version (list): an empty list, which will be filled iteratively\n    Returns:\n        list: the version\/tag string in a list with integers and strings i.e. [8, '.', 0, '.', 0, '.RC', 10]\n    \"\"\"\n    if type(input_version) != str:\n        raise TypeError(\n            \"The provided version should be a str data type but is of type {}.\".format(\n                type(input_version)\n            )\n        )\n\n    # when the part to split is only digits or no digits at all, the process is finished\n    if (\n        input_version.isdigit()\n        or any(char.isdigit() for char in input_version) == False\n    ):\n        version = output_version + [input_version]\n        return [int(segment) if segment.isdigit() else segment for segment in version]\n\n    # otherwise check until what position it is a digit (since we want to keep i.e. a multiple digits number as one integer)\n    pos = 0\n    while (\n        input_version[pos].isdigit() == input_version[pos + 1].isdigit()\n        and pos != len(input_version) - 2\n    ):  #\n        pos += 1\n\n    return recursively_split_version_string(\n        input_version[pos + 1 :], output_version + [input_version[: pos + 1]]\n    )","sha1":"b5e8be1d88d5113591e8199bbb39c89f803a20ea","id":22640}
{"content":"def flag_invalid_entry(row):\n    \"\"\"Flag entries to be filtered\n   \n    Args:\n        row (pandas.Series)\n    Returns:\n        filtered (str): '-' for valid entrr, otherwise invalid.\n\n    Example:\n        \n       Case 1:\n            chr  pos  rescued         ref_count alt_count\n            chrN 123  by_equivalence  30        1\n         ->      \n            chr  pos  rescued         ref_count alt_count filtered\n            chrN 123  by_equivalence  30        1         lt2count\n\n       Case 2:\n            chr  pos  rescued ref_count alt_count\n            chrN 123  - \n       ->\n            chr  pos  rescued ref_count alt_count filtered\n            chrN 123  -                           notfound\n            \n       Case 3:\n       The indel at chrN:123 was rescued by the nearest indel chrN:125.\n            chr  pos  rescued               ref_count  alt_count\n            chrN 123  rescued_by:chrN:125\n            chrN 125                        10         5\n       \n       ->   chr  pos  rescued               ref_count  alt_count filtereed\n            chrN 123  rescued_by:chrN:125                        by_nearest\n            chrN 125                        10         5         -   \n    \"\"\"\n    filtered = \"-\"\n\n    # Case 1\n    if row[\"alt_count\"] < 2 and row[\"rescued\"] != \"by_equivalence\":\n        filtered = \"lt2count\"\n    # Case 2\n    # not rescued and not found in the bam\n    elif row[\"rescued\"] == \"-\" and row[\"alt_count\"] != row[\"alt_count\"]:\n        filtered = \"notfound\"\n    # Case 3\n    # original call that is rescued by nearest indel\n    # (they are not found as specified)\n    elif (\n        row[\"rescued\"].startswith(\"rescued_by:\")\n        and row[\"alt_count\"] != row[\"alt_count\"]\n    ):\n        filtered = \"by_nearest\"\n    else:\n        pass\n\n    return filtered","sha1":"84d867d0d7f1a2b49da50b1d259b01749c75c9f2","id":579119}
{"content":"def camel_case_to_snake_case(camel_case_string):\n    \"\"\"\n    Convert a camelCase string to a snake_case string.\n\n    Args:\n        camel_case_string (str):\n            A string using lowerCamelCaseConvention.\n\n    Returns:\n        str:\n            A string using snake_case_convention.\n    \"\"\"\n\n    # https:\/\/stackoverflow.com\/a\/44969381\n    return ''.join(['_' + c.lower() if c.isupper() else c for c in camel_case_string]).lstrip('_')","sha1":"fc05ba07de498864088211fc447bc24d33b470b3","id":502837}
{"content":"import requests\n\n\ndef get_redirect_url(link):\n    \"\"\"\n        Function to retreive which url the link is redirected to.\n        e.g. http:\/\/www.google.com\/news returns http:\/\/news.google.com\/news\/\n    \"\"\"\n    response = requests.get(link)\n    if response.history:\n        if response.url != link:\n            return response.url\n        else:\n            return link","sha1":"1a84fa585182b5b1685eda0b99ca6b5b05164fb7","id":279990}
{"content":"from typing import List\n\n\ndef argv_pars(argv: List[str]) -> int:\n    \"\"\"Parses argv, returns first encountered int or 30, if no int found\n\n    Args:\n        argv (List[str]): sys.argv\n\n    Returns:\n        int: i >= 1, or 30\n    \"\"\"\n    for i in argv:\n        try:\n            return max(int(i), 1)\n        except Exception:\n            pass\n    return 30","sha1":"3ca78f51eb2020c6a612dc0958a1ba51a4086e30","id":470132}
{"content":"def is_pattern(s):\n    \"\"\"\n    Returns *True* if the string *s* represents a pattern, i.e., if it contains characters such as\n    ``\"*\"`` or ``\"?\"``.\n    \"\"\"\n    return \"*\" in s or \"?\" in s","sha1":"d49f0d5d27b8e30e2d712a3abd835ae3451b8362","id":190897}
{"content":"def hamming_distance(b1, b2):\n    \"\"\"\n    Calculates the hamming distance in bits, between two byte arrays.\n    \"\"\"\n    # Ensure that both byte arrays are the same length.\n    if len(b1) != len(b2):\n        raise Exception('Byte arrays are not of equal length.')\n\n    # Calculate the hamming distance.\n    x = bytearray([a ^ b for a, b in zip(b1, b2)])\n    return sum(sum([1 for bit in bin(b) if bit == '1']) for b in x)","sha1":"347d2affe5e670319717130f43b5caaca16ff461","id":229277}
{"content":"def generate_param(name, location, _type, required=None, _format=None):\n    \"\"\"\n    Generates a parameter definition dynamically.\n    \"\"\"\n    param = {\n        'in': location,\n        'type': _type,\n        'name': name,\n    }\n\n    if required is not None:\n        param['required'] = required\n\n    if _format is not None:\n        param['format'] = _format\n\n    return param","sha1":"e2c884e067635cb7c4e999505a60662491400cab","id":517151}
{"content":"from pathlib import Path\n\n\ndef extract_run_name(samplesheet: Path) -> str:\n    \"\"\"\n    Retrieves the 'Experiment Name' from SampleSheet.csv\n    :param samplesheet: Path to SampleSheet.csv\n    :return: value of 'Experiment Name'\n    \"\"\"\n    with open(str(samplesheet)) as f:\n        for line in f:\n            if 'Experiment Name' in line:\n                experiment_name = line.split(',')[1].strip()\n                return experiment_name\n            elif 'Description' in line:\n                experiment_name = line.split(',')[1].strip()\n                return experiment_name\n        else:\n            raise Exception(f\"Could not find 'Experiment Name' in {samplesheet}\")","sha1":"c45678a3bbe4c0ad22b56d45702e18a983a5053b","id":90866}
{"content":"def _int_from_val(x, default=None):\n    \"\"\"\n    Returns an integer from the passed in value, unless it is\n    None or the string 'None' - in which case it returns default.\n    \"\"\"\n    if (x is not None and x != 'None' and x != ''):\n        return int(x)\n    else:\n        return default","sha1":"c98a9e3b0491de14d42f68e15ed64386715fae7d","id":276204}
{"content":"import hashlib\n\n\ndef hash_values(values):\n    \"\"\" Create an MD5 has from a list of values representable as string.\n\n    :param values: The values to hash.\n    :return: An MD5 hash computed from the string representation of the\n    supplied values used as a seed.\n    \"\"\"\n    h = hashlib.md5()\n\n    for value in values:\n        h.update(str(value).encode('utf-8'))\n\n    return h.hexdigest()","sha1":"7437ebd4f52775634c5601041a5e7403d0bea674","id":231418}
{"content":"def Floor(x, factor=10):\n    \"\"\"Rounds down to the nearest multiple of factor.\n\n    When factor=10, all numbers from 10 to 19 get floored to 10.\n    \"\"\"\n    return int(x\/factor) * factor","sha1":"8977f835f1223e9958aee9b65693cac3ca32f949","id":346889}
{"content":"from typing import Optional\nfrom datetime import datetime\n\n\ndef get_dttm(date_time: Optional[datetime] = None) -> str:\n    \"\"\"\n    Get DTTM from datetime or actual time.\n\n    Args:\n        date_time: The input datetime or None for use now()\n\n    Returns:\n        dttm - str\n    \"\"\"\n    return (date_time or datetime.now()).strftime('%Y%m%d%H%M%S')","sha1":"a81f967cf0afa49069977562d336c562ac5d3f7a","id":481194}
{"content":"def _make_constraint_name(tname, *cnames, suffix=''):\n    \"\"\"Returns a constraint name from the given components.\"\"\"\n    constraint_name = f'{tname}_' + '_'.join(cnames)\n    return constraint_name[:63 - len(suffix)] + f'_{suffix}'","sha1":"55b0be29556250ba188532b6505278201d59fb41","id":638262}
{"content":"def null_process(line):\n    \"\"\"\n    Parameter:\n        line: string\n    Function: \n        if line is empty, return \"\\n\" ; else, return itself\n    \"\"\"\n    return line if line else \"\\n\"","sha1":"9c434deb66f0a2c2c9feb80ccc0a868de65c47f3","id":259072}
{"content":"import copy\n\n\ndef isolate_and_merge_station(inv, network_id, station_id):\n    \"\"\"\n    Takes an inventory object, isolates the given station and merged them.\n\n    Merging is sometimes necessary as many files have the same station\n    multiple times.\n\n    Returns the processed inventory object. The original one will not be\n    changed.\n\n    :param inv: The inventory.\n    :type inv: :class:`~obspy.core.inventory.inventory.Inventory`\n    :param network_id: The network id.\n    :type network_id: str\n    :param station_id: The station id.\n    :type station_id: str\n    \"\"\"\n    inv = copy.deepcopy(inv.select(network=network_id, station=station_id,\n                                   keep_empty=True))\n\n    # Merge networks if necessary.\n    if len(inv.networks) != 1:\n        network = inv.networks[0]\n        for other_network in inv.networks[1:]:\n            # Merge the stations.\n            network.stations.extend(other_network.stations)\n            # Update the times if necessary.\n            if other_network.start_date is not None:\n                if network.start_date is None or \\\n                        network.start_date > other_network.start_date:\n                    network.start_date = other_network.start_date\n            # None is the \"biggest\" end_date.\n            if network.end_date is not None and other_network.end_date is \\\n                    not None:\n                if other_network.end_date > network.end_date:\n                    network.end_date = other_network.end_date\n            elif other_network.end_date is None:\n                network.end_date = None\n            # Update comments.\n            network.comments = list(\n                set(network.comments).union(set(other_network.comments)))\n            # Update the number of stations.\n            if other_network.total_number_of_stations:\n                if network.total_number_of_stations or \\\n                        network.total_number_of_stations < \\\n                        other_network.total_number_of_stations:\n                    network.total_number_of_stations = \\\n                        other_network.total_number_of_stations\n            # Update the other elements\n            network.alternate_code = (network.alternate_code or\n                                      other_network.alternate_code) or None\n            network.description = (network.description or\n                                   other_network.description) or None\n            network.historical_code = (network.historical_code or\n                                       other_network.historical_code) or None\n            network.restricted_status = network.restricted_status or \\\n                other_network.restricted_status\n        inv.networks = [network]\n\n    # Merge stations if necessary.\n    if len(inv.networks[0].stations) != 1:\n        station = inv.networks[0].stations[0]\n        for other_station in inv.networks[0].stations[1:]:\n            # Merge the channels.\n            station.channels.extend(other_station.channels)\n            # Update the times if necessary.\n            if other_station.start_date is not None:\n                if station.start_date is None or \\\n                        station.start_date > other_station.start_date:\n                    station.start_date = other_station.start_date\n            # None is the \"biggest\" end_date.\n            if station.end_date is not None and other_station.end_date is \\\n                    not None:\n                if other_station.end_date > station.end_date:\n                    station.end_date = other_station.end_date\n            elif other_station.end_date is None:\n                station.end_date = None\n            # Update comments.\n            station.comments = list(\n                set(station.comments).union(set(other_station.comments)))\n            # Update the number of channels.\n            if other_station.total_number_of_channels:\n                if station.total_number_of_channels or \\\n                        station.total_number_of_channels < \\\n                        other_station.total_number_of_channels:\n                    station.total_number_of_channels = \\\n                        other_station.total_number_of_channels\n            # Update the other elements\n            station.alternate_code = (station.alternate_code or\n                                      other_station.alternate_code) or None\n            station.description = (station.description or\n                                   other_station.description) or None\n            station.historical_code = (station.historical_code or\n                                       other_station.historical_code) or None\n            station.restricted_status = station.restricted_status or \\\n                other_station.restricted_status\n        inv.networks[0].stations = [station]\n\n    # Last but not least, remove duplicate channels. This is done on the\n    # location and channel id, and the times, nothing else.\n    unique_channels = []\n    available_channel_hashes = []\n    for channel in inv[0][0]:\n        c_hash = hash((str(channel.start_date), str(channel.end_date),\n                       channel.code, channel.location_code))\n        if c_hash in available_channel_hashes:\n            continue\n        else:\n            unique_channels.append(channel)\n            available_channel_hashes.append(c_hash)\n    inv[0][0].channels = unique_channels\n\n    # Update the selected number of stations and channels.\n    inv[0].selected_number_of_stations = 1\n    inv[0][0].selected_number_of_channels = len(inv[0][0].channels)\n\n    return inv","sha1":"ce7756535f0fe95d411639e8e824975b384cbe9c","id":88258}
{"content":"def select_in(table, x, y, w):\n    \"\"\"Given a detection table and square ROI, return detections within it.\n\n    Parameters\n    ----------\n    table : pandas DataFrame\n        A dSTORM detection table, drift corrected. (Usually in file ending in\n        ``_LDCTracked.txt``.)\n    x, y : float\n        x and y coordinates of the top left corner of a square ROI. In \u00b5m.\n    w : float\n        Width (and height) in \u00b5m of the ROI.\n\n    Returns\n    -------\n    roi : pandas DataFrame\n        The subset of detections in `table` that fell within the square ROI\n\n    Notes\n    -----\n    This function hardcodes many details that may not generalise across dSTORM\n    microscopes or even experiments, such as the field of view width and the\n    scale of coordinates in the table (nm). Do some sanity checks (see below)\n    to make sure that the coordinate translations are working for you.\n    \"\"\"\n    # convert input coordinates (in \u00b5m) to table coordinates (nm)\n    x *= 1000\n    y *= 1000\n    w *= 1000\n    # Compute subset selection in x\n    x_in = (x < table['X_COORD']) & (table['X_COORD'] < x + w)\n    # Compute subset selection in y. For some reason, y coordinates are\n    # inverted in the table compared to the image in Fiji.\n    y_in = (y < 20480 - table['Y_COORD']) & (20480 - table['Y_COORD'] < y + w)\n    # Return the subsetted table\n    return table.loc[x_in & y_in][['Y_COORD', 'X_COORD']]","sha1":"2aefac2cc4c48dfbe03cb8bb2946396f527aa401","id":465687}
{"content":"def flatten(list_of_lists: list) -> list:\n    \"\"\"Helper function to flat a list out of lists\n\n    Args:\n        list_of_lists (list): [description]\n\n    Returns:\n        list: [description]\n    \"\"\"\n    flat_list = []\n    for sublist in list_of_lists:\n        for item in sublist:\n            flat_list.append(item)\n    return flat_list","sha1":"755ae2da5edf531378891ea0a5babddcc37278db","id":595126}
{"content":"def cohens_d(xbar, mu, s):\n\t\"\"\"\n\tGet the Cohen's d for a sample.\n\n\tParameters\n\t----------\n\t> xbar: mean of the sample\n\t> mu: mean of the population\n\t> s: standard distribution of the sample\n\n\tReturns\n\t-------\n\tCohen's d, or the number of standard deviations the sample mean is away from the population mean\n\t\"\"\"\n\n\treturn (xbar - mu) \/ s","sha1":"50729646229c531e3ab0cd6eb0e0381dea0e7a1d","id":409865}
{"content":"def set_defaults(hotpotato):\n    \"\"\"\n    Set default values for keys in a dictionary of input parameters.\n\n    Parameters\n    ----------\n    hotpotato : dictionary\n         Dictionary of input parameters gathered from a configuration script\n\n    Returns\n    -------\n    hotpotato : dictionary\n        Input dictionary with keys set to default values\n    \"\"\"\n    # Default S\/N threshold for ON pointings = 8\n    if hotpotato['on_cutoff']=='':\n        hotpotato['on_cutoff'] = 7.5\n    # Default S\/N threshold for OFF pointings = 6\n    if hotpotato['off_cutoff']=='':\n        hotpotato['off_cutoff'] = 6.0\n    # Default cluster radius = 1 ms\n    if hotpotato['cluster_radius']=='':\n        hotpotato['cluster_radius'] = 1.0e-3\n    return hotpotato","sha1":"9ce2583b3873b0c9102461eec21dc43682928508","id":458016}
{"content":"def collapse_z_samples_batch(t):\n    \"\"\"Merge n_z_samples and batch_size in a single dimension.\"\"\"\n    n_z_samples, batch_size, *rest = t.shape\n    return t.contiguous().view(n_z_samples * batch_size, *rest)","sha1":"e170e8a72e629e1e70911ed7a0c9512079c6a2cf","id":230333}
{"content":"def library_is_water(res_name):\n    \"\"\"Return True if the res_name is water.\n    \"\"\"\n    assert isinstance(res_name, str)\n\n    if res_name == \"HOH\" or res_name == \"WAT\":\n        return True\n\n    return False","sha1":"f79fd78c857f02b6a6288073de38b6619487acb7","id":186051}
{"content":"def can_be_formed(histogram_word_letters, histogram_rack_letters):\n     \"\"\"Takes two dictionaries of letter frequencies,\nreturns True if histogram_word can be formed with letters of histogram_letters\"\"\"\n     for item in histogram_word_letters:\n          if histogram_word_letters[item] > histogram_rack_letters.get(item, 0):\n               return False\n     return True","sha1":"318def1bda45cc0a9e9f10d04485e163c26ea541","id":221305}
{"content":"def get_parent_path(full_path):\n    \"\"\" Return parent path for full_path\"\"\"\n    parent_path, node_name = full_path.rstrip('\/').rsplit('\/',1)  # rstrip in case is group (ending with '\/')\n    if parent_path == \"\":\n        parent_path = \"\/\"\n    return parent_path","sha1":"61f208dd1646990d09b05daf95156b6d6601a286","id":65878}
{"content":"import re\n\n\ndef remove_numbers(wordlist):\n    \"\"\"Remove numbers in the wordlist\n\n    Arg:\n        wordlist: a list of words\n\n    Return:\n        new_wordlist: a list of words without any number\n    \"\"\"\n    new_wordlist = []\n\n    for word in wordlist:\n        if not re.search('\\d+', word):\n            new_wordlist.append(word)\n\n    return list(set(new_wordlist))","sha1":"26964efe57c1c44d158e77c0d0b6aa893285f0f3","id":123347}
{"content":"def _shift_x(i, old_x, n_instances):\n    \"\"\"Shift values to avoid overlap.\"\"\"\n    return old_x + 0.4 * (0.5 + i - n_instances \/ 2) \/ n_instances","sha1":"0f42cab723917f97879f33bb5fe1ffc917eb54e3","id":560623}
{"content":"def numer(x):\n    \"\"\"Return the numerator of rational number X.\"\"\"\n    return x[0]","sha1":"fa8bf012634f2bc6da3321ed1acf4c72ededa4b4","id":379747}
{"content":"def has_two_elements_that_sum_bonus(elements, k):\n    \"\"\"\n    Bonus: This implementation is a bit uglier, but it's still O(n) space and goest over it once\n    \"\"\"\n    required = set()\n    for e in elements:\n        difference = k - e\n        if e in required:\n            return True\n        else:\n            required.add(difference)\n    return False","sha1":"577c3188239b8acc7d32f13892c6eda6fe518281","id":534264}
{"content":"def genomeToBlock(chrom, start, end, L=1000000000, blockSize=5000000, delimiter='.'):\n    \"\"\"Transform genomic to block coordinates.\n    \n    @param chrom: Chromosome\n    @param start: Start coordinate\n    @param end: End coorinate\n    @param L: Chromosome length\n    @param blockSize: Block size (Default: 5000000)\n    @returns: (block, relStart, relEnd)\n    \"\"\"\n    strand = '+'\n    if start>end:\n        strand = '-'\n        start,end = end,start\n    \n    blockStart = 1 + blockSize*int(start\/blockSize)\n    blockEnd = min(L, blockStart+blockSize-1)\n    block = '%s%s%i-%i' % (chrom, delimiter, blockStart, blockEnd)\n    \n    relStart = start % blockSize\n    relEnd = end % blockSize\n    if strand=='-':\n        relStart,relEnd = relEnd,relStart\n    \n    return block,relStart,relEnd","sha1":"99ac9c640d4ad1b45f9b2622d9d835a3b7ed7354","id":585269}
{"content":"def is_unique(string):\n    \"\"\"Determines if a string is unique.\n    \n    Args:\n        string: any string of characters.\n\n    Returns:\n        a Boolean value dependant on the uniqueness\n\n    Raises:\n        ValueError: Empty string value given as an argument\n    \"\"\"\n    temp = list()\n    if string:\n        for character in string:\n            if character in temp:\n                return False\n            temp.append(character)\n        return True\n    raise ValueError('string: value is empty')","sha1":"c0abe0c4cdefeecfa7da80f0d9d8932362077be4","id":247148}
{"content":"def none(value, nonchar=''):\n    \"\"\"Converts ``None`` to ``''``.\n\n    Similar to ``|default('', true)`` in jinja2 but more explicit.\n    \"\"\"\n    if value is None:\n        return nonchar\n    return value","sha1":"4f9b13cd8b346c7ca582d76051f65a07e173f4c6","id":255785}
{"content":"def parse_slice_inv(text):\n    \"\"\"Parse a string into a slice notation.\n\n    This function inverts the result from 'parse_slice'.\n\n    :param str text: the input string.\n\n    :return str: the slice notation.\n\n    :raise ValueError\n\n    Examples:\n\n    parse_slice_inv('[None, None]') == \":\"\n    parse_slice_inv('[1, 2]') == \"1:2\"\n    parse_slice_inv('[0, 10, 2]') == \"0:10:2\"\n    \"\"\"\n    err_msg = f\"Failed to convert '{text}' to a slice notation.\"\n\n    if len(text) > 1:\n        try:\n            parts = [None if v.strip() == 'None' else int(v)\n                     for v in text[1:-1].split(',')]\n        except ValueError:\n            raise ValueError(err_msg)\n\n        if len(parts) == 2:\n            s0 = '' if parts[0] is None else str(parts[0])\n            s1 = '' if parts[1] is None else str(parts[1])\n            return f\"{s0}:{s1}\"\n\n        if len(parts) == 3:\n            s0 = '' if parts[0] is None else str(parts[0])\n            s1 = '' if parts[1] is None else str(parts[1])\n            s2 = '' if parts[2] is None else str(parts[2])\n            return f\"{s0}:{s1}:{s2}\"\n\n    raise ValueError(err_msg)","sha1":"4da2080910c15852de676829521e6d99eb990b6a","id":334936}
{"content":"def topo_sort(items):\n    \"\"\"\n    Topological sort with locality\n    Sorts a list of (item: (dependencies)) pairs so that 1) all dependency items are listed before the parent item,\n    and 2) dependencies are listed in the given order and as close to the parent as possible.\n    Returns the sorted list of items and a list of root items.  A single root indicates a fully-connected hierarchy;\n    multiple roots indicate disconnected items or hierarchies, and no roots indicate a dependency cycle.\n    \"\"\"\n    def walk_tree(item):\n        for i in deps[item]:\n            if i not in out:\n                walk_tree(i)\n                out.append(i)\n\n    out = []\n    deps = {i[0]:i[1] for i in items}\n    roots = {i[0] for i in items} - set().union(*[i[1] for i in items])\n    for item in roots:\n        walk_tree(item)\n        out.append(item)\n    out = out if out else [i[0] for i in items]     # if cycle detected, don't sort\n    return out, roots","sha1":"0007d753e5811cfc7071fc227b7d39a3c1190121","id":198566}
{"content":"def last_common_item(xs, ys):\n    \"\"\"Search for index of last common item in two lists.\"\"\"\n    max_i = min(len(xs), len(ys)) - 1\n    for i, (x, y) in enumerate(zip(xs, ys)):\n        if x == y and (i == max_i or xs[i+1] != ys[i+1]):\n            return i\n    return -1","sha1":"76baebee425dda29c08af32659a4418bef83e9bb","id":679702}
{"content":"def date_range(date):\n    # type: (str) -> str\n    \"\"\"Creates year range from years in the format 'YYYY - YYYY' end inclusive\"\"\"\n    start, end = date.split(\" - \")\n    return \"+\".join(str(x) for x in list(range(int(start), int(end) + 1)))","sha1":"82f1bc3799fe660584983191f380658fe8596861","id":446841}
{"content":"def mod_abs_diff(a, b, base):\n    \"\"\"Shortest distance between `a` and `b` in the modular integers base `base`.\n  \n    The smallest distance between a and b is returned.\n    Example: mod_abs_diff(1, 99, 100) ==> 2. It is not 98.\n  \n    mod_abs_diff is symmetric, i.e. `a` and `b` are interchangeable.\n  \n    Args:\n      a: First argument. An int.\n      b: Seconds argument. An int.\n      base: The modulo base. A positive int.\n  \n    Returns:\n      Shortest distance.\n    \"\"\"\n    diff = abs(a - b)\n    if diff >= base:\n        diff %= base\n    return min(diff, (-diff) + base)","sha1":"db447eed40ae3e6a0424ba79d2b3756c7932868a","id":570157}
{"content":"def bgr_to_rgb(image):\n  \"\"\"Swap blue and red channel in an image.\"\"\"\n  image = image.copy()\n  tmp = image[..., 0].copy()\n  image[..., 0] = image[..., 2]\n  image[..., 2] = tmp\n  return image","sha1":"1e056fc5bf4b4c77abd9b183b3261675faa5d3b6","id":579564}
{"content":"def key2int(key, dic):\n    \"\"\" Gets integer entry from dict and returns None if the key does not exist or if there is a ValueError. \"\"\"\n    value=None\n    if key in dic.keys():\n        try:\n            value=int(dic[key])\n        except ValueError:\n            value=None\n    return value","sha1":"a999b390dcf77d1af39bfe91c03a4835c8c20ecb","id":487190}
{"content":"from typing import List\n\n\ndef create_readable_data(data: List[int]) -> List[list]:\n    \"\"\"Create a program readable sudoku puzzle given a list of numbers\n\n    === Attributes ===\n    data: a list of numbers containing sudoku puzzle\n\n    === Returns ===\n    a readable sudoku puzzle\n    \"\"\"\n    result = [[data[col + 9 * row] for col in range(9)]\n              for row in range(9)]\n    return result","sha1":"152fe696ac81f5822ed4d2a76862a623959bb8a2","id":627052}
{"content":"import re\n\n\ndef extract_citations(tree):\n    \"\"\"\n    Extract number of citations from a given eutils XML tree.\n\n    Parameters\n    ----------\n    tree: Element\n        An lxml Element parsed from eutil website\n\n    Return\n    ------\n    n_citations: int\n        Number of citations that an article get until parsed date. If no citations found, return 0\n    \"\"\"\n    citations_text = tree.xpath('\/\/form\/h2[@class=\"head\"]\/text()')[0]\n    n_citations = re.sub(\"Is Cited by the Following \", \"\", citations_text).split(\" \")[0]\n    try:\n        n_citations = int(n_citations)\n    except:\n        n_citations = 0\n    return n_citations","sha1":"641dbed59fbfa47f8fac66cd615032b92e9d6014","id":110781}
{"content":"def move(space: int, d: int) -> int:\n    \"\"\"For parm current space and die roll, return the new space the pawn will land on.\"\"\"\n    return (space - 1 + d) % 10 + 1","sha1":"f5381b7de5fa96fb39d28a30656862d7b26d621a","id":606036}
{"content":"def _to_node_name(tensor_name: str) -> str:\n    \"\"\"Remove port from tensor name to give node name\"\"\"\n    return tensor_name.split(':')[0]","sha1":"8e54f13f9b05abcc9186d450e17526e6a83175ac","id":341498}
{"content":"def avg(nums, strict=True):\n    \"\"\"\n    Compute the average of the given list of numbers\n\n    Arguments:\n        nums {int|float} -- numbers\n\n    Keyword Arguments:\n        strict {bool} -- if True, throw an error for empty list,\n        if false return 0 (default: {True})\n\n    Raises:\n        e: ZeroDivisionError if list is empty\n\n    Returns:\n        float -- average of list\n    \"\"\"\n    try:\n        return sum(nums) \/ len(nums)\n    except ZeroDivisionError as e:\n        if strict:\n            raise e\n        return 0.0","sha1":"269ff075cc1de3e02b2124b8b21b766b3c3b300f","id":313810}
{"content":"def table_exists(db_conn, table_name):\n    \"\"\"\n    Checks if a table matching table_name exist in the database\n    \"\"\"\n    cur = db_conn.cursor()\n    tn = (table_name,)\n    cur.execute(\"select name from sqlite_master where type='table' and name=?\", tn)\n    result = cur.fetchone()\n    if(result):\n        return True\n    else:\n        return False","sha1":"ffe60c445a03530910084d01a7a488e7229bda0b","id":10847}
{"content":"def duplicates_checker(source, new):\n    \"\"\"\n    Checking for duplicates in existing list of contacts\n    \"\"\"\n    for item in source:\n        if new == item['phone']:\n            return False\n    return True","sha1":"5a7c7315ef3d9655369d75109b9073b99aea9486","id":674648}
{"content":"def rename_list_to_dict(rlist):\n    \"\"\"\n    Helper for main to parse args for rename operator.  The args are\n    assumed to be a pair of strings separated by a \":\".  These are\n    parsed into a dict that is returned with the old document key to\n    be replaced as the (returned) dict key and the value of the return\n    being set as the string defining the new key.\n\n    \"\"\"\n    result = dict()\n    for val in rlist:\n        pair = val.split(\":\")\n        if len(pair) != 2:\n            print(\"Cannot parse pair defined as \", val)\n            print(\"-r args are expected to be pairs of keys strings with a : separator\")\n            print(\"Type dbclean --help for usage details\")\n            exit(-1)\n        result[pair[0]] = pair[1]\n    return result","sha1":"a22b0931580488b9b3e8457eeb49a8509695e7fd","id":625737}
{"content":"def get_catalog_record_preferred_identifier(cr):\n    \"\"\"Get preferred identifier for a catalog record.\n\n    Args:\n        cr (dict): A catalog record.\n\n    Returns:\n        str: The preferred identifier of e dataset. If not found then ''.\n\n    \"\"\"\n    return cr.get('research_dataset', {}).get('preferred_identifier', '')","sha1":"c3ab1fe64c07b63760829124769fb3e39519c083","id":96615}
{"content":"def redcapdemo_url() -> str:\n    \"\"\"API url for redcapdemo testing site\"\"\"\n    return \"https:\/\/redcapdemo.vanderbilt.edu\/api\/\"","sha1":"b8ed52181bb9eab76ac92994d8c074b77ca32049","id":555076}
{"content":"import re\n\n\ndef mmyy_valid_date(date_str):\n    \"\"\"\n    Check if a date string is a valid date. Expected format is mmyy.\n    :param date_str: a date string\n    :return: date_str or None\n    \"\"\"\n    check = re.match(\"^(0[1-9]|1[0-2])\\\/?([0-9]{2})$\", date_str)\n    if check:\n        return date_str\n    else:\n        return None","sha1":"a915b9ef59f3e5d2e6b8b97ef033c81a2e34ff26","id":510576}
{"content":"import click\n\n\ndef select(choices, prompt=\"Please choose one\", default=0, required=True):\n    \"\"\" Let the user pick one of several choices.\n\n\n    :param choices:     Available choices along with their description\n    :type choices:      iterable of (object, str) tuples\n    :param default:     Index of default choice\n    :type default:      int\n    :param required:    If true, `None` can be returned\n    :returns:           The object the user picked or None.\n    \"\"\"\n    choices = list(choices)\n    for idx, choice in enumerate(choices):\n        _, choice_label = choice\n        if '\\x1b' not in choice_label:\n            choice_label = click.style(choice_label, fg='blue')\n        click.echo(\n            u\"{key} {description}\".format(\n                key=click.style(u\"[{}]\".format(idx), fg='green'),\n                description=choice_label))\n    while True:\n        choice_idx = click.prompt(prompt, default=default, type=int, err=True)\n        cutoff = -1 if not required else 0\n        if choice_idx < cutoff or choice_idx >= len(choices):\n            click.echo(\n                \"Value must be between {} and {}!\"\n                .format(cutoff, len(choices)-1), err=True)\n        elif choice_idx == -1:\n            return None\n        else:\n            return choices[choice_idx][0]","sha1":"7721448051cfd6d299c74e3345be5a13d3921d85","id":99867}
{"content":"import string\n\n\ndef build_cypher_map(keyword, decrypt):\n    \"\"\"\n    Builds the cypher substituition map that will be used for the encryption\n    and decryption.\n\n    :param str keyword: Cypher keyword\n    :param bool decrypt: If True, build the cypher map for decryption\n    :rtype: dict\n    :return: dictionary with the cypher substituitions\n    \"\"\"\n\n    # Build a list of uppercase letters, making it easier to loop and to\n    # generate the cypher dictionary\n    alphabet = list(string.ascii_uppercase)\n\n    # This cypher works with uppercase only\n    keyword = keyword.upper()\n\n    # Remove duplicated letters from the keyword\n    sequence = sorted(set(keyword), key=keyword.index)\n\n    # Append the other letters of the alphabet in reverse order\n    sequence.extend([c for c in reversed(alphabet) if c not in sequence])\n\n    if decrypt:\n        return dict(zip(sequence, alphabet))\n    else:\n        return dict(zip(alphabet, sequence))","sha1":"b7f88b6ae2bc124fea1607aba79d250a46f1d35c","id":320766}
{"content":"def location(latitude, longitude, elevation, zone):\n    \"\"\"Return a location data structure.\"\"\"\n    return [latitude, longitude, elevation, zone]","sha1":"0eb3cf07ddece387be545913e84d396cf2693494","id":234693}
{"content":"def to_psql_array(lst):\n    \"\"\"\n    Convert lists to psql friendly format.\n    \"\"\"\n    return \"{\" + \",\".join(map('{}'.format, lst)) + \"}\"","sha1":"503e911ed502c2a3611c312bcc78f8d404cbd971","id":159914}
{"content":"import torch\n\n\ndef one_hot_encoding(input):\n    \"\"\"\n    One-hot encoder\n    \n    Inputs:\n        - input : 1D tensor containing labels (N,)\n    Outputs:\n        - output: one-hot encoded tensor (N, C)\n        - classes: all unique class in order (C,)\n    \"\"\"\n    if len(input.shape)>1 and input.shape[1]>1:\n        raise ValueError(\"Tensor to be encoded, should have only one dimension or the second dimension should have size of one!\")\n    classes = input.unique()\n    N = input.shape[0]\n    C = classes.shape[0]\n    output = torch.zeros(N,C).long()\n    output[torch.arange(N), input] = 1\n    return output, classes","sha1":"0b774ca9d5c310ca0c8b7595838937b476541bd5","id":564343}
{"content":"def insert_np(n_array, target, start = 0):\n    \"\"\"\n    Inserts NumPy 1d array into target 1d array from starting position, treats out of bounds correctly.\n    \"\"\"\n    if start > target.shape[0]:\n        return\n\n    # Target start\/end\n    start_t = start\n    end_t = start + n_array.shape[0]\n\n    start_i = 0\n    if start < 0:\n        start_i = -start\n        start_t = 0\n\n    if start_i > n_array.shape[0]:\n        return\n\n    end_i = n_array.shape[0]\n    if end_t > target.shape[0]:\n        end_i = n_array.shape[0] - (end_t - target.shape[0])\n        end_t = target.shape[0]\n\n    if end_i <= 0:\n        return\n\n    target[start_t : end_t] = n_array[start_i : end_i]\n\n    return target","sha1":"839ea9cbae5317013b15d2be8b3fc476887815c3","id":189070}
{"content":"def convert_str_version_number(version_str):\n    \"\"\"\n    Convert the version number as a integer for easy comparisons\n\n    :param version_str: str of the version number, e.g. '0.33'\n\n    :returns: tuple of ints representing the version str\n    \"\"\"\n\n    version_numbers = version_str.split('.')\n\n    if len(version_numbers) != 2:\n        raise ValueError(f\"Version number is malformed: '{version_str}'\")\n\n    return tuple(int(part) for part in version_numbers)","sha1":"b550b7d07d9b226800de261f792bf0995ac21738","id":7265}
{"content":"def truncate_string_to_length(string: str, length: int) -> str:\n    \"\"\" Truncate a string to make sure its length not exceeding a given length. \"\"\"\n\n    if len(string) <= length:\n        return string\n    half_length = int(0.5 * length) - 1\n    head = string[:half_length]\n    tail = string[-half_length:]\n    return f\"{head}{'.' * (length - 2 * half_length)}{tail}\"","sha1":"67e5d1bbe7cd7aa6421fff647c6842af19faabc4","id":22426}
{"content":"def locate_all_occurrence(l, e):\n    \"\"\"\n    Return indices of all element occurrences in given list\n\n    :param l: given list\n    :type l: list\n    :param e: element to locate\n    :return: indices of all occurrences\n    :rtype: list\n    \"\"\"\n    return [i for i, x in enumerate(l) if x == e]","sha1":"95b662f359bd94baf68ac86450d94298dd6b366d","id":636}
{"content":"def is_natural_language(line):\n    \"\"\"Determine if a line is likely to be actual natural language text, as\n    opposed to, e.g., LaTeX formulas or tables.  Pretty crude heuristic, but as\n    long as it filters out most of the bad stuff it's okay, I guess.\"\"\"\n    line = line.strip()\n    if len(line) < 5:\n        return False\n    if \"\\\\\" in line or line[0] == \"$\" or line[-1] == \"$\":\n        # probably contains a LaTeX formula; skip\n        return False\n    if line.count(\" \") < 2:\n        # maybe a headline, or a fragment\n        return False\n    if \"   \" in line or not line.replace(\"-\", \"\").strip():\n        # table?\n        return False\n    return True","sha1":"563ced6c4fef520d1c4e0ca9010f053acaa107cc","id":239271}
{"content":"def get_regions(chr_pos):\n    \"\"\"\n    Modified get_regions() to work with reference names that include an underscore\n    Does the same thing as the usual get_regions from nanoRMS except, preserves the\n    actual reference name, instead of only the first element after _ split\n\n    Parameters\n    ----------\n    chr_pos : list\n        List of character positions in the format \"Reference-name_position\"\n\n    Returns\n    -------\n    regions : list of tuples\n        Each tuple corresponds to its position in the format required by trace_df()\n\n    \"\"\"\n\n    regions = [\n        (\"_\".join(cp.split(\"_\")[:-1]), int(cp.split(\"_\")[-1]), \"_\") for cp in chr_pos\n    ]\n    return regions","sha1":"71ff0150cb9658aee8d1a11dccecf9fab54d364c","id":341272}
{"content":"def is_better_sol(best_f, best_K, sol_f, sol_K, minimize_K):\n    \"\"\"Compares a solution against the current best and returns True if the\n    solution is actually better accordint to minimize_K, which sets the primary\n    optimization target (True=number of vehicles, False=total cost).\"\"\"\n    \n    if sol_f is None or sol_K is None:\n        return False\n    if best_f is None or best_K is None:\n        return True\n    elif minimize_K:\n        return (sol_K<best_K) or (sol_K==best_K and sol_f<best_f)\n    else:\n        return sol_f<best_f","sha1":"15ab7dd0294226870e476731066130253ae2986a","id":672121}
{"content":"def z2lin(array):\n    \"\"\"calculate linear values from a array of dBs\"\"\"\n    return 10**(array\/10.)","sha1":"e0ed221666398c9ca8488fd20f5e3b0711ad6a7c","id":40288}
{"content":"def _read_file(filename):\n    \"\"\"\n    Read a file and return the bytestring\n    Args:\n        filename: The location of the file\n    Return:\n        The bytestring of the file\n    \"\"\"\n    f = open(filename, 'rb')\n    data = f.read()\n    f.close()\n    return data","sha1":"d30735a80f2eff2c413d5063ce219e28feb773a0","id":601820}
{"content":"def split_permission(app, perm):\n    \"\"\"split permission string into permission and model\"\"\"\n    permission_name, *model = perm.split('_')\n    # handle models that have underscores\n    if len(model) > 1:  # pragma: no cover\n        app += '_' + '_'.join(model[:-1])\n        perm = permission_name + '_' + model[-1:][0]\n    model = model[-1:][0]\n    return perm, model","sha1":"d8a6e2cb0085383842dc2f5de3d95d955bbf198e","id":474781}
{"content":"import re\n\n\ndef natural_sort(s):\n    \"\"\" Sort the given list in the way that humans expect.\"\"\"\n    # Reference: https:\/\/nedbatchelder.com\/blog\/200712\/human_sorting.html\n    def alphanum_key(s):\n        return [int(c) if c.isdigit() else c for c in re.split('([0-9]+)', s)]\n    s.sort(key=alphanum_key)\n    return s","sha1":"1cade49a800690dbaed9f05ea90744c8dc57f888","id":458308}
{"content":"async def fetch_http(session, url, **kwargs):\n    \"\"\"Uses aiohttp to make http GET requests\"\"\"\n\n    async with session.get(url, **kwargs) as response:\n        return await response.json()","sha1":"3253292fa1c967d0514c337ae32ddcd353d613cf","id":429643}
{"content":"import csv\n\n\ndef read_slcsp_zip_codes(file_path):\n    \"\"\"\n    reads zip codes in slcsp.csv\n\n    Args:\n        file_path: the path to slcsp.csv file\n\n    Returns:\n        a list of the zip codes in the file\n    \"\"\"\n    zip_codes = []\n    with open(file_path, \"r\") as zip_codes_file:\n        csv_reader = csv.DictReader(zip_codes_file, delimiter=\",\")\n        for line in csv_reader:\n            zip_codes.append(line[\"zipcode\"])\n    return zip_codes","sha1":"8fb7955d80f83ba2c62de31525a01be10d0ff316","id":449454}
{"content":"import requests\n\n\ndef get_mc_uuid(username):\n    \"\"\"Gets the Minecraft UUID for a username\"\"\"\n    url = f\"https:\/\/api.mojang.com\/users\/profiles\/minecraft\/{username}\"\n    res = requests.get(url)\n    if res.status_code == 204:\n        raise ValueError(\"Users must have a valid MC username\")\n    else:\n        return res.json().get(\"id\")","sha1":"fceeb1d9eb096cd3e29f74d389c7c851422ec022","id":600}
{"content":"def generate_big_rules(L, support_data, min_conf):\n    \"\"\"\n    Generate big rules from frequent itemsets.\n    Args:\n        L: The list of Lk.\n        support_data: A dictionary. The key is frequent itemset and the value is support.\n        min_conf: Minimal confidence.\n    Returns:\n        big_rule_list: A list which contains all big rules. Each big rule is represented\n                       as a 3-tuple.\n    \"\"\"\n    big_rule_list = []\n    sub_set_list = []\n    for i in range(0, len(L)):\n        for freq_set in L[i]:\n            for sub_set in sub_set_list:\n                if sub_set.issubset(freq_set):\n                    conf = support_data[freq_set] \/ support_data[freq_set - sub_set]\n                    big_rule = (freq_set - sub_set, sub_set, conf)\n                    if conf >= min_conf and big_rule not in big_rule_list:\n                        # print freq_set-sub_set, \" => \", sub_set, \"conf: \", conf\n                        big_rule_list.append(big_rule)\n            sub_set_list.append(freq_set)\n    return big_rule_list","sha1":"1cab48d6d1f65b041ecf85134b86d7bbb1fac040","id":457465}
{"content":"from typing import Optional\nfrom typing import Dict\n\n\ndef compare_labels(\n    ref: str, test: str, silence_phone: str, mapping: Optional[Dict[str, str]] = None\n) -> int:\n    \"\"\"\n\n    Parameters\n    ----------\n    ref: str\n    test: str\n    mapping: Optional[dict[str, str]]\n\n    Returns\n    -------\n    int\n        0 if labels match or they're in mapping, 2 otherwise\n    \"\"\"\n    if ref == test:\n        return 0\n    if ref == silence_phone or test == silence_phone:\n        return 10\n    if mapping is not None and test in mapping:\n        if isinstance(mapping[test], str):\n            if mapping[test] == ref:\n                return 0\n        elif ref in mapping[test]:\n            return 0\n    ref = ref.lower()\n    test = test.lower()\n    if ref == test:\n        return 0\n    return 2","sha1":"160949694c336a4d17248f86958146d0541ae3db","id":139007}
{"content":"import re\n\n\ndef is_number_regex(s):\n    \"\"\" Returns True is string is a number. \"\"\"\n    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n        return s.isdigit()\n    return True","sha1":"fe1084bfcfc78594755f188d3033f92ca4ce5eae","id":674883}
{"content":"def ssl_scan(url):\n    \"\"\"\n    Method which change url to ssl url (https).\n\n    :param url: Main url of scanned Website.\n    :return: Link with changed Url (http to https)\n    \"\"\"\n    if url.startswith('http:\/\/'):\n        return url.replace('http:\/\/', 'https:\/\/')\n    return 'https:\/\/' + url","sha1":"5a3f767781a833bc25284cfad07ad4440cb19dc6","id":622375}
{"content":"def get_download_url_path_for_minecraft_lib(descriptor):\n    \"\"\"\n    Gets the URL path for a library based on it's name\n    :param descriptor: string, e.g. \"com.typesafe.akka:akka-actor_2.11:2.3.3\"\n    :return: string\n    \"\"\"\n    ext = \"jar\"\n\n    pts = descriptor.split(\":\")\n    domain = pts[0]\n    name = pts[1]\n\n    last = len(pts) - 1\n    if \"@\" in pts[last]:\n        idx = pts[last].index(\"@\")\n        ext = pts[last][idx+1:]\n        pts[last] = pts[last][0:idx+1]\n\n    version = pts[2]\n\n    classifier = None\n    if len(pts) > 3:\n        classifier = pts[3]\n\n    file = name + \"-\" + version\n\n    if classifier is not None:\n        file += \"-\" + classifier\n\n    file += \".\" + ext\n\n    path = domain.replace(\".\", \"\/\") + \"\/\" + name + \"\/\" + version + \"\/\" + file\n\n    return path","sha1":"5e7cbaafa90a1b4afaf1e545ce23dfb04e331ac1","id":396747}
{"content":"import networkx as nx\n\n\ndef significant_interactions(G, a=0.05):\n    \"\"\"Return a filtered graph containing only sigificanly differentiated ligands and receptor interaction.\n    \n    a: alpha, selects scores with a pvalue under alpha.\n    Return a MultiDiGraph\n    \"\"\"\n\n    G_filtered = nx.MultiDiGraph()\n    for u, v, n, d in G.edges(data=True, keys=True):\n        # Find interactions with significant p-values (pval < a)\n        # Must be significantly upregulated (zscore > 0)\n        if d[\"ligand_pval\"] < a and d[\"receptor_pval\"] < a and d[\"ligand_zscore\"] > 0 and d[\"receptor_zscore\"] > 0:\n            G_filtered.add_edge(u, v , key=n ,**d)\n            \n    G_filtered.add_nodes_from(G.nodes(data=True))\n    \n    return G_filtered","sha1":"c71caabdb4e416888326c5538e3acf60968ad73d","id":218358}
{"content":"def get_format(fmt, string):\n    \"\"\"Return a dictionary containing a format for each byte order.\"\"\"\n    return {\"big\": fmt(\">\" + string), \"little\": fmt(\"<\" + string)}","sha1":"7326e25454200945bb46db6275dd17518f88e8a4","id":303374}
{"content":"def filter_keypoints(points, shape):\n\t\t\n\t\t\"\"\" Keep only the points whose coordinates are\n        inside the dimensions of shape. \"\"\"\n\t\tmask = (points[:, 0] >= 0) & (points[:, 0] < shape[0]) &\\\n\t\t (points[:, 1] >= 0) & (points[:, 1] < shape[1])\n\t\treturn points[mask, :]","sha1":"3416653e6ed6c2cbe92d670ecb2b4b4c31518b23","id":187942}
{"content":"import pickle\n\n\ndef read_object(filename):\n    \"\"\"Convenience function for retrieving object from file using pickle.\n\n    Arguments:\n        filename {str} -- path to file\n\n    Returns:\n        object -- object in file (uses pickle)\n    \"\"\"\n\n    with open(filename, 'rb') as file:\n        obj = pickle.load(file)\n        return obj","sha1":"c6b53d5cb7260a397eb197fa84ed2b8802410bb4","id":669283}
{"content":"import random\n\n\ndef des_sky_brightness(bands=''):\n    \"\"\"\n    Sample from the distribution of single epoch sky brightness for DES\n    \"\"\"\n    # Figure 4 in https:\/\/arxiv.org\/pdf\/1801.03181.pdf\n    dist = {'g': {'VALUES': [21.016, 21.057, 21.106, 21.179, 21.228, 21.269, 21.326, \n                             21.367, 21.424, 21.465, 21.522, 21.571, 21.62, 21.677, \n                             21.717, 21.774, 21.823, 21.872, 21.921, 21.97, 22.019, \n                             22.068, 22.117, 22.174, 22.215, 22.272, 22.321, 22.378, \n                             22.427, 22.476],\n                  'WEIGHTS': [0.0, 0.0, 0.001, 0.001, 0.001, 0.001, 0.002, 0.003, \n                              0.005, 0.007, 0.009, 0.012, 0.016, 0.023, 0.034, 0.048, \n                              0.063, 0.073, 0.081, 0.093, 0.107, 0.099, 0.087, 0.076, \n                              0.061, 0.05, 0.027, 0.013, 0.005, 0.0]},\n            'r': {'VALUES': [20.16, 20.209, 20.266, 20.323, 20.372, 20.421, 20.47, \n                             20.519, 20.576, 20.625, 20.674, 20.715, 20.772, 20.821, \n                             20.87, 20.918, 20.976, 21.024, 21.073, 21.122, 21.171, \n                             21.22, 21.269, 21.326, 21.375, 21.424, 21.473, 21.522, \n                             21.571, 21.62, 21.668, 21.726],\n                  'WEIGHTS': [0.0, 0.0, 0.001, 0.001, 0.002, 0.002, 0.005, 0.008, \n                              0.011, 0.011, 0.012, 0.02, 0.023, 0.034, 0.043, 0.046, \n                              0.056, 0.07, 0.075, 0.083, 0.093, 0.095, 0.092, 0.078, \n                              0.057, 0.041, 0.024, 0.012, 0.004, 0.001, 0.0, 0.0]},\n            'i': {'VALUES': [18.921, 18.978, 19.027, 19.076, 19.125, 19.174, 19.223, \n                             19.272, 19.321, 19.378, 19.418, 19.476, 19.524, 19.573, \n                             19.622, 19.671, 19.728, 19.777, 19.826, 19.875, 19.924, \n                             19.973, 20.022, 20.071, 20.12, 20.177, 20.226, 20.274, \n                             20.323, 20.372, 20.421, 20.478, 20.527, 20.576, 20.617, \n                             20.674, 20.723, 20.772, 20.829],\n                  'WEIGHTS': [0.0, 0.0, 0.002, 0.002, 0.001, 0.002, 0.003, 0.005, \n                              0.013, 0.017, 0.018, 0.026, 0.029, 0.035, 0.036, 0.047, \n                              0.053, 0.067, 0.078, 0.084, 0.073, 0.073, 0.063, 0.05, \n                              0.045, 0.039, 0.031, 0.026, 0.021, 0.018, 0.014, 0.009, \n                              0.009, 0.003, 0.002, 0.002, 0.001, 0.0, 0.0]},\n            'z': {'VALUES': [17.715, 17.772, 17.804, 17.861, 17.918, 17.976, 18.024, \n                             18.073, 18.122, 18.171, 18.228, 18.277, 18.326, 18.375, \n                             18.424, 18.473, 18.522, 18.579, 18.628, 18.677, 18.726, \n                             18.774, 18.823, 18.872, 18.921, 18.97, 19.019, 19.076, \n                             19.125, 19.174, 19.231, 19.264, 19.329, 19.37, 19.427, \n                             19.467, 19.524, 19.573, 19.63],\n                  'WEIGHTS': [0.0, 0.0, 0.0, 0.001, 0.001, 0.004, 0.007, 0.008, \n                              0.012, 0.014, 0.015, 0.022, 0.028, 0.028, 0.033, 0.045, \n                              0.052, 0.058, 0.064, 0.073, 0.082, 0.078, 0.069, 0.059, \n                              0.051, 0.044, 0.036, 0.024, 0.019, 0.018, 0.017, 0.015, \n                              0.01, 0.005, 0.002, 0.002, 0.002, 0.001, 0.0]},\n            'Y': {'VALUES': [17.062, 17.128, 17.177, 17.226, 17.274, 17.323, 17.372, \n                             17.421, 17.47, 17.527, 17.576, 17.625, 17.674, 17.723, \n                             17.772, 17.821, 17.878, 17.927, 17.976, 18.024, 18.073, \n                             18.13, 18.179, 18.228, 18.277, 18.326, 18.375, 18.424, \n                             18.473, 18.53, 18.579, 18.628, 18.668, 18.726, 18.774, \n                             18.823, 18.88, 18.929, 18.97, 19.027, 19.076],\n                  'WEIGHTS': [0.001, 0.002, 0.002, 0.003, 0.006, 0.008, 0.011, 0.015, \n                              0.02, 0.027, 0.032, 0.041, 0.051, 0.051, 0.05, 0.05, \n                              0.056, 0.066, 0.072, 0.068, 0.056, 0.047, 0.042, 0.033, \n                              0.032, 0.029, 0.024, 0.022, 0.021, 0.02, 0.014, 0.011, \n                              0.006, 0.003, 0.002, 0.001, 0.001, 0.0, 0.002, 0.001, 0.0]}\n            }\n    return [random.choices(dist[b]['VALUES'], dist[b]['WEIGHTS'])[0] for b in bands.split(',')]","sha1":"0b5c1b4a084d640d441b7911dc68462d66a5cd1e","id":568539}
{"content":"def logout_user(_):\n    \"\"\"Log user out.\"\"\"\n    return {}, 200","sha1":"edb75ddc32f905c62600ef7706ccd060d02f466c","id":48310}
{"content":"import math\n\n\ndef calculate_percentage(num, total):\n    \"\"\"Calculate percentage.\"\"\"\n    return math.trunc(((num + 1) \/ total) * 100)","sha1":"cc927a23d2ee23721026799fe86a2af3fc9a7a85","id":362522}
{"content":"def angstroms_to_ev(array):\n    \"\"\"convert lambda array in angstroms to energy in eV\n    \n    Parameters:\n    ===========\n    array: numpy array or number in Angstroms\n    \n    Returns:\n    ========\n    numpy array or value of energy in eV\n    \"\"\"\n    return 81.787 \/ (1000. * array ** 2)","sha1":"45ff2ed7cda157d8d110499727cb68ccfed2779e","id":518430}
{"content":"def get_percentage(df, fragment_ion, weight):\n    \"\"\"Return the relative abundance of a given fragment ion.\n\n    Parameters\n    ----------\n    df : Dataframe\n        Dataframe containing the information of the fitted areas of a given fragment ion\n\n    fragment_ion : str\n        Desired fragment ion (e.g. \"y7\")\n\n    weight : str\n        Choose weather the light or heavy fragment ion should be selected\n\n    Returns\n    -------\n    float\n        Relative abundance (percentage) of the chosen fragment ion\n    \"\"\"\n    return (df[f\"fitted_area_{fragment_ion}_{weight}\"] \/ (df[f\"fitted_area_{fragment_ion}_light\"]+df[f\"fitted_area_{fragment_ion}_heavy\"])) * 100","sha1":"8158b7873c11bde62b2cd90182471f6a88fa3595","id":562213}
{"content":"def max_quantity(remain, coin, qty):\n    \"\"\"\n    Calculate max quantity of a coin type to give change\n    \"\"\"\n    max_qty = 0\n    while remain >= coin and qty - max_qty > 0:\n        remain = remain-coin\n        max_qty += 1\n    return max_qty","sha1":"a16d25f6bbfa4bf0c91b6b93a4102d57ed17cc34","id":505297}
{"content":"def build_mqtt_topic(*args):\n    \"\"\"Join topic components with a '\/' delimeters and encode as bytes\n\n    The umqtt library expects topic to be byte encoded\n\n    Arguments:\n        *args {string} -- String to be added to topic\n\n    Returns:\n        [bytearray] -- byte encoded mqtt topic\n    \"\"\"\n    topic = '\/'.join(args)\n    return topic.encode('utf-8')","sha1":"ab47c9ae48615a690a91c6543973bbd4fb08c2ae","id":177511}
{"content":"def formatTime(t):\n    \"\"\"\n    Formats time into the format yyyy\/mm\/dd.\n    \n    Args:\n        t (tuple): the original date\n        \n    Returns:\n        string: yyyy\/mm\/dd\n        \n    Examples:\n        1990\/1\/1 --> 1990\/01\/01\n    \n    \"\"\"\n    if t[1] < 10:\n        if t[2] < 10:\n            return '{0}\/0{1}\/0{2}'.format(t[0], t[1], t[2])\n        else:\n            return '{0}\/0{1}\/{2}'.format(t[0], t[1], t[2])\n    else:\n        return '{0}\/{1}\/{2}'.format(t[0], t[1], t[2])","sha1":"ebc3181d92bdcb7c08cb29695fd3b42ce7cee5d7","id":236328}
{"content":"def degree(node, row, col):\n    \"\"\"return the degree of the given node\"\"\"\n    neighbors = col[row == node]\n    return len(neighbors)","sha1":"9186f748b84ccae5d274a01c47e02d8e078ec08d","id":584841}
{"content":"import torch\n\n\ndef logbound(logx, logdelta):\n    \"\"\"Calculates log(exp(x)+exp(logdelta))\"\"\"\n    clipx = torch.clip(logx, logdelta, None)\n    boundx = clipx + torch.log(torch.exp(logx-clipx) +\n                               torch.exp(logdelta-clipx))\n    return boundx","sha1":"1e0a545f25b2da8ef2e4ed87e0b4dfa62cf3918f","id":198032}
{"content":"def factorial (n):\n    \"\"\"Factorial, computed recursively.\n\n    What is the base case?\n    How can you express n! recursively, in terms of itself?\n\n    Params: n (int): n >= 1\n    Returns: n!\n    \"\"\"\n    if n == 1:\n        return 1\n    else:\n        return n*factorial(n-1)\n\n    return n","sha1":"6ee4bdd5290fbc11ab972095a4a5f62b3dc93838","id":488697}
{"content":"def recursive_array_max(l:list):\n    \"\"\"Find the max element using recursion.\"\"\"\n    if len(l) < 2:\n        return 0 if len(l) == 0 else l[0]\n    else:\n        i = l.pop()\n        return i if i > max(l) else recursive_array_max(l)","sha1":"30ce8d9f898eb0b45a202cba5ca295b696bbbaa1","id":506963}
{"content":"def is_module_available(module_name):\n    \"\"\"Return True if Python module is available\"\"\"\n    try:\n        __import__(module_name)\n        return True\n    except ImportError:\n        return False","sha1":"527092a60534e263dd9f8e8ce43bbfcb0ba19f31","id":38355}
{"content":"def lag(series, i=1):\n    \"\"\"\n    Returns a series shifted backwards by a value. `NaN` values will be filled\n    in the beginning.\n\n    Same as a call to `series.shift(-i)`\n\n    Args:\n        series: column to shift backward.\n        i (int): number of positions to shift backward.\n    \"\"\"\n\n    shifted = series.shift(i)\n    return shifted","sha1":"c026a54b1288301c1ae757a146f36dfdf5a87c2b","id":654609}
{"content":"def parse_boolean(value):\n    \"\"\"Parses a boolean from test i.e. from \"0\", \"yes\", \"false\"\n    etc.\n    \"\"\"\n    return value.lower() in (\"1\",\"yes\",\"true\")","sha1":"094af647683fe5aaefe2d2ba6ce17409c6cd6fb4","id":417399}
{"content":"def avg(ts):\n    \"\"\"Calculates the average of the timeseries.\n\n    Args:\n        ts: A timeseries list of [time, value].\n\n    Returns:\n        Average of the timeseries.\n    \"\"\"\n    return sum([float(v[1]) for v in ts]) \/ len(ts)","sha1":"b853d70841bf064a4f01f846469d76f9110739d4","id":72386}
{"content":"import re\n\n\ndef isslug(value):\n    \"\"\"\n    Validate whether or not given value is valid slug.\n    Valid slug can contain only alphanumeric characters, hyphens and\n    underscores. If the value is a slug, this function returns ``True``, otherwise ``False``.\n\n    Examples::\n\n        >>> isslug('my-slug-2134')\n        True\n\n        >>> isslug('my.slug')\n        False\n\n    :param value: value to validate\n    \"\"\"\n    slug = re.compile(r'^[-a-zA-Z0-9_]+$')\n    return bool(slug.match(value))","sha1":"e8f79351d7ec6939344cdcb278a22e2316ed592f","id":578212}
{"content":"import torch\n\n\ndef topk_acc(y_pred:torch.Tensor, y_true:torch.Tensor, k = 5):\n    \"\"\"\n    Returns topk accuracy from multiclass classification.\n    Expect that `y_pred` as logits of size (y_true.shape[0], classes).\n    \"\"\"\n    # Get indices of top k predictions along axis 1\n    top_k_ixs = y_pred.topk(k = k, dim = 1).indices\n    acc = torch.eq(y_true.view(-1,1), top_k_ixs).sum().item() \/ y_true.shape[0]\n    return acc","sha1":"7a655a8b11a997d571d46054ff22a3eed7324587","id":500097}
{"content":"def _has_method(arg, method):\n    \"\"\"Returns true if the given object has a method with the given name.\n\n    :param arg: the object\n\n    :param method: the method name\n    :type method: string\n\n    :rtype: bool\n    \"\"\"\n    return hasattr(arg, method) and callable(getattr(arg, method))","sha1":"361e96017bc8da070ab0c240540bf623fedcbf2a","id":380106}
{"content":"def slug(s):\n    \"\"\"\n    Generates a slug of the string `s`\n    \"\"\"\n    return s.replace(\" \", \"_\")","sha1":"cf0112f6b25568029052f107ca367a99785fbeb2","id":384024}
{"content":"import functools\n\n\ndef noop_if_no_unpatch(f):\n    \"\"\"\n    A helper for PatchTestCase test methods that will no-op the test if the\n    __unpatch_func__ attribute is None\n    \"\"\"\n    @functools.wraps(f)\n    def wrapper(self, *args, **kwargs):\n        if getattr(self, '__unpatch_func__') is None:\n            return\n        return f(self, *args, **kwargs)\n    return wrapper","sha1":"b6e8829b9a04d9acdb53bfeb9b084652942eb870","id":669510}
{"content":"def mol_pubchem_id(mol):\n    \"\"\"\n    Returns the PubChem Compound ID from the molecule data.\n\n    Parameters\n    ----------\n    mol : pybel.Molecule\n        A molecule.\n\n    Returns\n    -------\n    str\n       PubChem Compound ID\n    \"\"\"\n    return mol.data['PUBCHEM_COMPOUND_CID'].strip()","sha1":"45c1f1f843012d361d49cf5b11f41912502ede74","id":560739}
{"content":"import itertools\n\n\ndef get_chemical_gene_combinations(_dict_):\n    \"\"\"\n    Parameters\n    ----------\n    _dict_ : dictionary\n        Dictionary with annotated entities. Keys: PMID, Values: another\n        dictionary with keys 'chemicals' and 'genes' and value the \n        annotation mark\n\n    Returns\n    -------\n    combinations : dictionary\n        PMIDs as keys, all possible CHEMICAL-GENE combinations are values.\n    NCOMB : int\n        DESCRIPTION.\n\n    \"\"\"\n\n    combinations = {}\n    NCOMB = 0\n    for pmid, entities in _dict_.items():\n        chem = entities['chemicals']\n        genes = entities['genes']\n        combinations[pmid] = list(itertools.product(chem, genes))\n        NCOMB = NCOMB + len(combinations[pmid] )\n    return combinations, NCOMB","sha1":"c87b842709125d1c174e89ba56ad3766bc915bce","id":314759}
{"content":"def split_path(path):\n    \"\"\"\n    Get the parent path and basename.\n    \n    >>> split_path('\/')\n    ['', '']\n    \n    >>> split_path('')\n    ['', '']\n    \n    >>> split_path('foo')\n    ['', 'foo']\n    \n    >>> split_path('\/foo')\n    ['', 'foo']\n    \n    >>> split_path('\/foo\/bar')\n    ['\/foo', 'bar']\n    \n    >>> split_path('foo\/bar')\n    ['\/foo', 'bar']\n    \"\"\"\n    \n    if not path.startswith('\/'): path = '\/' + path\n    return path.rsplit('\/', 1)","sha1":"44ca28d0877e1f24d62ed5cf246ea007e71aa27e","id":109295}
{"content":"def get_add_vswitch_port_group_spec(client_factory, vswitch_name,\n                                    port_group_name, vlan_id):\n    \"\"\"Builds the virtual switch port group add spec.\"\"\"\n    vswitch_port_group_spec = client_factory.create('ns0:HostPortGroupSpec')\n    vswitch_port_group_spec.name = port_group_name\n    vswitch_port_group_spec.vswitchName = vswitch_name\n\n    # VLAN ID of 0 means that VLAN tagging is not to be done for the network.\n    vswitch_port_group_spec.vlanId = int(vlan_id)\n\n    policy = client_factory.create('ns0:HostNetworkPolicy')\n    nicteaming = client_factory.create('ns0:HostNicTeamingPolicy')\n    nicteaming.notifySwitches = True\n    policy.nicTeaming = nicteaming\n\n    vswitch_port_group_spec.policy = policy\n    return vswitch_port_group_spec","sha1":"c49061bb0689c64edbf4af439fd7c45c05f536d9","id":305900}
{"content":"import re\n\n\ndef clean_html(raw_html):\n    \"\"\"Remove HTML tags from a text\"\"\"\n    \n    clean_regex = re.compile('<.*?>')\n    clean_text = re.sub(clean_regex, '', raw_html)\n    return clean_text\n\n\n#def process_column(column_name, dataframe, process):\n    \"\"\"Define a method to apply the same process to every row in a\n    pandas DataFrame column\"\"\"","sha1":"af05b910e5a86bcf881d0f218caf40a738bc98d7","id":642914}
{"content":"def tradeToAction(trade_enum_val):\n    \"\"\"Extract an action enum from a trade enum\"\"\"\n    return trade_enum_val \/ abs(trade_enum_val)","sha1":"0efd8ff4901d292ba7f454458b376b62b7a715ed","id":141582}
{"content":"import requests\n\n\ndef send_request(url, method='GET', params={}, data={}, files={}):\n    \"\"\"\n    Send a request with given method and data\n    \"\"\"\n\n    if method == 'GET':\n        response = requests.get(url, params=params)\n    else:\n        response = requests.post(url, params=params, data=data, files=files)\n\n    data = response.content.decode()\n    if response.status_code == 200:\n        return data\n    else:\n        print(f'*** {response.status_code} Error: {data}')\n\n    return","sha1":"0feaf54eac5fc2198a77bf446a1d56c1bc3b6654","id":373722}
{"content":"def sufficient_coverage(bed_record, min_coverage):\n    \"\"\"Return True if bed_record has sufficient coverage of supportive reads,\n    otherwise, False\"\"\"\n    cov = sum([bed_record.fmts[sample].ad for sample in bed_record.samples])\n    return cov >= min_coverage","sha1":"bc10c8bb10da2d6d14498d49242ecb963465163b","id":447650}
{"content":"def get_subject_name_from_csv(full_subject_name):\n    \"\"\"This function gets subject name from csv\n\n    Args:\n        full_subject_name (str): Full subject name with format XXX_YYY_ZZZ\n\n    Return:\n        subject_name (str): Subject name\n    \"\"\"\n    full_subject_name_list = full_subject_name.split(\"_\", 3)\n    subject_name = full_subject_name_list[1]\n    return subject_name","sha1":"8a63dd724c840e23dc008b1ac416f61721876417","id":636216}
{"content":"import uuid\n\n\ndef generate_slug(model) -> str:\n    \"\"\"Generate a unique slug\n        Args:\n            model (cls): Django Model\n        Returns:\n            slug (string)\n    \"\"\"\n    query_manager = model.objects\n    slug = uuid.uuid4().hex[:6]\n    while query_manager.filter(slug=slug).exists():\n        slug = uuid.uuid4().hex[:6]\n    return slug","sha1":"5219320b740083843a10055eb8b78409624fcebf","id":634760}
{"content":"def spiral_matrix(matrix):\n  \"\"\"Navigates a 2D array and traverses the edges in a spiral-pattern.\n\n  Args:\n    matrix; list: a 2D list to traverse.\n\n  Returns:\n    A string representation of all points traveled in the matrix in the order\n    they were traveled to.\n  \"\"\"\n  output = list()\n  rows = len(matrix[0])\n  columns = len(matrix)\n\n  # Our iterators for columns and rows respectively.\n  x = 0\n  y = 0\n\n  # Our edge coordinates.\n  edge_x = 0\n  edge_y = 0\n\n  while x < rows and y < columns:\n    # Traverses the top edge of the matrix.\n    for x in range(0 + edge_x, rows):\n      output.append(matrix[y][x])\n    rows -= 1\n\n    # Traverses down the right edge of the matrix.\n    for y in range(1 + edge_y, columns):\n      output.append(matrix[y][x])\n    columns -= 1\n\n    if edge_x < x and edge_y < y:\n      # Traverses along the bottom edge of the matrix.\n      for x in range(rows - 1, -1 + edge_x, -1):\n        output.append(matrix[y][x])\n      edge_x += 1\n    \n      # Traverses along the left edge of the matrix.\n      for y in range(columns - 1, 0 + edge_y, -1):\n        output.append(matrix[y][x])\n      edge_y += 1\n    else:\n      break\n\n  \"\"\"\n  The instructions seemed to indicate that I should return a string, so\n  I chose to use the generator notation here to perform that function. I\n  chose this vs. a list comprehension since it is a bit more space-efficient.\n  \"\"\"\n  output_strings = ' '.join((str(number) for number in output))\n  return output_strings","sha1":"013e9e9a8f47ed0d8b5631cb2e912654f85c0897","id":72837}
{"content":"def _family_name(set_id, name):\n    \"\"\"\n    Return the FAM object name corresponding to the unique set id and a list of subset\n    names\n    \"\"\"\n    return \"FAM\" + \"_\" + str(set_id) + \"_\" + \"_\".join(name)","sha1":"0ee1d1f24f433f46aed6ead5b08f2c5e571df70a","id":613204}
{"content":"import math\n\n\ndef calc_4piDsquared(distance, distance_err):\n    \"\"\"Calculates :math:`4\\\\pi D^2`, to convert flux to luminosity.\n\n    Args:\n        distance (float): The distance to the supernova in centimeters.\n        distance_err (float): The uncertainty in the distance to the supernova.\n\n    Returns:\n        tuple: A tuple containing the :math:`4 \\\\pi D^2`, and the uncertainty of this number.\n\n        (4piDsquared, uncertainty)\n    \"\"\"\n    fourPiDsquared = 4.0 * math.pi * distance**2.0\n    fourPiDsquared_uncertainty = 8.0 * math.pi * distance * distance_err\n\n    return fourPiDsquared, fourPiDsquared_uncertainty","sha1":"96d0f97cf98ace1220c6c6d72b849ac28d58475e","id":406365}
{"content":"def version_tuple_to_str(version, sep='.'):\n    \"\"\"Join the version components using '.' and return the string.\"\"\"\n    return sep.join([str(x) for x in version])","sha1":"2170b33a66762666e3e9e2ad2af3c5a5bec255e9","id":431870}
{"content":"def reorder_column_list(column_list_to_reorder, reference_column_list):\n    \"\"\"Keep the target list in same order as the training dataset, for consistency of forecasted columns order\"\"\"\n    reordered_list = []\n    for column_name in reference_column_list:\n        if column_name in column_list_to_reorder:\n            reordered_list.append(column_name)\n    return reordered_list","sha1":"018f05099d662d399e8a8f7bd8308fa4ff355c94","id":26590}
{"content":"def count(text, pattern):\n    \"\"\"How many times the given pattern appears in the text\"\"\"\n    i = 0\n    n = 0\n    while i <= len(text) - len(pattern):\n        if text[i:i+len(pattern)] == pattern:\n            n = n + 1\n        i = i + 1\n    return n","sha1":"9879b823aa640371a265ed1594bd726957d52866","id":536732}
{"content":"import torch\n\n\ndef accuracy(probs, targets):\n    \"\"\"Computes accuracy given predicted probabilities and expected labels.\n\n    Args:\n        probs: torch.FloatTensor[batch_size, 1], probabilities of a positive class\n        targets: torch.LongTensor[batch_size, 1], true classes\n\n    Returns:\n        0 <= float <= 1, proportion of correct predictions\n    \"\"\"\n    predictions = (probs >= 0.5).flatten()\n    targets = targets.flatten()\n    acc = torch.sum(predictions == targets).float() \/ targets.shape[0]\n    acc = float(acc)\n\n    return acc","sha1":"de7775e6c43cacf2e0045eda511d3b5e5903406a","id":374032}
{"content":"def _parse_message(exc):\n    \"\"\"Return a message for a notification from the given exception.\"\"\"\n    return '%s: %s' % (exc.__class__.__name__, str(exc))","sha1":"89cd5fafb6b79bd575bea2fb0c9de30b5157c130","id":675336}
{"content":"import random\n\n\ndef position_mod_normal(in_str):\n  \"\"\"Select any position in the given input string with normally distributed\n     likelihood where the average of the normal distribution is set to one\n     character behind the middle of the string, and the standard deviation is\n     set to 1\/4 of the string length.\n\n     This is based on studies on the distribution of errors in real text which\n     showed that errors such as typographical mistakes are more likely to\n     appear towards the middle and end of a string but not at the beginning.\n\n     Return 0 is the string is empty.\n  \"\"\"\n\n  if (in_str == ''):  # Empty input string\n    return 0\n\n  str_len = len(in_str)\n\n  mid_pos = str_len \/ 2.0 + 1\n  std_dev = str_len \/ 4.0\n  max_pos = str_len - 1\n\n  pos = int(round(random.gauss(mid_pos, std_dev)))\n  while ((pos < 0) or (pos > max_pos)):\n    pos = int(round(random.gauss(mid_pos, std_dev)))\n\n  return pos","sha1":"6ed75d80ccb4c4328639549133748126c7e3eec4","id":76853}
{"content":"def esc_regex(string: str) -> str:\n    \"\"\"Escape non-literal regex characters in a string.\n\n    :param string: the string to escape characters from\n    :return: the string with escaped characters\n    \"\"\"\n    return (\n        string.replace(\"[\", \"\\\\[\")\n        .replace(\"]\", \"\\\\]\")\n        .replace(\"\\\\\", \"\\\\\\\\\")\n        .replace(\"^\", \"\\\\^\")\n        .replace(\"$\", \"\\\\$\")\n        .replace(\".\", \"\\\\.\")\n        .replace(\"|\", \"\\\\|\")\n        .replace(\"?\", \"\\\\?\")\n        .replace(\"*\", \"\\\\*\")\n        .replace(\"+\", \"\\\\+\")\n        .replace(\"(\", \"\\\\(\")\n        .replace(\")\", \"\\\\)\")\n    )","sha1":"e23514628ce2a8f634e2dc4b3d991e8fb9830873","id":60356}
{"content":"def normalize_0_1_min_max(data, _min, _denominator, reverse=False):\n    \"\"\"\n    Normalize data in a [0, 1] interval, using the minmax technique.\n    It involves subtracting the minimum, and then dividing by the range (maximum - minimum)\n    :param data: the data to normalize; normalization is NOT performed in situ, so this data will be preserved\n    :param _min: the minimum value of the data; may not actually be min(data), can be a more 'global' value\n    :param _denominator: denominator, assumed to be maximum - minimum\n    :param reverse: if True, will actually denormalize - multiply by (max - min), then sum min. This expects the data\n    parameter to be normalized in [0, 1] interval\n    :return: the normalized (or denormalized if reverse=True) data\n    \"\"\"\n    if reverse:\n        # denormalize\n        return data * _denominator + _min\n    else:\n        # normalize\n        return (data - _min) \/ _denominator","sha1":"b11dfca950d48388aaacc2377a291abab7ac3bbc","id":293532}
{"content":"import fcntl\nimport errno\n\n\ndef lock_file(filename, mode='r+', blocking=False):\n  \"\"\"\n    Lock a file (exclusively.)\n    Requires that the file mode be a writable mode.\n\n    If blocking=True, return once the lock is held.\n    If blocking=False, return the file pointer if the lock succeeded, None if not.\n\n    Returns:\n      None if no file exists or could not access the file\n      False if could not acquire the lock\n      file object if the lock was acquired\n  \"\"\"\n\n  try:\n    fp = open(filename, mode)\n  except IOError:\n    return None\n\n  try:\n    fcntl.flock(fp, fcntl.LOCK_EX | fcntl.LOCK_NB if not blocking else fcntl.LOCK_EX)\n  except IOError as e:\n    if e.errno in (errno.EACCES, errno.EAGAIN):\n      fp.close()\n      return False\n\n  return fp","sha1":"9872d6b88fddd3303a36caba926e48e207274e80","id":283126}
{"content":"import threading\n\n\ndef thread() -> int:\n    \"\"\"Return thread id.\"\"\"\n\n    return threading.current_thread().ident or -1","sha1":"1ec3ff9f8405fb4b349870c273b764474d80eb2f","id":621269}
{"content":"import pkg_resources\n\n\ndef resource_string(path):\n    \"\"\"Handy helper for getting resources from our kit.\"\"\"\n    return pkg_resources.resource_string(__name__, path).decode(\"utf8\")","sha1":"583f0327bae3a17c45d064e028dea12193d12c6c","id":551214}
{"content":"def _fixup_find_links(find_links):\n    \"\"\"Ensure find-links option end-up being a list of strings.\"\"\"\n    if isinstance(find_links, str):\n        return find_links.split()\n    assert isinstance(find_links, (tuple, list))\n    return find_links","sha1":"468e86cab566709d230178d8b0af8f0a7bc0895b","id":399648}
{"content":"def get_best_k(trials,\n               k=1,\n               status_whitelist=None,\n               optimization_goal='minimize'):\n  \"\"\"Returns the top k trials sorted by objective_value.\n\n  Args:\n    trials: The trials (Trail objects) to sort and return the top_k of.\n    k: The top k trials to return. If k=1 we don't return a list.\n    status_whitelist: list of statuses to whitelist. If None, we use all trials.\n    optimization_goal: string, minimize or maximize.\n\n  Returns:\n    The top k trials (list unless k=1) sorted by objective_value.\n  \"\"\"\n  trials_ = trials\n  valid_status = status_whitelist\n  if valid_status is not None:\n    trials_ = [\n        trial for trial in trials_\n        if trial.status in valid_status and not trial.trial_infeasible\n    ]\n  if not trials_:\n    return None\n\n  maximizing = optimization_goal != 'minimize'\n  top = sorted(\n      trials_,\n      key=lambda t: t.final_measurement.objective_value,\n      reverse=maximizing)\n  return top[:k] if k > 1 else top[0]","sha1":"626c9664f013ed37c435a466603372d6f615d4c8","id":69921}
{"content":"def accumulate(iterable):\n    \"\"\"Accumulate the intermediate steps in summing all elements.\n\n    The result is a list with the length of `iterable`.\n    The last element is the sum of all elements of `iterable`\n    >>> accumulate(range(5))\n    [0, 1, 3, 6, 10]\n    >>> accumulate(range(10))\n    [0, 1, 3, 6, 10, 15, 21, 28, 36, 45]\n    \"\"\"\n    acm = [iterable[0]]\n    for elem in iterable[1:]:\n        old_value = acm[-1]\n        new_value = old_value + elem\n        acm.append(new_value)\n    return acm","sha1":"99b9876f39c0df5f067046df712fe833d98612e1","id":274914}
{"content":"def _replace_class_names(class_names):\n  \"\"\"Replaces class names based on a synonym dictionary.\n\n  Args:\n    class_names: Original class names.\n\n  Returns:\n    A list of new class names.\n  \"\"\"\n  synonyms = {\n      'traffic light': 'stoplight',\n      'fire hydrant': 'hydrant',\n      'stop sign': 'sign',\n      'parking meter': 'meter',\n      'sports ball': 'ball',\n      'baseball bat': 'bat',\n      'baseball glove': 'glove',\n      'tennis racket': 'racket',\n      'wine glass': 'wineglass',\n      'hot dog': 'hotdog',\n      'potted plant': 'plant',\n      'dining table': 'table',\n      'cell phone': 'cellphone',\n      'teddy bear': 'teddy',\n      'hair drier': 'hairdryer',\n  }\n  return [synonyms.get(x, x) for x in class_names]","sha1":"83fa636020ce6429f949ed6b9657083f5ad5bc34","id":320179}
{"content":"def find_unique_contig(contig, s_l):\n    \"\"\"\n    Returns a list of scaffolds that contain a certain contig\n    Called by new_resolve_unique_contigs()\n    \"\"\"    \n    in_scaffold = []\n    for scaffold in s_l:\n        if contig in scaffold:\n            in_scaffold.append(scaffold)\n            \n    return in_scaffold","sha1":"e2aa34755f4c3622639d89e694d5d5bdb0b30212","id":222921}
{"content":"def _triangulate(g, comb_emb):\n    \"\"\"\n    Helper function to schnyder method for computing coordinates in the plane to\n    plot a planar graph with no edge crossings.\n\n    Given a connected graph g with at least 3 vertices and a planar combinatorial\n    embedding comb_emb of g, modify g in place to form a graph whose faces are\n    all triangles, and return the set of newly created edges. Also ``comb_emb``\n    is updated in place.\n\n    The simple way to triangulate a face is to just pick a vertex and draw\n    an edge from that vertex to every other vertex in the face. Think that this\n    might ultimately result in graphs that don't look very nice when we draw them\n    so we have decided on a different strategy.  After handling special cases,\n    we add an edge to connect every third vertex on each face.  If this edge is\n    a repeat, we keep the first edge of the face and retry the process at the next\n    edge in the face list.  (By removing the special cases we guarantee that this\n    method will work on one of these attempts.)\n\n    INPUT:\n\n    - g -- the graph to triangulate\n    - ``comb_emb`` -- a planar combinatorial embedding of g\n\n    OUTPUT:\n\n    A list of edges that are added to the graph (in place)\n\n    EXAMPLES::\n\n        sage: from sage.graphs.schnyder import _triangulate\n        sage: g = Graph(graphs.CycleGraph(4))\n        sage: g.is_planar(set_embedding=True)\n        True\n        sage: _triangulate(g, g._embedding)\n        [(2, 0), (1, 3)]\n\n        sage: g = graphs.PathGraph(3)\n        sage: g.is_planar(set_embedding=True)\n        True\n        sage: new_edges = _triangulate(g, g._embedding)\n        sage: [sorted(e) for e in new_edges]\n        [[0, 2]]\n    \"\"\"\n    # first make sure that the graph has at least 3 vertices, and that it is connected\n    if g.order() < 3:\n        raise ValueError(\"A Graph with less than 3 vertices doesn't have any triangulation.\")\n    if not g.is_connected():\n        raise NotImplementedError(\"_triangulate() only knows how to handle connected graphs.\")\n\n    # At this point we know that the graph is connected, has at least 3\n    # vertices. This is where the real work starts.\n\n    faces = g.faces(comb_emb)\n    # We start by finding all of the faces of this embedding.\n\n    edges_added = []   # The list of edges that we add to the graph.\n    # This will be returned at the end.\n\n    for face in faces:\n        new_face = []\n        if len(face) < 3:\n            raise RuntimeError('Triangulate method created face %s with < 3 edges.' % face)\n        if len(face) == 3:\n            continue  # This face is already triangulated\n        elif len(face) == 4:  # In this special case just add diagonal edge to square\n            u, v, w, x = (e[0] for e in face)\n            if w == u or g.has_edge(w,u):\n                u, v, w, x = v, w, x, u\n            new_face = (w, u)\n            comb_emb[w].insert(comb_emb[w].index(x), u)\n            comb_emb[u].insert(comb_emb[u].index(v), w)\n            g.add_edge(new_face)\n            edges_added.append(new_face)\n        else:\n            N = len(face)\n            i = 0\n            while i < N - 1:\n                new_edge = (face[i + 1][1], face[i][0])  # new_edge is from third vertex in face to first\n                if g.has_edge(new_edge) or new_edge[0] == new_edge[1]:  # check for repeats\n                    new_face.append(face[i])  # if repeated, keep first edge in face instead\n                    if i == N - 2:   # if we are two from the end, found a triangle already\n                        break\n                    i += 1\n                    continue\n\n                g.add_edge(new_edge)\n                edges_added.append(new_edge)\n                comb_emb[new_edge[0]].insert(comb_emb[new_edge[0]].index((face + new_face)[i + 2][1]), new_edge[1])\n                comb_emb[new_edge[1]].insert(comb_emb[new_edge[1]].index(face[i][1]), new_edge[0])\n                new_face.append((new_edge[1], new_edge[0]))\n                i += 2\n            if i != N:\n                new_face.append(face[-1])\n            faces.append(new_face)\n\n    return edges_added","sha1":"c8ecd268c4de7d15f7d7a9071d6607c1b3f1943e","id":658322}
{"content":"import json\n\n\ndef load_config(json_f: str) -> dict:\n    \"\"\"Load a json file containing configuration data\n\n    Parameters:\n        json_f: path to json file\n\n    Returns:\n        config (dict): dictionary of configuration data\n\n    \"\"\"\n\n    with open(json_f, 'r') as json_src:\n        config = json.load(json_src)\n    return config","sha1":"8a1d1736c67d43338806d38609c30a43d83fb313","id":216287}
{"content":"async def process_headers(headers):\n    \"\"\"Filter out unwanted headers and return as a dictionary.\"\"\"\n    headers = dict(headers)\n    header_keys = (\n        \"user-agent\",\n        \"referer\",\n        \"accept-encoding\",\n        \"accept-language\",\n        \"x-real-ip\",\n        \"x-forwarded-for\",\n    )\n    return {k: headers.get(k) for k in header_keys}","sha1":"32feeb40c12c4b69d65da1c178e396e85fc9e557","id":701500}
{"content":"def _ifOnlyWalk(row):\n    \"\"\"Helper funtion to return True if row only contains Walk mode.\"\"\"\n    modes = set(row.split(\",\"))\n    walkExist = \"Mode::Walk\" in modes\n    length = len(modes) == 1\n    return walkExist & length","sha1":"61fa0c853936dd53dd8981aa25f63a3a2488565b","id":565233}
{"content":"import math\n\n\ndef t_dp(tdb, rh):\n    \"\"\"Calculates the dew point temperature.\n\n    Parameters\n    ----------\n    tdb: float\n        dry-bulb air temperature, [\u00b0C]\n    rh: float\n        relative humidity, [%]\n\n    Returns\n    -------\n    t_dp: float\n        dew point temperature, [\u00b0C]\n    \"\"\"\n\n    c = 257.14\n    b = 18.678\n    d = 234.5\n\n    gamma_m = math.log(rh \/ 100 * math.exp((b - tdb \/ d) * (tdb \/ (c + tdb))))\n\n    return round(c * gamma_m \/ (b - gamma_m), 1)","sha1":"1f79b970f8a1a32535c591fe114c82a9550ff573","id":360955}
{"content":"import torch\n\n\ndef compute_acc(pred, labels):\n    \"\"\"\n    Compute the accuracy of prediction given the labels.\n    \"\"\"\n    return (torch.argmax(pred, dim=1) == labels).float().sum() \/ len(pred)","sha1":"1b1ad83b9b4ae06f2bc80209e4e7339a421a39f3","id":709062}
{"content":"def _title_to_snake(src_string):\n    \"\"\"Convert title case to snake_case.\"\"\"\n    return src_string.lower().replace(\" \", \"_\").replace(\"\/\", \"-\")","sha1":"ff0e56d48be19ed1ab7404978acf1eef146dcdf3","id":540121}
{"content":"import re\n\n\ndef process_disk_usage(df_output):\n    \"\"\"\n    Extract the file-system and disk usage from 'df -h' output.\n    Could also use same \"pattern\" and re.search (while looping over the lines, but\n    re.find was a more elegant solution).\n    \"\"\"\n    # Strip out the header line\n    df_output = re.sub(r\"^Filesystem.*Mounted on$\", \"\", df_output, flags=re.M)\n    pattern = r\"^(\\S+).*?(\\d+%) .*\"\n    match = re.findall(pattern, df_output, flags=re.M)\n    if not match:\n        raise ValueError(\"Failed to parse 'df' output correctly\")\n    return match","sha1":"a8006f6e46b5525c8e74bb1d7608b913bcfd73d9","id":120378}
{"content":"def get_subdomain(environment):\n    \"\"\"\n    Get the Salesforce sub-domain based on Production or Test\n    \"\"\"\n    return 'login' if environment == 'Production' else 'test'","sha1":"32434f40c31e6b0e6143421898595a6b861dba37","id":567367}
{"content":"def total_rain(rain_hours: list) -> int:\n    \"\"\"\n    Sum up the rain mm from list of rain hours\n    \"\"\"\n    if len(rain_hours) > 0:\n        return round(sum([list(x.values())[0]['1h'] for x in rain_hours]), 2)\n    else:\n        return 0","sha1":"72e26bdb2600eaaa321a532712dd1a815ba01789","id":638707}
{"content":"def get_overlay_color(argument):\n    \"\"\"\n    Determines the color the user wants the watermark overlay to be.\n    Returns a string containing said color.\n    Returns \"color\" if no valid color can be found in `argument`.\n    \"\"\"\n    if argument:\n        if \"black\" in argument:\n            return \"black\"\n        elif \"white\" in argument:\n            return \"white\"\n    return \"color\"","sha1":"94f6c8c51a115ad425e381b39af856684aec822d","id":357805}
{"content":"import typing\n\n\ndef get_ctffind_4_1_0_header_names() -> typing.List[str]:\n    \"\"\"\n    Returns the header names for the ctffind4 input file.\n\n    Arguments:\n    None\n\n    Returns:\n    List of names\n    \"\"\"\n    return [\n        'DefocusU',\n        'DefocusV',\n        'DefocusAngle',\n        'PhaseShift',\n        'CtfFigureOfMerit',\n        'CtfMaxResolution',\n        ]","sha1":"8e8ea28cc1a66690b67c5fe1d5f4f11aed79a49d","id":631413}
{"content":"def refund_assistant_ability(abilities_dict):\n    \"\"\"\n    Gets the amount of resources for investigation_assistant ability refund\n    Args:\n        abilities_dict: dictionary of our abilities\n\n    Returns:\n        The amount of resources to refund them.\n    \"\"\"\n    return 50 * abilities_dict.get(\"investigation_assistant\", 0)","sha1":"0f5ea5c32f8ff180cfd7ddfe47235f302a7c524e","id":292334}
{"content":"def btc_to_satoshi(btc):\n    \"\"\"\n    Converts a value in BTC to satoshi\n    outputs => <int>\n\n    btc: <Decimal> or <Float>\n    \"\"\"\n    value = btc * 100000000\n    return int(value)","sha1":"c75fe3bec4389b8fc332188edfeda575766aeae2","id":627552}
{"content":"def find_codon_new(codon, seq):\n    \"\"\" Find a specified codon within a given sequence. \"\"\"\n    i = 0\n    # Scan sequenece until we hit a start codon or the end of sequence\n    while seq[i:i+3] != codon and i < len(seq):\n        i += 3\n\n    if i == len(seq):\n        return -1\n\n    return i","sha1":"9e28bff92c75b9e4a81642887ce0d147eda9500d","id":478868}
{"content":"def translate_word(word):\n    \"\"\"Translates a single word to Pig Latin\"\"\"\n    VOWELS = {'a', 'e', 'i', 'o', 'u'}\n\n    # Handle punctuations\n    if not word.isalpha():\n        return word\n\n    # Handle capitalized words\n    if word[0].isupper():\n        return translate_word(word.lower()).capitalize()\n\n    # Case: word begins with a vowel\n    if word[0].lower() in VOWELS:\n        return word.lower() + 'yay'\n\n    # Case: word begins with a consonant        \n    else:\n        for index, char in enumerate(word):\n            if char.lower() in VOWELS:\n                return word[index:].lower() + word[:index].lower() + 'ay'\n\n    # Case: word contains no vowels\n    return word + 'ay'","sha1":"304ee11df40b2f113166f85484ecbc8abf7e5e7a","id":440280}
{"content":"def timedelta_to_str(aTimedelta):\n    \"\"\" a conversion function for time deltas to string in the form\n    DD:HH:MM:SS\n    \"\"\"\n    days = aTimedelta.days\n    temp_seconds = aTimedelta.seconds\n    hours = temp_seconds \/ 3600\n    minutes = (temp_seconds - hours * 3600) \/ 60\n    seconds = temp_seconds - hours * 3600 - minutes * 60\n    return '%d:%d:%d:%d' % (days, hours, minutes, seconds)","sha1":"9aef036edaf295f0f120de92028ce219b3e449e0","id":86876}
{"content":"def clip(num, num_min=None, num_max=None):\n    \"\"\"Clip to max and\/or min values.  To not use limit, give argument None\n\n    Args:\n        num (float): input number\n        num_min (float): minimum value, if less than this return this num\n            Use None to designate no minimum value.\n        num_max (float): maximum value, if more than this return this num\n            Use None to designate no maximum value.\n\n    Returns\n        float: clipped version of input number\n    \"\"\"\n    if num_min is not None and num_max is not None:\n        return min(max(num, num_min), num_max)\n    elif num_min is not None:\n        return max(num, num_min)\n    elif num_max is not None:\n        return min(num, num_max)\n    else:\n        return num","sha1":"fe46f5a200ab24d517c57c5e1d93d4bf86192e13","id":700310}
{"content":"import torch\n\n\ndef get_mask_without_background(mask, label, background=0):\n    \"\"\"\n    Return given mask with `False` where `label == background`.\n\n    :param mask: A [*, d] bool tensor\n    :param label: A [*, d] tensor\n    :param background: A value (must be same type of `label`'s cells)\n    :return: A [*, d] bool tensor\n    \"\"\"\n    return torch.masked_fill(mask, mask=label == background, value=0).to(torch.bool)","sha1":"eeb51876eabbf98b41220d7198517e27aae7ad9b","id":429781}
{"content":"from zlib import crc32\n\n\ndef safe_crc(string):\n    \"\"\"64 bit safe crc computation.\n\n    See http:\/\/docs.python.org\/library\/zlib.html#zlib.crc32:\n\n        To generate the same numeric value across all Python versions \n        and platforms use crc32(data) & 0xffffffff.\n    \"\"\"\n\n\n    return crc32(string) & 0xffffffff","sha1":"43c6693c7d30ef19be7cda3d270f5c1225d70778","id":636960}
{"content":"from typing import List\n\n\ndef padding_list(someList: List[str], N: int) -> List[str]:\n    \"\"\"Padding the list with <s> at the front and <\/s> behind\n\n    Args:\n        someList (List[str]): The list to be padded with\n        N (int): The amount of <s>, <\/s> to be padded\n\n    Returns:\n        List[str]: Padded list\n    \"\"\"\n    for i in range(N):\n        someList = ['<s>'] + someList + ['<\/s>']\n    return someList","sha1":"e8df315d715e5e1e4575b42da5f6ff1691107732","id":31767}
{"content":"from functools import reduce\n\n\ndef union(*sets):\n    \"\"\"Get the union of all input sets\"\"\"\n    return reduce(set.union, sets)","sha1":"a36a5d8b7cb1b6f40a342418e183cc4098f16823","id":203366}
{"content":"def reinforce_grad(loss):\n    \"\"\"\n    A closure to modify the gradient of a nn module. \n    Use to implement REINFORCE gradient. Gradients will\n    be multiplied by loss.\n    Arguments: \n    - loss: Gradients are multiplied by loss, should be a scalar\n    \"\"\"\n    def hook(module, grad_input, grad_output):\n        new_grad = grad_input * loss\n        return new_grad\n        \n    return hook","sha1":"c1dfcf5079e2516785867dd6677ece04b831fcb4","id":75834}
{"content":"def aggregate_shares(df):\n    \"\"\"Compute share types by year and field.\"\"\"\n    return (df.groupby([\"Year\", \"types\", \"field\"]).size()\n              .reset_index()\n              .rename(columns={0: \"count\"}))","sha1":"696325e80cc6249906181ee670f428e54b22c204","id":508885}
{"content":"from typing import List\n\n\ndef datum_is_sum(datum: int, preamble: List[int]) -> bool:\n    \"\"\"Iterate the preamble numbers and determine whether datum is a sum.\n\n    Args:\n        datum (int): a number that should be a sum of any two numbers\n            in preamble\n        preamble (List[int]): a list of preceeding n numbers where\n            n is preamble_size in check_data_for_invalid()\n\n    Returns:\n        bool: True if the datum is a sum; False otherwise\n\n    \"\"\"\n    for pre in preamble:\n        diff = datum - pre\n        # The difference must not be the same as the addend that produced it\n        if diff == pre:\n            continue\n        elif diff in preamble:\n            return True\n\n    return False","sha1":"e5024bb0adbada3ba07a949505db8a31c0170958","id":682497}
{"content":"from typing import List\nfrom typing import Union\n\n\ndef make_it_fit(cell_list: List[str], limit: int) -> Union[None, List[str]]:\n    \"\"\"\n    This function attempts to shorten a list of strings by finding and\n    elimininating empty string elements from left to rigth. If succesfull\n    it will return the modified list. Otherwise it will return None.\n    \"\"\"\n\n    if len(cell_list) <= limit:\n        return cell_list\n    else:\n        while sum([len(x) == 0 for x in cell_list]):\n            cell_list.remove(\"\")\n            if len(cell_list) == limit:\n                return cell_list\n        else:\n            return None","sha1":"48884f977074b82f391c7fb989e08dab1d285bf7","id":44736}
{"content":"def get_mil_std_limits(name, mil_std_dict):\n    \"\"\" Returns the min and max values for an specific dimension check \"\"\"\n    std_min = mil_std_dict[name]['min']\n    std_max = mil_std_dict[name]['max']\n    return std_min, std_max","sha1":"3963d7c4e0098564511664ea3b0ff050730120f4","id":314492}
{"content":"def combine_crops(cancer_set, non_cancer_set, batch_sizes=(10, 10), hu_lims=(-1000, 400)):\n    \"\"\" Pipeline for generating batches of cancerous and non-cancerous crops from\n    ct-scans in chosen proportion.\n\n    Parameters\n    ---------\n    cancer_set : dataset\n        dataset of cancerous crops in blosc format.\n    non_cancer_set : dataset\n        dataset of non-cancerous crops in blosc format.\n    batch_sizes : tuple, list of int\n        seq of len=2, (num_cancer_batches, num_noncancer_batches).\n    hu_lims : tuple, list of float\n        seq of len=2, representing limits of hu-trimming in normalize_hu-action.\n\n    Returns\n    -------\n    pipeline\n    \"\"\"\n    # pipeline generating cancerous crops\n    ppl_cancer = (cancer_set.p\n                  .load(fmt='blosc')\n                  .normalize_hu(min_hu=hu_lims[0], max_hu=hu_lims[1])\n                  .run(lazy=True, batch_size=batch_sizes[0], shuffle=True)\n                 )\n\n    # pipeline generating non-cancerous crops merged with first pipeline\n    pipeline = (non_cancer_set.p\n                .load(fmt='blosc')\n                .normalize_hu(min_hu=hu_lims[0], max_hu=hu_lims[1])\n                .merge(ppl_cancer)\n                .run(lazy=True, batch_size=batch_sizes[1], shuffle=True)\n               )\n\n    return pipeline","sha1":"4206374336246aea8855292ddf2c93f744da01fe","id":314435}
{"content":"def is_property(class_to_check, name):\n    \"\"\"Determine if the specified name is a property on a class\"\"\"\n    if hasattr(class_to_check, name) and isinstance(\n        getattr(class_to_check, name), property\n    ):\n        return True\n    return False","sha1":"0f7dce28f1e78e8b6b937a5a6536060886666371","id":698321}
{"content":"import json\n\n\ndef read_json(file_path):\n    \"\"\"Convenience method for reading jsons\"\"\"\n    return json.load(open(file_path))","sha1":"585aa6133e6901c9b62e97c700fd10c712a58f7a","id":328767}
{"content":"def transition_model(corpus, page, damping_factor):\n    \"\"\"\n    Return a probability distribution over which page to visit next,\n    given a current page.\n\n    With probability `damping_factor`, choose a link at random\n    linked to by `page`. With probability `1 - damping_factor`, choose\n    a link at random chosen from all pages in the corpus.\n    \"\"\"\n    pagePointers = set()\n    for x, y in corpus.items():\n        if x == page:\n            pagePointers = y\n            break\n\n    chances = dict()\n    if len(pagePointers) == 0:\n        chance = 1\/len(corpus)\n        for x in corpus:\n            chances[x] = chance\n        return chances\n\n    chanceAmortecimento = (1-damping_factor)\/(len(corpus))\n    chanceComum = (damping_factor\/len(pagePointers))\n    for x in corpus:\n        if x in pagePointers:\n            chances[x] = chanceAmortecimento + chanceComum\n        else:\n            chances[x] = chanceAmortecimento\n\n    return chances","sha1":"dc12174d6de4e0d576b43f3783a484c41dc51497","id":216573}
{"content":"def reverseIP(ip):\n    \"\"\" Reverses the order of octets in the IPv4 address \"\"\"\n    return '.'.join(reversed(ip.split('.')))","sha1":"b97b0913677e47971f62749d228bb6846138d4b5","id":488510}
{"content":"import pytz\nfrom datetime import datetime\nimport math\n\n\ndef reltime(date, compare_to=None, at='@'):\n    \"\"\"\n    Takes a datetime and returns a relative representation of the\n    time.\n    :param date: The date to render relatively\n    :param compare_to: what to compare the date to. Defaults to datetime.now()\n    :param at: date\/time separator. defaults to \"@\". \"at\" is also reasonable.\n    >>> from datetime import datetime, timedelta\n    >>> today = datetime(2050, 9, 2, 15, 00)\n    >>> earlier = datetime(2050, 9, 2, 12)\n    >>> reltime(earlier, today)\n    'today @ 12pm'\n    >>> yesterday = today - timedelta(1)\n    >>> reltime(yesterday, compare_to=today)\n    'yesterday @ 3pm'\n    >>> reltime(datetime(2050, 9, 1, 15, 32), today)\n    'yesterday @ 3:32pm'\n    >>> reltime(datetime(2050, 8, 31, 16), today)\n    'Wednesday @ 4pm (2 days ago)'\n    >>> reltime(datetime(2050, 8, 26, 14), today)\n    'last Friday @ 2pm (7 days ago)'\n    >>> reltime(datetime(2049, 9, 2, 12, 00), today)\n    'September 2nd, 2049 @ 12pm (last year)'\n    >>> today = datetime(2012, 8, 29, 13, 52)\n    >>> last_mon = datetime(2012, 8, 20, 15, 40, 55)\n    >>> reltime(last_mon, today)\n    'last Monday @ 3:40pm (9 days ago)'\n    \"\"\"\n\n    def ordinal(n):\n        \"\"\"\n        Returns a string ordinal representation of a number\n        Taken from: http:\/\/stackoverflow.com\/a\/739301\/180718\n        \"\"\"\n        if 10 <= n % 100 < 20:\n            return str(n) + 'th'\n        else:\n            return str(n) + {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th')\n\n    compare_to = compare_to or pytz.UTC.localize(datetime.now())\n    if date > compare_to:\n        return NotImplementedError('reltime only handles dates in the past')\n    # get timediff values\n    diff = compare_to - date\n    if diff.seconds < 60 * 60 * 8:  # less than a business day?\n        days_ago = diff.days\n    else:\n        days_ago = diff.days + 1\n    months_ago = compare_to.month - date.month\n    years_ago = compare_to.year - date.year\n    weeks_ago = int(math.ceil(days_ago \/ 7.0))\n    # get a non-zero padded 12-hour hour\n    hr = date.strftime('%I')\n    if hr.startswith('0'):\n        hr = hr[1:]\n    wd = compare_to.weekday()\n    # calculate the time string\n    if date.minute == 0:\n        time = '{0}{1}'.format(hr, date.strftime('%p').lower())\n    else:\n        time = '{0}:{1}'.format(hr, date.strftime('%M%p').lower())\n    # calculate the date string\n    if days_ago == 0:\n        datestr = 'today {at} {time}'\n    elif days_ago == 1:\n        datestr = 'yesterday {at} {time}'\n    elif (\n        wd in (5, 6) and days_ago in (wd + 1, wd + 2)\n    ) or wd + 3 <= days_ago <= wd + 8:\n        # this was determined by making a table of wd versus days_ago and\n        # divining a relationship based on everyday speech. This is somewhat\n        # subjective I guess!\n        datestr = 'last {weekday} {at} {time} ({days_ago} days ago)'\n    elif days_ago <= wd + 2:\n        datestr = '{weekday} {at} {time} ({days_ago} days ago)'\n    elif years_ago == 1:\n        datestr = '{month} {day}, {year} {at} {time} (last year)'\n    elif years_ago > 1:\n        datestr = '{month} {day}, {year} {at} {time} ({years_ago} years ago)'\n    elif months_ago == 1:\n        datestr = '{month} {day} {at} {time} (last month)'\n    elif months_ago > 1:\n        datestr = '{month} {day} {at} {time} ({months_ago} months ago)'\n    else:\n        # not last week, but not last month either\n        datestr = '{month} {day} {at} {time} ({days_ago} days ago)'\n    return datestr.format(\n        time=time,\n        weekday=date.strftime('%A'),\n        day=ordinal(date.day),\n        days=diff.days,\n        days_ago=days_ago,\n        month=date.strftime('%B'),\n        years_ago=years_ago,\n        months_ago=months_ago,\n        weeks_ago=weeks_ago,\n        year=date.year,\n        at=at,\n    )","sha1":"a739cf473dffb3da48ac5720224af9e3a8eca881","id":188671}
{"content":"def evaluate_classifier(classifier, eval_set, batch_size):\n    \"\"\"\n    Function to get accuracy and cost of the model, evaluated on a chosen dataset.\n\n    classifier: the model's classfier, it should return genres, logit values, and cost for a given minibatch of the evaluation dataset\n    eval_set: the chosen evaluation set, for eg. the dev-set\n    batch_size: the size of minibatches.\n    \"\"\"\n    correct = 0\n    genres, hypotheses, cost = classifier(eval_set)\n    cost = cost \/ batch_size\n    full_batch = int(len(eval_set) \/ batch_size) * batch_size\n    for i in range(full_batch):\n        hypothesis = hypotheses[i]\n        if hypothesis == eval_set[i]['label']:\n            correct += 1        \n    return correct \/ float(len(eval_set)), cost","sha1":"29894df63de989f86d473b76d3ca9c0bc992bec8","id":255404}
{"content":"from typing import Counter\n\n\ndef get_bow(tokenized_text):\n    \"\"\"\n    Function to generate bow_list and word_freq from a tokenized_text\n    -----PARAMETER-----\n    tokenized_text should be in the form of [['a'], ['a', 'b'], ['b']] format,\n    where the object is a list of survey response, with each survey response\n    as a list of word tokens\n    -----OUTPUT-----\n    The function returns two objects\n    bow_list: a list of Counter objects with word frequency of each response\n    word_freq: a Counter object that summarizes the word frequency of the input\n    tokenized_text\n    \"\"\"\n    bow_list = []\n    word_freq = Counter()\n    for text in tokenized_text:\n        bow = Counter(text)\n        word_freq.update(text)\n        bow_list.append(bow)\n    print(f\"This corpus has {len(word_freq.keys())} key words, and the 10 \\\nmost frequent words are: {word_freq.most_common(10)}\")\n    return bow_list, word_freq","sha1":"656d9dab1b2bee350cecca5fd693fcbc3eafb2bd","id":688368}
{"content":"def get_dot(dic, key, default=None):\n    \"\"\" Similar to dict.get(), but key is given with dot (eg. foo.bar) and\n    result is evaluated in generous way. That is, get_dot(dic, 'foo.bar.vaz')\n    will return dic['foo']['bar'] if both dic['foo'] and dic['foo']['baz'] exists,\n    but return default if any of them does not exists.\n    \"\"\"\n    keys = key.split('.')\n    res = dic\n    for k in keys:\n        if not isinstance(res, dict):\n            return default\n        elif k in res:\n            res = res[k]\n        else:\n            return default\n    return res","sha1":"cc764debf77b6982733226ec5547e26e2394cd89","id":46083}
{"content":"def sphinpq(phi2, phinot, ql, phinoopt=0.2, fmf0=4.88):\n    \"\"\"Calculate S~PhiNPQ\n\n    S~PhiNPQ = 1 - ((phi2 * 1 \/ (1 + phi2 * (1\/phinoopt - 1\/phinot) \/ (ql * fmf0) ))+fmf0)\n\n    :param phi2: Phi2\n    :param phinot: PhiNOt\n    :param ql: qL\n    :param phinoopt: Optimum PhiNO (default: 0.2)\n    :param fmf0: Fv\/Fm (default: 4.88)\n    :returns: SPhiNPQ (float)\n    \"\"\"\n    return 1 - ((phi2 * 1 \/ (1 + phi2 * (1\/phinoopt - 1\/phinot) \/ (ql * fmf0)))+phinoopt)","sha1":"487440fbf1b34d58bbb15d212db12fdcc5c82700","id":587426}
{"content":"def getdepth(tree):\n    \"\"\"\n\tReturns the depth of the tree\n\n\tparam tree -- the decision tree\n\n\t\"\"\"\n    if tree.tb is None and tree.fb is None:\n        return 0\n    return max(getdepth(tree.tb), getdepth(tree.fb)) + 1","sha1":"0eaa619c20ac9743bd2663f7ed3a507a09ea7d45","id":613523}
{"content":"import re\n\n\ndef prep_for_search(string):\n    \"\"\"\n    Expects a string. Encodes strings in a search-friendy format,\n    lowering and replacing spaces with \"+\"\n    \"\"\"\n    string = re.sub('[^A-Za-z0-9 ]+', '', string).lower()\n    string = string.replace(\" \", \"+\")\n    return string","sha1":"61a6762598fe3538b2d2e2328bc77de53fba4d74","id":41638}
{"content":"def incrementdefault(obj: dict, key, default: int = 0) -> int:\n    \"\"\"Increments value stored in the `obj` under `key`.\n\n    Returns the incremented value.\n\n    If value with specified key does not exist, then initializes it with\n    `default` and then increments it.\n    \"\"\"\n\n    val = obj.setdefault(key, default) + 1\n    obj[key] = val\n    return val","sha1":"3f440a6f030b38220ee447cf5f9b75f91329bd4b","id":377212}
{"content":"def g2N(T):\n    \"\"\"\n    Converting gram to Newton\n    \"\"\"\n    return [0.00980665 * i for i in T]","sha1":"648ff44af335d762fa083e1ba1bb62f1474f60a4","id":653632}
{"content":"from typing import Iterable\nfrom typing import Tuple\nimport networkx\n\n\ndef build_graph(data: Iterable[Tuple[dict, str, dict]]):\n    \"\"\"\n    Builds a NetworkX DiGraph object from (Child, Edge, Parent) triples.\n    Each triple is represented as a directed edge from Child to Parent\n    in the DiGraph.\n\n    Child and Parent must be dictionaries containing all hashable values\n    and a 'name' key (this is the name of the node in the DiGraph).\n\n    Edge must be a string representing an edge label from Child to Parent.\n\n    :param data: Iterable of (Child, Edge, Parent) triples.\n    :rtype: networkx.DiGraph\n    \"\"\"\n    g = networkx.DiGraph()\n\n    for child_attrs, edge, parent_attrs in data:\n        if 'name' not in child_attrs or 'name' not in parent_attrs:\n            raise ValueError(\n                \"Both child and parent dicts must contain a 'name' key.\\n\"\n                \"Provided Child data: {}\\n\"\n                \"Provided Parent data: {}\\n\".format(child_attrs, parent_attrs)\n            )\n        # Copy dicts so popping 'name' doesn't affect the underlying data\n        child_attrs, parent_attrs = child_attrs.copy(), parent_attrs.copy()\n        child_name, parent_name = child_attrs.pop('name'), parent_attrs.pop('name')\n\n        # Update node attributes only if the updated version has strictly more data\n        # than the previous version\n        if child_name not in g or set(child_attrs).issuperset(g.nodes[child_name]):\n            g.add_node(child_name, **child_attrs)\n        if parent_name not in g or set(parent_attrs).issuperset(g.nodes[parent_name]):\n            g.add_node(parent_name, **parent_attrs)\n        g.add_edge(child_name, parent_name, label=edge)\n\n    return g","sha1":"6d43f3d2b9698eaac54cfcee38fd005e6762eaf6","id":20813}
{"content":"def split_file_name(file_absolute_name):\n    \"\"\"\n    Splits a file name into: [directory, name, extension]\n\n    :param full_name_template:\n    :return:\n    \"\"\"\n    split_template = file_absolute_name.split('\/')\n    [name, extension] = split_template[-1].split('.')\n    directory = \"\/\".join(split_template[0: len(split_template) - 1])\n    return [directory, name, extension]","sha1":"f3144b0f75cbde26c8a8244cb8d8eda712e8edfa","id":635306}
{"content":"from typing import Dict\nfrom typing import Any\n\n\ndef my_theme() -> Dict[str, Any]:\n    \"\"\"Define an Altair theme to use in all visualisations.\n\n    This defines a simple theme, specifying the aspect ratio of 4:6 \n    and removing the grid from the figure which is distracting.\n\n    \"\"\"\n    font = \"Roboto\"\n    return {\n        \"config\": {\n            \"view\": {\"height\": 400, \"width\": 600},\n            \"legend\": {\n                \"titleFontSize\": 20,\n                \"labelFontSize\": 16,\n                \"labelFont\": font,\n                \"titleFont\": font,\n            },\n            \"axis\": {\n                \"grid\": False,\n                \"labelFontSize\": 16,\n                \"titleFontSize\": 20,\n                \"labelFont\": font,\n                \"titleFont\": font,\n            },\n            \"header\": {\n                \"titleFontSize\": 22,\n                \"labelFontSize\": 18,\n                \"titleFont\": font,\n                \"labelFont\": font,\n            },\n            \"background\": \"white\",\n        }\n    }","sha1":"cecc404e69b533af81a7221ad6c72cd261a7babb","id":446887}
{"content":"def _all_repo_priorities_same(allrepos):\n    \"\"\" Are all repos at the same priority \"\"\"\n    first = None\n    for repo in allrepos:\n        if first is None:\n            first = repo.priority\n        elif first != repo.priority:\n            return False\n    return True","sha1":"96ba265e76be2001913c825e7eeff288ee41b5d3","id":363250}
{"content":"import re\n\n\ndef like(matchlist: list, array: list, andop=False):\n    \"\"\"Returns a list of matches in the given array by doing a comparison of each object with the values given to the matchlist parameter.\n\n    Examples:\n        >>> subdir_list = ['get_random.py',\n        ...  'day6_15_payload-by-port.py',\n        ...  'worksheet_get_exif.py',\n        ...  'worksheet_get_random.py',\n        ...  'day6_19_browser-snob.py',\n        ...  'day6_12_letterpassword.py',\n        ...  'day6_21_exif-tag.py',\n        ...  'day6_17_subprocess_ssh.py',\n        ...  'day6_16._just_use_split.py']\n\n        >>> like('day', subdir_list)\\n\n        ['day6_15_payload-by-port.py', 'day6_19_browser-snob.py', 'day6_12_letterpassword.py', 'day6_21_exif-tag.py', 'day6_17_subprocess_ssh.py', 'day6_16._just_use_split.py']\n\n        >>> like(['get','exif'], subdir_list)\\n\n        ['get_random.py', 'worksheet_get_exif.py', 'worksheet_get_random.py', 'day6_21_exif-tag.py']\n\n        >>> like(['get','exif'], subdir_list, andop=True)\\n\n        ['worksheet_get_exif.py']\n\n    Args:\n        matchlist (list): Submit one or many substrings to match against\n        array (list): This is the list that we want to filter\n        andop (bool, optional): This will determine if the matchlist criteria is an \"And Operation\" or an \"Or Operation. Defaults to False (which is the \"Or Operation\").  Only applies when multiple arguments are used for the \"matchlist\" parameter\n\n    Returns:\n        list: Returns a list of matches\n\n    References:\n        https:\/\/stackoverflow.com\/questions\/469913\/regular-expressions-is-there-an-and-operator\n        https:\/\/stackoverflow.com\/questions\/3041320\/regex-and-operator\n        https:\/\/stackoverflow.com\/questions\/717644\/regular-expression-that-doesnt-contain-certain-string\n    \"\"\"\n\n    if isinstance(matchlist, str):\n        # matchlist is a single string object\n        thecompile = re.compile(rf\"^(?=.*{matchlist}).*$\")\n        result_list = [x for x in array if re.findall(thecompile, x)]\n        return result_list\n    else:\n        if andop:\n            # We will be doing an \"AND\" match or an \"And\" \"Operation\"\n            match_string = r\"(?=.*?\" + r\".*?)(?=.*?\".join(matchlist) + r\".*?)\"\n            # e.g. for the above... ['6_19','6_21','6_24'] turns to: '(?=.*?6_19.*?)(?=.*?6_21.*?)(?=.*?6_24.*?)'\n            thecompile = re.compile(rf\"^{match_string}.*$\")\n            # equivalent to: '^(?=.*?6_19.*?)(?=.*?6_21.*?)(?=.*?6_24.*?).*$'\n            result_list = [x for x in array if re.findall(thecompile, x)]\n            return result_list\n        else:\n            # We will be doing an \"OR\" match\n            match_string = r\"(?=.*\" + r\"|.*\".join(matchlist) + \")\"\n            # e.g. for the above... ['6_19','6_21','6_24'] turns to: '(?=.*6_19|.*6_21|.*6_24)'\n            thecompile = re.compile(rf\"^{match_string}.*$\")\n            # equivalent to: '^(?=.*6_19|.*6_21|.*6_24).*$'\n            result_list = [x for x in array if re.findall(thecompile, x)]\n            return result_list","sha1":"b7cf450cdd06bd8e0e3bc6d35c3a6f8ba0cfa457","id":680465}
{"content":"def add_reprompt(response):\n    \"\"\"\n    Adds a response message to tell the user to ask their question again.\n    \"\"\"\n    response['response']['reprompt'] = {\n        \"outputSpeech\": {\n            \"type\": \"PlainText\",\n            \"text\": \"Please ask your crypto price question\"\n        }\n    }\n    return response","sha1":"64fa9f057e84d7332a7c3273a6d1940bcbe03d36","id":636991}
{"content":"def replace_vars(cmd, params):\n    \"\"\" Replace any occurance of $VAR in cmd, with its value in params if\n        one present. Returns string with replacement.\"\"\"\n    fullcmd = \"\"\n    args = cmd.split(\" \")\n    for arg in args:\n        if arg.startswith(\"$\"):\n            param = arg[1:]\n            if param in params:\n                fullcmd = fullcmd + params[param]\n            else:\n                fullcmd = fullcmd + arg\n        else:\n            fullcmd = fullcmd + arg\n        fullcmd = fullcmd + \" \"\n    fullcmd = fullcmd.strip()\n    return fullcmd","sha1":"e99e810ae0d2e23ceaf23dc1c528a3d75e26647b","id":467472}
{"content":"import math\nimport itertools\n\n\ndef rangeinf(start, stop=None, step=1):\n    \"\"\"Acts as range, however passing stop=math.inf or np.inf will have it go forever.\"\"\"\n    # Allows us to pass just one argument and has it act as 'stop', with 'start' set to zero. This is the same\n    # behaviour as the usual range function\n    if stop is None:\n        stop = start\n        start = 0\n    if stop == math.inf:  # Also True for numpy.inf\n        return itertools.count(start, step)\n    else:\n        return range(start, stop, step)","sha1":"2e9f1496c75dcaef4927bf8a22c55241357c0c23","id":520866}
{"content":"def create_fts_metadata(all_keywords, action_metadata, source_fts_metadata_actions, extra_metadata):\n    \"\"\"\n    Create an FTS metadata dictionary.\n\n    Parameters\n    ----------\n    all_keywords: list\n        List of all keywords associated with the resource\n    action_metadata: list\n        List of action metadata for the current action.\n    source_fts_metadata_actions: list\n        List of action metadata for any actions found in the source.\n    extra_metadata: dict\n        The extra metadata gathered during download\n\n    Returns\n    -------\n    Dictionary of FTS metadata.\n    \"\"\"\n    return {\n        'allKeywords': all_keywords,\n        'actions': [action_metadata] + source_fts_metadata_actions,\n        'extra_metadata': extra_metadata\n    }","sha1":"c1db5efd7a9c4b3cf32fbca414eb9dc3455e007b","id":603224}
{"content":"def get_poly_area(poly1):\n    \"\"\" Calcualte the number of pixels the polygon covered.\n\n    Parameters\n    -------\n    poly1 : Polygon\n        contour with shapely polygon format\n\n    Returns\n    -------\n    area_val : int\n        number of pixels inside the contour\n\n    \"\"\"\n\n    area_val = poly1.area\n\n    return area_val","sha1":"4552b3907b9a8e35f58fe38e011bd933a1ce994a","id":207245}
{"content":"def invert_expression(exp):\n    \"\"\"Inverts and returns logical operator expressions\n       ex. \"<\" -> \">=\"\n\n    Args:\n        exp (str): original conditional expression\n\n    Returns:\n        str: inverted string representation of original conditional expression\n    \"\"\"\n    if \">=\" in exp:\n        return exp.replace(\">=\", \"<\")\n    elif \"<=\" in exp:\n        return exp.replace(\"<=\", \">\")\n    elif \">\" in exp:\n        return exp.replace(\">\", \"<=\")\n    elif \"<\" in exp:\n        return exp.replace(\"<\", \">=\")\n    elif \"True\" in exp:\n        return exp.replace(\"True\", \"False\")\n    elif \"False\" in exp:\n        return exp.replace(\"False\", \"True\")\n    else:\n        return exp","sha1":"875998d6b3569e0229dbfcf9dbdf1fdff493a083","id":511509}
{"content":"def get_thousands_string(f):\n    \"\"\"\n    Get a nice string representation of a field of 1000's of NIS, which is int or None.\n    \"\"\"\n    if f is None:\n        return \"N\/A\"\n    elif f == 0:\n        return \"0 NIS\"\n    else:\n        return \"%d000 NIS\" % f","sha1":"4201c6acd74d237e6721c6aa2ef4813df9388e2a","id":556337}
{"content":"import torch\n\n\ndef point_cloud_from_depth(depth, K):\n    \"\"\"Transform depth image pixels selected in mask into point cloud in camera frame\n    depth: [N, H, W] depth image\n    mask: [N, H, W] mask of pixels to transform into point cloud\n    K: [N, 3, 3] Intrinsic camera matrix\n    returns: [N, 3, K]\n    \"\"\"\n    batch_size, H, W = depth.shape\n    # Create 3D grid data\n    u_img_range = torch.arange(0, W, device=depth.device)\n    v_img_range = torch.arange(0, H, device=depth.device)\n    u_grid, v_grid = torch.meshgrid(u_img_range, v_img_range)\n    u_grid = u_grid.t()[None, :].repeat(batch_size, 1, 1)\n    v_grid = v_grid.t()[None, :].repeat(batch_size, 1, 1)\n    u_img, v_img, d = (\n        u_grid.reshape(batch_size, -1),\n        v_grid.reshape(batch_size, -1),\n        depth.reshape(batch_size, -1),\n    )\n\n    # homogenuous coordinates\n    uv = torch.stack((u_img, v_img, torch.ones_like(u_img)), dim=1).float()\n\n    # get the unscaled position for each of the points in the image frame\n    unscaled_points = torch.linalg.inv(K) @ uv\n\n    # scale points by their depth value\n    return d[:, None] * unscaled_points","sha1":"6d8b1f29fdd8997e5e003ea85c0920b791853fc1","id":79309}
{"content":"def get_course_content(database, branch_ids):\n    \"\"\"Get course content for each branch id.\"\"\"\n    content = {}\n\n    for branch_id in branch_ids:\n        collection = 'modulestore.structures'\n        query = {\"_id\": branch_id}\n        cursor = database[collection].find(query)\n\n        for result in cursor:\n            content[branch_id] = result\n\n    return content","sha1":"678f90ada05c092f82d01b1fbab7b03de718d297","id":438174}
{"content":"from typing import Dict\n\n\ndef find_security(content: Dict, scheme: str) -> bool:\n    \"\"\"Check if security scheme is used in the provided content.\n\n    Arguments\n    ---------\n    content\n        OpenAPI document to be cleaned up.\n    scheme\n        Security scheme to be searched.\n\n    Returns\n    -------\n    Flag determining presence of the security scheme in the content.\n\n    \"\"\"\n    if isinstance(content, list):\n        for item in content:\n            if find_security(item, scheme):\n                return True\n    if isinstance(content, dict):\n        for key, value in content.items():\n            if key == 'security':\n                for security in value:\n                    if isinstance(security, dict) and scheme in security:\n                        return True\n            if find_security(value, scheme):\n                return True\n    return False","sha1":"0fe2485893fbf520cce10c7002346792bdd541e1","id":84972}
{"content":"import requests\n\n\ndef fetch_json(uri):\n    \"\"\"Perform an HTTP GET on the given uri, return the results as json.\n\n    If there is an error fetching the data, raise an exception.\n\n    Args:\n        uri: the string URI to fetch.\n\n    Returns:\n        A JSON object with the response.\n\n    \"\"\"\n    data = requests.get(uri)\n    # Raise an exception if the fetch failed.\n    data.raise_for_status()\n    return data.json()","sha1":"d42cf52a43e9a06e49b0ede3cae69e074b2cbae9","id":686211}
{"content":"def determined(problem):\n    \"\"\"\n    Check if the problem has all cells set to a colour. Returns\n    True\/False accordingly\n\n    To actually assess whether the problem is satisfactorily\n    solved use evaluate_state(problem)\n    \"\"\"\n    return all(len(cell) == 1 for cell in problem['variables'].values())","sha1":"eed73ce906cfa93bbc76b788fbd9d6b00cd08c8f","id":144505}
{"content":"def format_fing(fingerprint):\n    \"\"\"\n    Format a fingerprint by capitalizing it and adding spaces every\n    four characters.\n\n    >>> format_fing('abc123def456ghi789jkl012mno345pqr678stu9')\n    'ABC1 23DE F456 GHI7 89JK L012 MNO3 45PQ R678 STU9'\n\n    @type fingerprint: C{string} or C{buffer}\n    @param fingerprint: The 40-character fingerprint.\n    @rtype: C{string}\n    @return: The capitalized fingerprint with spaces every four\n    characters.\n    \"\"\"\n    fingerprint_list = [str(fingerprint)[i:(i + 4)] for i in\n            range(0, 40, 4)]\n    new_fingerprint = \" \".join(fingerprint_list)\n\n    return new_fingerprint.upper()","sha1":"52269d9f3bc93b90e184d20288613213d250f317","id":211218}
{"content":"import base64\n\n\ndef get_base64_hash_digest_string(hash_object):\n  \"\"\"Takes hashlib object and returns base64-encoded digest as string.\"\"\"\n  return base64.b64encode(hash_object.digest()).decode(encoding='utf-8')","sha1":"f8d0b6474f45ecaa0a1d564376db68c8852b9303","id":92210}
{"content":"def delete_index(es, index):\n    \"\"\"\n    Delete Elasticsearch index.\n    \"\"\"\n    return es.indices.delete(index)","sha1":"178570c82a7a9cb5d2bc3dcc136454997b967b09","id":149481}
{"content":"import torch\n\n\ndef prepare_target(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\" Convert the target mask to long and remove the channel dimension. \"\"\"\n    return tensor.long().squeeze(1)","sha1":"9b4343b43db2ab5a2df10352bcce7238abd56139","id":610248}
{"content":"def parse_tva_id_Descriptor(data,i,length,end):\n    \"\"\"\\\n    parse_tva_id_Descriptor(data,i,length,end) -> dict(parsed descriptor elements).\n    \n    Parses a descriptor that carries one or more TVA IDs along with associated state for each.\n    The returned dict is:\n        { \"type\" : \"tva_ids\",\n          \"ids\" : [\n            { \"id\" : number, \"running_status\", number },\n            { \"id\" : number, \"running_status\", number },\n            ...\n          ]\n        }\n\n    (Defined in ETSI TS 102 323 specification)\n    \"\"\"\n    ids = []\n    n=0\n    while n < length:\n        id = (ord(data[i+2+n]) << 8) + ord(data[i+3+n])\n        rs = ord(data[i+4+n]) & 0x7\n        ids.append({\"id\":id, \"running_status\":rs})\n        n=n+3\n    return { \"type\":\"tva_ids\", \"ids\" : ids }","sha1":"39c62b0430c6dfb6a37a66dda61fb94bdee73ebd","id":145400}
{"content":"def get_document_and_text(data_dic,data_index,document_id):\n\t\"\"\" return the document name and its text from its id and the output of read_file\n\t\"\"\"\n\tdocument_name = data_index[int(document_id)]\n\treturn document_name,data_dic[document_name]['text']","sha1":"a94c916021cc2ca0348f57ecf9237833617cde28","id":362243}
{"content":"import uuid\n\n\ndef mock_user_pool(mock_cognitoidp_client):\n    \"\"\"Create a mocked cognito user pool used to instantiate a mock AWS Connector\n\n    Args:\n        mock_cognitoidp_client: a mocked Cognito IDP client\n\n    Returns:\n        dict: containing a mocked user pool id (key: UserPoolId) and a mocked user pool client id (key: UserPoolClientId)\n\n    \"\"\"\n    name = str(uuid.uuid4())\n    value = str(uuid.uuid4())\n    result_user_pool = mock_cognitoidp_client.create_user_pool(PoolName=name, LambdaConfig={\"PreSignUp\": value})\n\n    client_name = str(uuid.uuid4())\n    value = str(uuid.uuid4())\n    user_pool_id = result_user_pool[\"UserPool\"][\"Id\"]\n    result_user_pool_client = mock_cognitoidp_client.create_user_pool_client(\n        UserPoolId=user_pool_id,\n        ClientName=client_name,\n        CallbackURLs=[value]\n    )\n    return {\n        \"UserPoolId\": user_pool_id,\n        \"UserPoolClientId\": result_user_pool_client[\"UserPoolClient\"][\"ClientId\"],\n    }","sha1":"4fc880e68c7d65001a6958ce44c10cf1fdb46a01","id":114882}
{"content":"def collatz_even(number):\n    \"\"\"Return n\/2 for given number.\"\"\"\n    return number \/ 2","sha1":"a7e0b6dfa12189d9f6f1af09db6d03a51f3ec63d","id":412761}
{"content":"def raise_to_list(input):\n    \"\"\"\n    This will take an input and raise it to a List (if applicable). It will preserve None values as None.\n\n    Parameters\n    ----------\n    input : object\n        The object to raise to a list.\n\n    Returns\n    -------\n    list\n        The object as a list, or None.\n    \"\"\"\n    if input is None:\n        return None\n    elif isinstance(input, list):\n        return input\n    else:\n        return [input]","sha1":"c87ad7671d46cd08ce84aac595368217f1b04c5f","id":520041}
{"content":"def get_tuple(string, n, as_int = False):\n  \"\"\"\n  Splits a string into n tuples, ignoring the rest if there are more, and replicating\n  if there is only one\n\n  @param string   The string\n  @param n        The number of elements\n  @param as_int   If true, it will cast to int\n  \"\"\"\n  t = string.split(\",\")\n  if len(t) == n:\n    t = tuple(t)\n  elif len(t) > n:\n    t = tuple(t[:n - 1])\n  elif len(t) == 1:\n    t = tuple(t * n)\n  else:\n    raise IndexError(\"Invalid number of values\")\n\n  if as_int:\n    return tuple([int(i) for i in t])\n  else:\n    return t","sha1":"59b339c561bccc428f4e2882c369a7dcda86aaff","id":10948}
{"content":"def shaping_by_fields(fields: list) -> str:\n    \"\"\"\n    Creating a shaping for the request which is from the fields and seperated by comma's\n    Args:\n        fields: List of fields that would be part of the shaping.\n    Returns:\n        str of the shaping.\n    \"\"\"\n    shaping = 'hd_ticket all'\n    for field in fields:\n        shaping += f',{field} limited'\n    return shaping","sha1":"3312e6bcb6de890149fd1f45b3abdd22c6f1ab91","id":436164}
{"content":"def count_lines(filename):\n  \"\"\"\n  Counts the number of lines in the given file.\n  \"\"\"\n  n_lines = 0\n  with open(filename) as f:\n    for line in f:\n      n_lines += 1\n  return n_lines","sha1":"3e2d707b488a29512963223682ad418cce1aa70e","id":432359}
{"content":"def make_comeback_strategy(margin, num_rolls=5):\n    \"\"\"Return a strategy that rolls one extra time when losing by MARGIN.\"\"\"\n\n    def s1(score,opponent_score):\n        if ((opponent_score - score) >= margin):\n            return num_rolls+1\n        else:\n            return num_rolls\n    return s1","sha1":"bf344a1188e77801269c4722a79dc7635f2e260c","id":363542}
{"content":"def intersect_line_ray(lineSeg, raySeg):\n    \"\"\" Constructs a line from the start and end points of a given line \n    segment, and finds the intersection between that line and a ray \n    constructed from the start and end points of a given ray segment.\n    \n    If there is no intersection (i.e. the ray goes in the opposite direction \n    or the ray is parallel to the line), returns None.\n    \n    \"\"\"\n    \n    lineStart, lineEnd = lineSeg\n    rayStart, rayEnd = raySeg\n    \n    lineVector = (lineEnd[0] - lineStart[0], lineEnd[1] - lineStart[1])\n    rayVector = (rayEnd[0] - rayStart[0], rayEnd[1] - rayStart[1])\n    \n    p1x, p1y = lineStart\n    p2x, p2y = rayStart\n    \n    d1x, d1y = lineVector\n    d2x, d2y = rayVector\n    \n    # Check if the ray is parallel to the line.\n    parallel = (\n        (d1x == 0 and d2x == 0)\n        or ((d1x != 0 and d2x != 0) and\n            (float(d1y) \/ d1x == float(d2y) \/ d2x))\n    )\n    \n    intersection = None\n    \n    # Only non-parallel lines can ever intersect.\n    if not parallel:\n        # Parametrize the line and ray to find the intersection.\n        parameter = (\n            float(p2y * d1x - p1y * d1x - p2x * d1y + p1x * d1y)\n            \/ (d2x * d1y - d1x * d2y)\n        )\n        \n        # Only consider intersections that occur in front of the ray.\n        if parameter >= 0:\n            intersection = (\n                p2x + parameter * d2x,\n                p2y + parameter * d2y,\n            )\n            \n    return intersection","sha1":"365836039161666eb30d0051916dceb7260f3c19","id":90062}
{"content":"def parse_summary(line):\n    \"\"\"get the summary of an SSH failure from an SSH log event line\"\"\"\n    return line.split(' sshd[', 1)[-1].split(' ', 1)[-1][:40] + '...'","sha1":"f0f27779fb464c65bcfedc142069e35a2ea44915","id":335920}
{"content":"def get_tool_id(tool_id):\n    \"\"\"\n    Convert ``toolshed.g2.bx.psu.edu\/repos\/devteam\/column_maker\/Add_a_column1\/1.1.0``\n    to ``Add_a_column``\n\n    :param str tool_id: a tool id, can be the short kind (e.g. upload1) or the long kind with the full TS path.\n\n    :returns: a short tool ID.\n    :rtype: str\n    \"\"\"\n    if tool_id.count('\/') == 0:\n        # E.g. upload1, etc.\n        return tool_id\n\n    # what about odd ones.\n    if tool_id.count('\/') == 5:\n        (server, _, owner, repo, name, version) = tool_id.split('\/')\n        return name\n\n    return tool_id","sha1":"ddac4527623f99f18d7a048cb94e17e2edfc8800","id":228816}
{"content":"def compute_confidence_intervals(param_estimate, std_dev, critical_value):\n    \"\"\"Compute confidence intervals (ci). Note assumptions about the distributions\n    apply.\n\n    Parameters\n    ----------\n    param_estimate: float\n        Parameter estimate for which ci should be computed\n    variance: float\n        Variance of parameter estimate.\n    critical_value: float\n        Critical value of the t distribution, e.g. for the 95-percent-ci it's 1.96.\n\n    Returns\n    -------\n    confidence_interval_dict: dict\n        Lower (upper) bound of the ci can be accessed by the key 'lower_bound'\n        ('upper_bound').\n\n    \"\"\"\n    confidence_interval_dict = {}\n    confidence_interval_dict[\"lower_bound\"] = param_estimate - critical_value * std_dev\n    confidence_interval_dict[\"upper_bound\"] = param_estimate + critical_value * std_dev\n    return confidence_interval_dict","sha1":"651f8430f595b9c67d039dd8898cdc848e9b9b75","id":312170}
{"content":"import six\n\n\ndef scrub_tags(tags):\n    \"\"\"\n    Ensure that layer tags pass in as a string are formatted properly.\n    \"\"\"\n    if isinstance(tags, six.string_types):\n        tags = [tag.strip() for tag in tags.split(\"|\")\n                if tag.strip().isalnum()]\n    return \" | \".join(tags)","sha1":"deb92172414bde96261be7ff5568d8a679abc7b7","id":278775}
{"content":"def inol_percent(int_reps, flt_inol):\n    \"\"\"\n    Gives percent max of a lift using INOl\n    :param int_reps: number of reps\n    :param flt_inol: INOL score\n    :return: percent of max that should be used (float)\n    \"\"\"\n    return float(-(int_reps \/ flt_inol - 100)) \/ 100","sha1":"06239bc5dafc0188ad46112e7e89ab5304303b99","id":551041}
{"content":"def get_prefix(string, split):\n    \"\"\"\n    Returns the prefix of the given string\n    :param string: str, string to get prefix of\n    :param split: str, split character\n    :return: str\n    \"\"\"\n    return string.split(split)[0]","sha1":"92748d6a2990f0b4de9e2fb7c98a2c41ca108f24","id":322014}
{"content":"def find_cal_indices(epoch_time):\n    \"\"\"\n    Cal events are any time a standard is injected and being quantified by the system. Here, they're separated as though\n    any calibration data that's more than 10s away from the previous cal data is a new event.\n\n    :param epoch_time: array of epoch times for all of the supplied data\n    :return: list of cal events indices, where each index is the beginning of a new cal event\n    \"\"\"\n    epoch_diff = epoch_time.diff()\n    indices = [i - 1 for i in epoch_diff.loc[epoch_diff > 10].index.values.tolist()]  # subtract one from all indices\n    return indices","sha1":"323a3a62f5ec7745883490bd8f1fa8ccc8ac0112","id":484706}
{"content":"def is_control_gate(op_type: str) -> bool:\n    \"\"\"Get whether a gate type includes a control (q)bit.\"\"\"\n    return op_type in {\n        \"CX\",\n        \"CY\",\n        \"CZ\",\n        \"CH\",\n        \"CRx\",\n        \"CRy\",\n        \"CRz\",\n        \"CU1\",\n        \"CU3\",\n        \"CV\",\n        \"CVdg\",\n        \"CSx\",\n        \"CSXdg\",\n        \"CSWAP\",\n        \"CnRy\",\n        \"CnX\",\n        \"CCX\",\n        \"Control\",\n        \"QControlBox\",\n        \"Conditional\",\n    }","sha1":"a26fa5175fbfa48495e72d7f60548bccb093acf5","id":22834}
{"content":"def rectified_disparity_to_depth(fx, B, disparity):\n    \"\"\"Convert a rectified disparity map to depthmap.\n\n    Assumes left\/right cameras are rectified.\n\n    fx: Horizontal camera focal length (batch, 1, rows, cols).\n    B: Baseline between the left and right camers (batch, 1, rows, cols)\n    \"\"\"\n    eps = 1e-7\n    depthmap = fx * B \/ (disparity + eps)\n    return depthmap","sha1":"488b9e3e0aba3259d267cc6d11ecfc794ee4e51c","id":494047}
{"content":"def fun(x, y):\n    \"\"\"\n    Simple function that sums two numbers\n    :param x: first input number\n    :param y: second input number\n    :return: sum of the two numbers\n    \"\"\"\n    return x+y","sha1":"77f01edf6f410698ee9b692f98e2f1995866f08c","id":176462}
{"content":"def group(lst, count):\n    \"\"\"Group a list into consecutive count-tuples. Incomplete tuples are\n    discarded.\n\n    `group([0,3,4,10,2,3], 2) => [(0,3), (4,10), (2,3)]`\n\n    From: http:\/\/aspn.activestate.com\/ASPN\/Cookbook\/Python\/Recipe\/303060\n    \"\"\"\n    return list(zip(*[lst[i::count] for i in range(count)]))","sha1":"42cadd27487d8cd90c22158860a13e04cb8d1fc8","id":443154}
{"content":"def usernameslist(data):\n    \"\"\"\n    Returns list of usernames of all users.\n    \"\"\"\n    return [u['name'] for u in data['users']]","sha1":"6efd89d2c63af074ebf2a7133a8680be40c08679","id":644517}
{"content":"from typing import Callable\nfrom typing import Sequence\nimport inspect\n\n\ndef get_decorators(function: Callable) -> Sequence[str]:\n    \"\"\"Returns list of decorators names\n\n    Args:\n        function (Callable): decorated method\/function\n\n    Return:\n        List of decorators as strings\n\n    Example:\n        Given:\n\n        @my_decorator\n        @another_decorator\n        def decorated_function():\n            pass\n\n        >>> get_decorators(decorated_function)\n        ['@my_decorator', '@another_decorator']\n\n    \"\"\"\n    source = inspect.getsource(function)\n    index = source.find(\"def \")\n    return [\n        line.strip().split()[0]\n        for line in source[:index].strip().splitlines()\n        if line.strip()[0] == \"@\"\n    ]","sha1":"502add8a64f92152ed6ccc44008709e6a3b40a2b","id":368875}
{"content":"def calculate_density(x):\n    \"\"\"\n    This function calculates the density of the number of accidents by the area of the SA2 area\n    :param x: geopandas geodataframe\n    :return: float density value\n    \"\"\"\n    return x['NUM_ACCIDENTS'] \/ x['AREASQKM16']","sha1":"9eef186c603cdda33c05e037c1908e4dde9d8adb","id":602429}
{"content":"def interactive_strategy(game) -> int:\n    \"\"\"\n    Return a move for game through interactively asking the user for input.\n    \"\"\"\n    move = input(\"Enter a move: \")\n    return int(move)","sha1":"c2327d241f85ce69997655d0d414cccecd0fc512","id":270985}
{"content":"def flatten_errors(error):\n    \"\"\"\n    Django's ValidationError contains nested ValidationErrors, which each have a list\n    of errors, so we need to flatten them.\n    \"\"\"\n    return {k: [item for items in v for item in items] for k, v in error.items()}","sha1":"68d7b22ace6cc9beb44572df98b9f5041f6902ca","id":156830}
{"content":"import csv\n\n\ndef write_page_report(out_path, history):\n    \"\"\"\n    Write out page report from given path and data.\n\n    Rows which contain duplicate URLs will be skipped. The data is expected to\n    be sorted already so duplicates are removed by comparing consecutive rows.\n\n    :param out_path: Path of CSV to write page report to.\n    :param history: History data to use.\n\n    :return: Count of rows written out.\n    \"\"\"\n    header = (\n        'year_month',\n        'timestamp',\n        'domain',\n        'path',\n        'query',\n        'fragment',\n        'title',\n        'full_url',\n    )\n\n    with open(out_path, 'w') as f_out:\n        writer = csv.DictWriter(f_out, fieldnames=header)\n        writer.writeheader()\n\n        wrote_count = 0\n        # Skip rows which contain a duplicate of the previous row's URL.\n        # We've sorted by URL and then timestamp\n        previous_row = None\n        for row in history:\n            if previous_row is None or row['full_url'] != previous_row['full_url']:\n                writer.writerow(row)\n                wrote_count += 1\n            previous_row = row\n\n    return wrote_count","sha1":"132c9a4eafbadfe1325c7df82eec0ae1d2de9ea0","id":131632}
{"content":"import six\n\n\ndef parse_methods(root, methods=None):\n    \"\"\"Parses methods from the given Discovery document.\n\n    Args:\n        root (dict): A Discovery document. When called recursively, this is a\n            resource within a Discovery document.\n        methods (dict): A mapping of method ID to Discovery method. Do not set,\n            this is used to collect method IDs while recursing.\n\n    Returns:\n        dict: A mapping of method ID to method.\n    \"\"\"\n    if methods is None:\n        methods = {}\n    for method in six.itervalues(root.get('methods', {})):\n        id_ = method['id']\n        methods[id_] = method\n    for resource in six.itervalues(root.get('resources', {})):\n        parse_methods(resource, methods)\n    return methods","sha1":"d120a0851d20d2d4f5f2d219955472efed69d511","id":127435}
{"content":"def get_openlibrary_key(key):\n    \"\"\"convert \/books\/OL27320736M into OL27320736M\"\"\"\n    return key.split(\"\/\")[-1]","sha1":"b4b266e8d79d9b5cd4401439835eff1175ae4d7c","id":648184}
{"content":"import operator\n\n\ndef _make_encode_wrapper(reference):\n    \"\"\"Create a function that will be called with a string argument. If\n    the reference is bytes, values will be encoded to bytes.\n    \"\"\"\n    if isinstance(reference, str):\n        return lambda x: x\n\n    return operator.methodcaller(\"encode\", \"latin1\")","sha1":"382cef507da56753a28d70b083002729978860e1","id":509048}
{"content":"from pathlib import Path\n\n\ndef _run_jlab_string(talktorials_dst_dir):\n    \"\"\"\n    Print command for starting JupyterLab from workspace folder.\n\n    Parameters\n    ----------\n    talktorials_dst_dir : str or pathlib.Path\n        Path to directory containing the talktorial folders.\n    \"\"\"\n\n    talktorials_dst_dir = Path(talktorials_dst_dir)\n\n    message = f\"\"\"\nTo start working with the talktorials in JupyterLab run:\n\n    jupyter lab {talktorials_dst_dir}\n\nEnjoy!\n\"\"\"\n\n    return message","sha1":"76c45f77f419b6a302d63d92e51cd0ea1bb92ebb","id":7869}
{"content":"import json\n\n\ndef load_json_from_string(string):\n    \"\"\"Load schema from JSON string\"\"\"\n    try:\n        json_data = json.loads(string)\n    except ValueError as e:\n        raise ValueError('Given string is not valid JSON: {}'.format(e))\n    else:\n        return json_data","sha1":"66f96373a8e02bf69289e5e4594ac319906475f5","id":5839}
{"content":"def parse_sql_condition(cohort_ids, where_condition=False):\n    \"\"\" Parse the sql condition to insert in another sql statement.\n    \"\"\"\n    return f\"\"\"{\"WHERE\" if where_condition else \"AND\"} id IN {cohort_ids}\"\"\" \\\n        if cohort_ids else \"\"","sha1":"b7c60fe7e3d34f330b6709fb5ce83eb8d00de59f","id":434262}
{"content":"def merge_into(s1, s2, keys):\n  \"\"\"Merge 2 namedtuples. Copy keys from s2 into s1.\"\"\"\n  dd = {}\n  for k in s1._fields:\n    dd[k] = getattr(s1, k)\n\n  for k in keys:\n    dd[k] = getattr(s2, k)\n  return s1.__class__(**dd)","sha1":"bd9b61b5c70162aa972cd57111ebccc0e9aa9087","id":427445}
{"content":"def inpoly(xv, yv, xt, yt):\n    \"\"\"\n    Originally in C by Bob Stein and Craig Yap\n    http:\/\/www.visibone.com\/inpoly\/\n    Converted to python by Bryan Miller\n    2013.04.18\n    Inputs:\n      xv - x vertices of polygon (does not have to be 'closed', ie. last = first)\n      yv - y vertices of polygon\n      xt - x of test point(s)\n      yt - y of test point(s)\n\n    # 2016.06.25 - generalize to handle input arrays\n    \"\"\"\n\n    nvert = len(xv)\n    if nvert != len(yv) or nvert < 3:\n        return -1\n\n    l_xt = xt\n    l_yt = yt\n\n    try:\n        npoints = len(l_xt)\n    except Exception:\n        l_xt = [l_xt]\n        npoints = len(l_xt)\n    try:\n        npointsy = len(l_yt)\n    except Exception:\n        l_yt = [l_yt]\n        npointsy = len(l_yt)\n\n    if npoints != npointsy:\n        return -1\n\n    inside = [False for ii in range(npoints)]\n\n    for jj in range(npoints):\n        xold = xv[nvert-1]\n        yold = yv[nvert-1]\n        for i in range(nvert):\n            xnew = xv[i]\n            ynew = yv[i]\n            if xnew > xold:\n                x1 = xold\n                x2 = xnew\n                y1 = yold\n                y2 = ynew\n            else:\n                x1 = xnew\n                x2 = xold\n                y1 = ynew\n                y2 = yold\n    #        \/* edge \"open\" at one end *\/\n            if (xnew < l_xt[jj]) == (l_xt[jj] <= xold) and (l_yt[jj]-y1)*(x2-x1) < ((y2-y1)*(l_xt[jj]-x1)):\n                inside[jj] = not inside[jj]\n            xold = xnew\n            yold = ynew\n\n    if npoints == 1:\n        inside = inside[0]\n\n    return inside","sha1":"bc6f847813620c5ad92a59a56e1d8f643b6d82eb","id":608246}
{"content":"import torch\n\n\ndef select_device() -> torch.device:\n    \"\"\"Select a device.\"\"\"\n    if torch.cuda.is_available():\n        return torch.device(f\"cuda:{torch.cuda.current_device()}\")\n    else:\n        return torch.device(\"cpu\")","sha1":"e431d919da2252ffd26b72aeb9088d29a9d52171","id":237622}
{"content":"def remove_duplicates(duplist):\n    \"\"\"remove all duplicates from a list, so that only one of each item remains and output a list\"\"\"\n\n    nondup_list = list(set(duplist))\n\n    return nondup_list","sha1":"0f78f897e3e829734e60dd95828af24f6898957e","id":55059}
{"content":"def str2rsi(key):\n    \"\"\"\n    Convert a string of the form 'rlz-XXXX\/sid-YYYY\/ZZZ'\n    into a triple (XXXX, YYYY, ZZZ)\n    \"\"\"\n    rlzi, sid, imt = key.split('\/')\n    return int(rlzi[4:]), int(sid[4:]), imt","sha1":"a2af2cda1c14e1527d53d4990eab0241099d803f","id":463616}
{"content":"def find_sublcasses(cls):\n    \"\"\" Finds all known (imported) subclasses of the given class \"\"\"\n    cmds = []\n    for subclass in cls.__subclasses__():\n        cmds.append(subclass)\n        cmds.extend(find_sublcasses(subclass))\n    return cmds","sha1":"938f08b5ff3782333af4fdc1b0a4f8944369cb30","id":553055}
{"content":"import ast\n\n\ndef get_literal_tuple(param_type):\n    \"\"\"\n    get literal option from annotation string and return result in tuple.\n    \"\"\"\n    s_index: int = str(param_type).index('[')\n    e_index: int = str(param_type).index(']')\n    str_list = str(param_type)[s_index + 1:e_index]\n\n    res = ast.literal_eval(str_list)\n    if res:\n        return tuple(res)\n\n    return None","sha1":"8ee28a06fa10bc5ed8ac524c15bbd0bef0d2e1df","id":645955}
{"content":"def find(view, start_point, selector, forward=True):\n    \"\"\"Given a View, a start point, and a selector, return the first point\n    to the right of the start point that matches the selector.\n\n    If `forward=False`, walk left instead.\"\"\"\n    point = start_point if forward else start_point - 1\n    max_size = view.size()\n\n    while point >= 0 and point <= max_size:\n        if view.match_selector(point, selector):\n            return point\n\n        if forward:\n            point += 1\n        else:\n            point -= 1\n\n    return -1","sha1":"125ace54152a80bd4cb94a11b9289e97d05c92ee","id":490999}
{"content":"def gen_index_name(index_list):\n    \"\"\"Generate an index name based on the list of keys with directions.\"\"\"\n\n    return '_'.join('_'.join([str(i) for i in ix]) for ix in index_list)","sha1":"af0f091fdfee69c37e7c4b265a8fd6a4df51a846","id":254510}
{"content":"def _max_full_form_prefix_length(full_form, lemma, min_rule_length):\n    \"\"\"Return the maximum possible length of the prefix (part of the full form preceding the suffix).\"\"\"\n    full_form_prefix = len(full_form) - min_rule_length\n    return min(len(lemma), full_form_prefix)","sha1":"b9ee8c850f41c69019bfed6e827d83fe90e416e5","id":560482}
{"content":"import torch\n\n\ndef sumto1(v, dim=None, axis=None, keepdim=True):\n    \"\"\"\n    Make v sum to 1 across dim, i.e., make dim conditioned on the rest.\n    dim can be a tuple.\n\n    :param v: tensor.\n    :param dim: dimensions to be conditioned upon the rest.\n    :param axis: if given, overrides dim.\n    :return: tensor of the same shape as v.\n    :rtype: torch.Tensor\n    \"\"\"\n    if axis is not None:\n        dim = axis\n    if dim is None:\n        return v \/ torch.sum(v)\n    else:\n        return v \/ torch.sum(v, dim, keepdim=keepdim)","sha1":"5f71cc42f3f285c4acbfb1fa15492cfce83ee764","id":251287}
{"content":"import random\n\n\ndef pick_random(prob=0.5):\n    \"\"\"Return True if given prob > random\"\"\"\n    if random.random() < prob:\n        return True\n    return False","sha1":"9e6850ef94fd3b2089c2a3f438490b05aa8b11b8","id":452063}
{"content":"def assemble_subpeak_record(subpeak, celltype_activations, sequence):\n    \"\"\" Assemble the FASTA record of sequence and activation \"\"\"\n    # make the header\n    header ='\\t'.join([s for s in subpeak])\n    header = '>' + header\n\n    # make the activation string\n    activation = ';'.join([ct+' '+str(score) for (ct,score) in celltype_activations])\n\n    # append the sequence\n    seq_string = str(sequence)\n    return header, activation, seq_string","sha1":"5bddb16130dba20bb9d1c4f341d842371a71838a","id":684125}
{"content":"def flip(d):\n    \"\"\"Flip direction d : left -> right, right -> left\"\"\"\n    return not d","sha1":"a36061b7d6a05d78c90b0891942ce3789c557ee4","id":465419}
{"content":"def incremental_search(f, a, b, dx):\n    \"\"\"\n    Searches the interval (a,b) in increments dx for\n    the bounds (x1,x2) of the smallest root of f(x)\n\n    Parameters\n    ----------\n        f  - function\n        a  - left bound of the interval, (a, b)\n        b  - right bound of the interval, (a, b)\n        dx - value by which to increment\n\n    Returns\n    -------\n    x1 = x2 = None if no roots were detected\n    \"\"\"\n    x1 = a\n    x2 = x1 + dx\n\n    f1 = f(x1)\n    f2 = f(x2)\n\n    while (f1*f2) > 0:\n        if x1 >= b:\n            return None, None\n\n        x1 = x2\n        x2 = x1 + dx\n\n        f1 = f2\n        f2 = f(x2)\n\n    return x1, x2","sha1":"d75d54e7842459339540a8faa650da046d48bc3d","id":272476}
{"content":"def GetUserDomain(user_email):\n  \"\"\"Retrieve the domain of a user from an email address.\n\n  Args:\n    user_email: String email address of the form user@domain.com.\n\n  Returns:\n    String domain of the user.\n  \"\"\"\n  return user_email.split('@')[1]","sha1":"7b8ab3fc7ea50e5f0c14076016a3feca8f2204d1","id":394451}
{"content":"import glob\n\n\ndef found_with_glob(value):\n    \"\"\"\n    Check if at least one file match the given glob expression.\n    :param value: glob expression\n    \"\"\"\n    for _ in glob.glob(value):\n        return True\n    return False","sha1":"38677a00e2c80981baa8d1d27ad0bec2b8df009f","id":675610}
{"content":"def drop_msg(module_name, module_package):\n    \"\"\" Create info message for dropped modules.\n    \"\"\"\n    if module_package is not None:\n        ignore_msg = \"drop %s (in %s)\" % (module_name, module_package)\n    else:\n        ignore_msg = \"drop %s\" % module_name\n    return ignore_msg","sha1":"e56461cbb6a195babd3d8038654df1d3ffb57bbe","id":648721}
{"content":"def valid_offset(offset, ubound=None):\n    \"\"\"Given a user-provided offset, return a valid int, or raise.\"\"\"\n    offset = int(offset)\n    assert offset >= -1, \"offset cannot be negative\"\n    if ubound is not None:\n        assert offset <= ubound, \"offset too large\"\n    return offset","sha1":"c774f6f5a4ccca6546f0c3c7708f0ef744f987f2","id":206584}
{"content":"def parse_swift_ring_builder(ring_builder_output):\n    \"\"\"Parse the supplied output into a dictionary of swift ring data.\n    Args:\n        ring_builder_output (str): The output from the swift-ring-builder\n                                   command.\n    Returns:\n        dict of {str: float}: Swift ring data. Empty dictionary if parse fails.\n\n    Example data:\n        {'zones': 1.0,\n         'replicas': 3.0,\n         'devices': 9.0,\n         'regions': 1.0,\n         'dispersion': 0.0,\n         'balance': 0.78,\n         'partitions': 256.0}\n    \"\"\"\n\n    swift_data = {}\n    swift_lines = ring_builder_output.split('\\n')\n    matching = [s for s in swift_lines if \"partitions\" and \"dispersion\" in s]\n    if matching:\n        elements = [s.strip() for s in matching[0].split(',')]\n        for element in elements:\n            v, k = element.split(' ')\n            swift_data[k] = float(v)\n\n    return swift_data","sha1":"8937af55abd8d26337992b2563983f5ce3f9dc2f","id":271020}
{"content":"import struct\n\n\ndef calcChecksum(data):\n\t\"\"\"Calculate the checksum for an arbitrary block of data.\n\tOptionally takes a 'start' argument, which allows you to\n\tcalculate a checksum in chunks by feeding it a previous\n\tresult.\n\t\n\tIf the data length is not a multiple of four, it assumes\n\tit is to be padded with null byte. \n\n\t\t>>> print calcChecksum(b\"abcd\")\n\t\t1633837924\n\t\t>>> print calcChecksum(b\"abcdxyz\")\n\t\t3655064932\n\t\"\"\"\n\tremainder = len(data) % 4\n\tif remainder:\n\t\tdata += b\"\\0\" * (4 - remainder)\n\tvalue = 0\n\tblockSize = 4096\n\tassert blockSize % 4 == 0\n\tfor i in range(0, len(data), blockSize):\n\t\tblock = data[i:i+blockSize]\n\t\tlongs = struct.unpack(\">%dL\" % (len(block) \/\/ 4), block)\n\t\tvalue = (value + sum(longs)) & 0xffffffff\n\treturn value","sha1":"7678edec74457c74123ac3de1d1fe83008f67f31","id":512074}
{"content":"import jinja2\n\n\ndef render(path, variables={}):\n    \"\"\"Render the given template using the given variables.\"\"\"\n    env = jinja2.Environment(\n        loader=jinja2.PackageLoader('gutenberg'),\n        trim_blocks=True,\n        lstrip_blocks=True)\n\n    template = env.get_template(path)\n    return template.render(variables)","sha1":"a0f7fbe1a8165c99264ba93834df40077b6ba8b6","id":59803}
{"content":"def pack(obj):\n  \"\"\"\n  Takes an object and packs it into a JSONable form consisting purely of\n  dictionaries, lists, numbers, strings, booleans, and Nones. If the object has\n  a `_pack_` method, that will be called without arguments and the result\n  returned, otherwise if it's one of the above types it'll be returned directly\n  (or after having its contents packed).\n  \"\"\"\n  if hasattr(obj, \"_pack_\"):\n    return obj._pack_()\n  elif isinstance(obj, (list, tuple)):\n    return [ pack(o) for o in obj ]\n  elif isinstance(obj, dict):\n    return {\n      pack(k): pack(v)\n        for (k, v) in obj.items()\n    }\n  elif isinstance(obj, (int, float, str, bool, type(None))):\n    return obj\n  else:\n    raise ValueError(\n      \"Cannot pack value '{}' of type {}.\".format(\n        repr(obj),\n        type(obj)\n      )\n    )\n    return None","sha1":"4d909128a2cabb6e01d854545f3dcffcc394a14c","id":475227}
{"content":"def all_ones(slice):\n    \"\"\"\n    Given a slice of an array, return whether the slice is all ones.\n    >>> arr = np.ones((3,3))\n    >>> all_ones(arr[:,:])\n    True\n    >>> arr = np.zeros((3,3))\n    >>> all_ones(arr[:,:])\n    False\n    \"\"\"\n    return slice.all()","sha1":"09c529bb9bdac3cca0859f4836e8135377316481","id":186208}
{"content":"def find_faces_at_vertices(faces, npoints):\n    \"\"\"\n    For each vertex, find all faces containing this vertex.\n    Note: faces do not have to be triangles.\n\n    Parameters\n    ----------\n    faces : list of lists of three integers\n        the integers for each face are indices to vertices, starting from zero\n    npoints: integer\n        number of vertices on the mesh\n\n    Returns\n    -------\n    faces_at_vertices : list of lists of integers\n        faces_at_vertices[i] is a list of faces that contain the i-th vertex\n\n    Examples\n    --------\n    >>> # Simple example:\n    >>> from mindboggle.guts.mesh import find_faces_at_vertices\n    >>> faces = [[0,1,2],[0,2,3],[0,3,4],[0,1,4],[4,3,1]]\n    >>> npoints = 5\n    >>> find_faces_at_vertices(faces, npoints)\n    [[0, 1, 2, 3], [0, 3, 4], [0, 1], [1, 2, 4], [2, 3, 4]]\n\n    \"\"\"\n    faces_at_vertices = [[] for i in range(npoints)]\n    for face_id, face in enumerate(faces):\n        for vertex in face:\n           faces_at_vertices[vertex].append(face_id)\n\n    return faces_at_vertices","sha1":"30bd730631e09653e0bcf5ffba98400f123559b2","id":132974}
{"content":"import textwrap\n\n\ndef build_raw_influx_query(field, measurement, function, filters, group_by_time=\"$__interval\", fill=\"null\") -> str:\n    \"\"\"\n    Build a raw InfluxDB query.\n    \"\"\"\n    copied_filters = filters.copy()\n    copied_filters.append(\"$timeFilter\")\n    string_filters = \" AND \".join(copied_filters)\n    query = f\"\"\"\n    SELECT {function}(\"{field}\")\n    FROM \"{measurement}\"\n    WHERE {string_filters} GROUP BY time({group_by_time}) fill({fill})\n    \"\"\"\n\n    return textwrap.dedent(query).replace(\"\\n\", \" \").strip()","sha1":"7eadcaecf4b1d9d8bd904b7a4271656fd55d987e","id":172474}
{"content":"def int_2_bool(value):\n    \"\"\"Converts an integer, eg. 0, 1, 11, 123 to boolean\n\n    Args:\n        value (int)\n\n    Returns:\n        False for value: 0\n        True otherwise\n    \"\"\"\n    if isinstance(value, int):\n        return (bool(value))\n    else:\n        return False","sha1":"00e52bf90d04628909a22cb9279a355e27a414b1","id":261850}
{"content":"def number_axles(num_axles):\n    \"\"\"Numbers the axles starting with 1.\"\"\"\n    axle_num = []\n\n    for i in range(num_axles):\n        axle_num.append(i+1)\n\n    return axle_num","sha1":"b4bd09151e84968951cb3227862282897c007005","id":564134}
{"content":"import struct\n\n\ndef data2stack(data):\n    \"\"\"\n    Convert binary ocotomap data into three lists (occupied, free, unknown)\n    :param data: binary octomap data (depth first)\n    :return: (occupied, free, unknown) lists of sequences\n    \"\"\"\n    stack = [[]]\n    unknown = []\n    free = []\n    occupied = []\n    for i in range(len(data) \/\/ 2):\n        prefix = stack.pop(0)\n        d = struct.unpack_from('<H', data, i * 2)[0]\n        for rot in range(14, -2, -2):  # range(0, 16, 2):\n            val = (d & (0x3 << rot)) >> rot\n            if val == 3:\n                stack.insert(0, prefix + [rot \/\/ 2])\n            elif val == 2:\n                occupied.append(prefix + [rot \/\/ 2])\n            elif val == 1:\n                free.append(prefix + [rot \/\/ 2])\n            elif val == 0:\n                unknown.append(prefix + [rot \/\/ 2])\n    assert len(stack) == 0, len(stack)\n    return occupied, free, unknown","sha1":"209bd92264f2b764d96287e90fd148bae7847bd0","id":455083}
{"content":"from pathlib import Path\n\n\ndef load_if_exists(filepath: Path) -> str:\n    \"\"\"If a file exists, return its contents, otherwise raise an error.\"\"\"\n    if not filepath.exists():\n        raise ValueError(f\"Could not locate source file: '{filepath}'\")\n\n    return filepath.read_text()","sha1":"62b77ed3dbba26c063129ab471879697162bc07e","id":188898}
{"content":"def get_obs2coords(obs):\n    \"\"\"Create an observation to coordinate xwalk.\n\n    Parameters\n    ----------\n    obs : tigernet.Observations\n\n    Returns\n    -------\n    o2c : dict\n        Observations to coordinates lookup.\n\n    \"\"\"\n\n    o2c = {\n        (ix, obs.df.loc[ix, obs.df_key]): (\n            obs.df.loc[ix, obs.geo_col].x,\n            obs.df.loc[ix, obs.geo_col].y,\n        )\n        for ix in obs.df.index\n    }\n\n    return o2c","sha1":"c3561b009741b75ba1b31ea4452a77954ca05a3f","id":395512}
{"content":"def repeat_str(repeat, string):\n    \"\"\"\n    Repeats a given string a specified amount of times.\n    :param repeat: integer of times to be repeated.\n    :param string: the string to be multiplied.\n    :return: the string repeated n times.\n    \"\"\"\n    return string * repeat","sha1":"7fadf06b400833addafd09d04461a974fb21078d","id":109806}
{"content":"def hamming_weight(x: int):\n    \"\"\"\n    Count the number of on bits in an integer.\n\n    Args:\n        x: the number to count on bits\n\n    Returns:\n        Integer representing the number of bits\n        that are on (1).\n    \"\"\"\n    count = 0\n    while x:\n        # bitwise AND number with itself minus 1\n        x &= x - 1\n        count += 1\n    return count","sha1":"5b4079a8cf0a66900e1bd21dc7d3fb7d1bc07f9a","id":390580}
{"content":"def combine_real_imag(real_data, imag_data):\n    \"\"\"Combines two float data arrays into one complex64 array\"\"\"\n    return real_data + 1j * imag_data","sha1":"a9f3eb9f7e6f2d2b70cca941a2c14802f4a5d069","id":101857}
{"content":"def key_exists(dictionary, key):\n    \"\"\"Tests if a key exists in a dictionary\n    \n    Arguments:\n        dictionary {dict} -- The dictionary to test\n        key {str} -- The key to test\n    \n    Returns:\n        bool -- `True` if the key exists in the dictionary\n    \"\"\"\n\n    exists = dictionary.get(key, None)\n    return exists is not None","sha1":"b1c350d979826159f081fe84dca5b63b77a9bce4","id":426257}
{"content":"def get_candidate_or_assignee(commentators, assignee):\n    \"\"\"If assignee post comments. At least half of most common commentator, return assignee.\n    Most common otherwise.\"\"\"\n\n    most_common, count = commentators.most_common(1)[0]\n    if commentators[assignee] \/ count >= 0.5:\n        return assignee\n    return most_common","sha1":"b9c672b337e7f95652d1807ac9f916088a7b76d6","id":346309}
{"content":"def validate_password(pw: str):\n    \"\"\"\n    Check if a password is valid and return a tuple of (bool, reason)\n    This function does not perform checks for strong passwords.\n    \"\"\"\n    pw = pw.strip() if pw is not None else \"\"\n    if pw == \"\":\n        return (False, \"Password is blank\")\n    elif len(pw) < 4:\n        # 4-digit PINS could be valid\n        return (False, \"Password is too short\")\n    elif len(set(list(pw))) == 1:\n        # try to catch an edge case found in my data where a password was just \".........\"\n        return (False, \"Password is all the same character\")\n\n    return (True, \"\")","sha1":"ad046cb6222c137fd235e75c8a733c7f32f08337","id":406080}
{"content":"def get_accuracy(true_positives, true_negatives, false_positives, false_negatives):\n    \"\"\"\n     The accuracy is the ability to correctly predict the class of an observation.\n    Args:\n        true_positives: amount of items that have correctly been identifies as positives\n        true_negatives: amount of items that have correctly been identifies as negatives\n        false_positives: amount of items that have wrongly been identifies as positives\n        false_negatives: amount of items that have wrongly been identifies as negatives\n\n    Returns:\n\n    \"\"\"\n    return (true_positives + true_negatives) \/ (true_positives + true_negatives + false_positives + false_negatives)","sha1":"d0b3287eab44dedf33033fe90b554dd62479dacd","id":600679}
{"content":"import re\n\n\ndef postprocess_output(text, max_length, stop_string, output_regex):\n    \"\"\"\n    Modify model output to satisfy stop_string and output_regex keywords.\n\n    Args:\n      text: A string or list of strings containing model outputs.\n      max_length: Model output will be truncated to be at most this length.\n      stop_string: Model output will be truncated to the shortest string\n        which includes stop_string. If None, no truncation will be performed.\n        e.g. if stop_string='.', the model output \"The apple is on the\n        ground. What is\" will be truncated to \"The apple is on the ground.\"\n        before being returned.\n      output_regex: Rather than returning the full output, return the first\n        match to the python regular expression output_regex in the output\n        string. If there is no match, an empty string will be returned. If\n        output_regex=None, no regular expression matching will be performed.\n        e.g. if output_regex=r\"\\\\d+\", and the model output is \"= 42. 7*7 =\n        49. 7*8 =\", then \"42\" will be returned.\n\n    Returns: A string or list of strings, all processed as described for each\n      argument.\n    \"\"\"\n\n    if isinstance(text, list):\n        return [\n            postprocess_output(mo, max_length, stop_string, output_regex) for mo in text\n        ]\n\n    # Ensure it is a string (will convert from bytes, ... as needed)\n    if not isinstance(text, str):\n        text = str(text, \"utf-8\")\n\n    # truncate at max_length\n    if max_length:\n        text = text[:max_length]\n\n    # Remove all text after any stop_string\n    if stop_string:\n        index = text.find(stop_string)\n        if index > 0:\n            text = text[: index + len(stop_string)]\n\n    # extract substring matching regex (empty string for no match)\n    if output_regex:\n        _text = text\n        text = next(iter(re.findall(output_regex, text)), \"\")\n        assert (\n            not type(text) is tuple\n        ), f'Regex {output_regex} returned multiple matching groups when applied to string {_text}. Try using non-capturing groups, by starting regex groups with ?: (e.g. \"(stuff)\" -> \"(?:stuff)\").'\n\n    return text","sha1":"6f95fd488ad2924024acfe2bd45ffdc869d3a252","id":492395}
{"content":"import random\nimport hashlib\n\n\ndef sha512(*args):\n    \"\"\"Generates SHA512 based on the provided string or random int\"\"\"\n    value = \"\"\n    if len(args) > 0:\n        value = args[0]\n    if value == \"\":\n        value = str(random.getrandbits(512))\n    return hashlib.sha512(value.encode('utf-8')).hexdigest()","sha1":"f281950acf14d9137faf493ff8bc1b3897da0681","id":492332}
{"content":"def is_data(data):\n\t\"\"\" Check if a packet is a data packet. \"\"\"\n\treturn len(data) > 26 and ord(data[25]) == 0x08 and ord(data[26]) in [0x42, 0x62]","sha1":"edb2a6b69fde42aef75923a2afbd5736d1aca660","id":5302}
{"content":"def halt_after_time(time,at_node,node_data,max_time=100):\n    \"\"\"\n    Halts after a fixed duration of time.\n    \"\"\"\n    return time>=max_time","sha1":"c1d2b82d5a8b2019be8ea845bf5b135ed7c5209f","id":690127}
{"content":"def is_fasta_file_extension(file_name):\n\t\"\"\"\n\tCheck if file has fasta extension\n\n\tParameters\n\t----------\n\tfile_name : str\n\t\tfile name\n\n\tReturns\n\t-------\n\tbool\n\t\tTrue if file has fasta extension, False otherwise\n\t\"\"\"\n\tif file_name[-4:] == \".fna\":\n\t\treturn True\n\telif file_name[-4:] == \".faa\":\n\t\treturn True\n\telif file_name[-6:] == \".fasta\":\n\t\treturn True\n\telif file_name[-6:] == \".fastq\":\n\t\treturn True\n\telif file_name[-3:] == \".fa\":\n\t\treturn True\n\telse:\n\t\treturn False","sha1":"711267ab304ca188b03f3a7e816a0df70c3f4fa3","id":689185}
{"content":"def make_select(\n    name,\n    selected,\n    data,\n    jscallback=None,\n    cssclass=None,\n    multiple=False,\n    showvalue=True,\n) -> str:\n    \"\"\"Generate a HTML select.\n\n    The trick here is what `data` looks like.  The basic form is a dict.\n    You can get `optgroup`s by having the dictionary keys be additional\n    lists or dicts.\n\n    Args:\n      name (str): The select[name] to assign.\n      selected (mixed): The option value that should be set to selected.\n      data (dict): The structure to build our select from.\n      jscallback (str): javascript to place in the `onChange` attribute.\n      cssclass (str): CSS class to assign to the select element.\n      showvalue (bool): Should option label be prepended by [key].\n\n    Returns:\n      html_string\n    \"\"\"\n    if not isinstance(selected, (list, tuple)):\n        selected = [selected]\n    s = '<select name=\"%s\"%s%s%s>\\n' % (\n        name,\n        (\n            \"\"\n            if jscallback is None\n            else f' onChange=\"{jscallback}(this.value)\"'\n        ),\n        \"\" if cssclass is None else f' class=\"{cssclass}\"',\n        \"\" if not multiple else \" MULTIPLE\",\n    )\n    for key, val in data.items():\n        if isinstance(val, (tuple, list)):\n            val = dict(list(zip(val, val)))\n        if not isinstance(val, dict):  # simple\n            s += '<option value=\"%s\"%s>%s%s<\/option>\\n' % (\n                key,\n                ' selected=\"selected\"' if key in selected else \"\",\n                f\"[{key}] \" if showvalue else \"\",\n                val,\n            )\n            continue\n        s += f'<optgroup label=\"{key}\">\\n'\n        for key2, val2 in val.items():\n            s += '<option value=\"%s\"%s>%s%s<\/option>\\n' % (\n                key2,\n                ' selected=\"selected\"' if key2 in selected else \"\",\n                f\"[{key2}] \" if showvalue else \"\",\n                val2,\n            )\n        s += \"<\/optgroup>\\n\"\n    s += \"<\/select>\\n\"\n    return s","sha1":"7d47b562d3c3bd58f7f946032207c1a32dc3f04b","id":38756}
{"content":"def proctime(d):\n    \"\"\"\n    Convers D to an integer in seconds\n    Args:\n        d (str): Duration\n    Returns:\n        int: Time in seconds of duration\n    \"\"\"\n    t = {\"s\": 1, \"m\": 60, \"h\": 3600, \"d\": 86400, \"w\": 604800}\n    suffix = d[-1]\n    d = int(d[:-1])\n    d = d * t[suffix]\n    return d","sha1":"7f2dfbf82e35776fed44408388e4003675e4c87d","id":272658}
{"content":"def issue_to_id(issue):\n    \"\"\"Create a unique id from an issue.\"\"\"\n    return '{0.organization}-{0.repository}-{0.number:d}'.format(issue)","sha1":"c57fca45cc4ed2706a48d2664d77ea7f9d631c36","id":363685}
{"content":"def get_and_update_or_create(model, unique, update):\n    \"\"\"\n    Given a model, a dictionary of lookup arguments, and a dictionary of update\n    arguments, this convenience function gets an object and updates it in the\n    database if necessary.\n\n    Returns a tuple (object, int) where int is 0 if the object was not updated,\n    1 if the object was created, and 2 if the object was updated in the\n    database.\n\n    >>> resp = get_and_update_or_create(User, {'username': 'example'}, {'email': 'example@example.com'})\n    >>> resp\n    (<User: example>, 1)\n    >>> resp[0].email\n    'example@example.com'\n    >>> resp = get_and_update_or_create(User, {'username': 'example'}, {'email': 'example@example.com'})\n    >>> resp\n    (<User: example>, 0)\n    >>> resp[0].email\n    'example@example.com'\n    >>> resp = get_and_update_or_create(User, {'username': 'example'}, {'email': 'another@example.com'})\n    >>> resp\n    (<User: example>, 2)\n    >>> resp[0].email\n    'another@example.com'\n    \"\"\"\n    obj, created = model.objects.get_or_create(dict(unique, default=update))\n\n    # If we just created it, then the defaults kicked in and we're good to go\n    if created:\n        return obj, 1\n\n    # Iterate over all of the fields to update, updating if needed, and keeping\n    # track of whether any field ever actually changed\n    modified = False\n    for name, val in update.iteritems():\n        if getattr(obj, name) != val:\n            modified = True\n            setattr(obj, name, val)\n\n    # If a field did change, update the object in the database and return\n    if modified:\n        obj.save(force_update=True)\n        return obj, 2\n\n    # Otherwise the object in the database is up to date\n    return obj, 0","sha1":"5c6c010ae7a38bb5d928b4b611954aae8bad5ffa","id":523497}
{"content":"import csv\n\n\ndef load_handles(csv_file_path):\n    \"\"\"Load twitter handles from csv file\"\"\"\n    with open(csv_file_path) as fin:\n        csv_reader = csv.reader(fin)\n        return [\n            handle[0]\n            for handle in csv_reader\n            if handle[0].startswith('@')\n        ]","sha1":"9e6d61d93dd1c199455bb59bf62c887824242eef","id":512948}
{"content":"from functools import reduce\nimport operator\n\n\ndef product(values, base=1):\n    \"\"\"Get the product of the given values.\n\n    :param values: An iterable of values to be multiplied.\n\n    :param base: Applied to the beginning of the calculation.\n    :type base: int\n\n    \"\"\"\n    return reduce(operator.mul, values, base)","sha1":"c645ec9f354f4ef265026128dc0914c3a60b8d1f","id":648935}
{"content":"def find_mismatching_target_names(plates_df, hits_df):\n    \"\"\"\n    Identify and print gene names that are mismatching between the plates\n    dataframe and the hits dataframe so that the names could be manually changed\n    \"\"\"\n\n    # get target names from hits_df\n    hits_genes = set([x[0].split('_', 1)[1] for x in list(hits_df) if 'P0' in x[0]])\n\n    # get target names from plates_df\n    plate_genes = set(plates_df['target_name'].values.tolist())\n    return (hits_genes - plate_genes)","sha1":"271e506fb69747878bf3c01f070795a2d9a7ee8e","id":389589}
{"content":"def filter_data_set(complete_data_set, filter_indices):\n    \"\"\"\n    filter a dataset given in the 'standard' form by specifying the relevant indices\n    :param complete_data_set:\n    :param filter_indices: indices that determine the elements to keep\n    :return: a filter dataset, i.e. a subset of the input complete_data_set\n    \"\"\"\n    complete_data_set_copy = complete_data_set.copy()\n    complete_data_set_copy.update(time=complete_data_set_copy['time'][filter_indices],\n                                  external_voltage=complete_data_set_copy['external_voltage'][filter_indices, :],\n                                  disturbance_strength_Delta_V=complete_data_set_copy['disturbance_strength_Delta_V'][\n                                                               filter_indices, :],\n                                  disturbance_length_Delta_t=complete_data_set_copy['disturbance_length_Delta_t'][\n                                                             filter_indices, :],\n                                  states_initial=complete_data_set_copy['states_initial'][filter_indices, :],\n                                  states_results=complete_data_set_copy['states_results'][filter_indices, :],\n                                  states_prediction=complete_data_set_copy['states_prediction'][filter_indices, :],\n                                  data_type=complete_data_set_copy['data_type'][filter_indices, :],\n                                  simulation_id=complete_data_set_copy['simulation_id'][filter_indices, :],\n                                  )\n\n    return complete_data_set_copy","sha1":"d7d309f2dd9be5ad94a9407430816bd3bf0b0a02","id":498627}
{"content":"import random\n\n\ndef _choose_bases(n):\n    \"\"\"Choose appropriate bases for the Miller-Rabin primality test.\n\n    If n is small enough, returns a tuple of bases which are provably\n    deterministic for that n. If n is too large, return a mostly random\n    selection of bases such that the chances of an error is less than\n    1\/4**40 = 8.2e-25.\n    \"\"\"\n    # The Miller-Rabin test is deterministic and completely accurate for\n    # moderate sizes of n using a surprisingly tiny number of tests.\n    # See: Pomerance, Selfridge and Wagstaff (1980), and Jaeschke (1993)\n    # http:\/\/en.wikipedia.org\/wiki\/Miller%E2%80%93Rabin_primality_test\n    if n < 1373653:  # Blah, it's too hard to read big ints at a glance.\n        # ~1.3 million\n        bases = (2, 3)\n    elif n < 9080191:  # ~9.0 million\n        bases = (31, 73)\n    elif n < 4759123141:  # ~4.7 billion\n        # Note to self: checked up to approximately 394 million in 9 hours.\n        bases = (2, 7, 61)\n    elif n < 2152302898747:  # ~2.1 trillion\n        bases = (2, 3, 5, 7, 11)\n    elif n < 3474749660383:  # ~3.4 trillion\n        bases = (2, 3, 5, 7, 11, 13)\n    elif n < 341550071728321:  # ~341 trillion\n        bases = (2, 3, 5, 7, 11, 13, 17)\n    else:\n        # n is too large, so we have to use a probabilistic test. There's no\n        # harm in trying some of the lower values for base first.\n        bases = (2, 3, 5, 7, 11, 13, 17) + tuple(\n                    [random.randint(18, n-1) for _ in range(40)]\n                    )\n        # Note: we can always be deterministic, no matter how large N is, by\n        # exhaustive testing against each i in the inclusive range\n        # 1 ... min(n-1, floor(2*(ln N)**2)). We don't do this, because it is\n        # expensive for large N, and of no real practical benefit.\n    return bases","sha1":"4d1b8c447667538dda618f667af859856174c054","id":583885}
{"content":"def within_extent(x, y, extent):\n    \"\"\"Test x, y coordinates against (xmin, xmax, ymin, ymax) extent\n    \"\"\"\n    xmin, xmax, ymin, ymax = extent\n    return (xmin < x) and (x < xmax) and (ymin < y) and (y < ymax)","sha1":"90dd0e578498fdfd8008b26f4f6163bce8e80ec2","id":449733}
{"content":"def format_movie_year_min(pick_movie_year_min):\n    \"\"\"\n    Ensures that the minimum movie year is properly formatted (parameter is the minimum movie year)\n    \"\"\"\n    if bool(pick_movie_year_min) == True:\n            movie_year_min = str(pick_movie_year_min) + \"-01-01\"\n    else:\n        movie_year_min = None\n    return movie_year_min","sha1":"5a97a1f059d5dfb445042ea0a31bb8aed6342a0f","id":670775}
{"content":"def convert_path_objects_to_path_strings(t_paths):\n    \"\"\"Convert a tuple of WindowsPath objects to a simple list of paths.\"\"\"\n    return [str(path) for path in t_paths]","sha1":"6d0cd4a861e87c841d4cae3a78fb0a016734b4c4","id":457775}
{"content":"def task_compile() -> dict:\n    \"\"\"Compile *.po files to *.mo files.\"\"\"\n    return {\n        \"actions\": [\n            \"pybabel compile -D shooter -d shooter\/locale -l ru\",\n            \"pybabel compile -D shooter -d shooter\/locale -l en\"\n            ],\n        # \"file_dep\": [\n        #     'src\/py-simple-shooter\/locale\/ru\/LC_MESSAGES\/py-simple-shooter.po',\n        #     'src\/py-simple-shooter\/locale\/en\/LC_MESSAGES\/py-simple-shooter.po',],\n        \"targets\": [\n            'shooter\/locale\/ru\/LC_MESSAGES\/shooter.mo',\n            'shooter\/locale\/en\/LC_MESSAGES\/shooter.mo',\n            ],\n        \"clean\": True,\n    }","sha1":"b655bc700f4cdf7be217d8b4f64d415aa688304c","id":195271}
{"content":"import copy\n\n\ndef get_tabs_content(tabs: dict,\n                     full_query: str,\n                     search_type: str,\n                     translation: dict) -> dict:\n    \"\"\"Takes the default tabs content and updates it according to the query.\n\n    Args:\n        tabs: The default content for the tabs\n        full_query: The original search query\n        search_type: The current search_type\n        translation: The translation to get the names of the tabs\n\n    Returns:\n        dict: contains the name, the href and if the tab is selected or not\n    \"\"\"\n    tabs = copy.deepcopy(tabs)\n    for tab_id, tab_content in tabs.items():\n        # update name to desired language\n        if tab_id in translation:\n            tab_content['name'] = translation[tab_id]\n\n        # update href with query\n        query = full_query.replace(f'&tbm={search_type}', '')\n\n        if tab_content['tbm'] is not None:\n            query = f\"{query}&tbm={tab_content['tbm']}\"\n\n        tab_content['href'] = tab_content['href'].format(query=query)\n\n        # update if selected tab (default all tab is selected)\n        if tab_content['tbm'] == search_type:\n            tabs['all']['selected'] = False\n            tab_content['selected'] = True\n    return tabs","sha1":"ce7c21fc98ac1ad9c50b70c1e05a75b391befa8b","id":556833}
{"content":"def render_cell(cell, context, jinja_env):\n    \"\"\"Render the Jinja-templated source of a single notebook cell.\n\n    Parameters\n    ----------\n    cell : `nbformat.NotebookNode`\n        An individual cell from a notebook, as a `~nbformat.NotebookNode`.\n        This object can also be created programatically, see\n        `nbformat.v4.new_code_cell`, `nbformat.v4.new_markdown_cell`, and\n        `nbformat.v4.new_raw_cell`.\n    context : `dict`-like\n        The template context. Usually this is constructed via\n        `cookiecutter.generate.generate_context`.\n    jinja_env : `cookiecutter.environment.StrictEnvironment`\n        The Jinja environment.\n\n    Returns\n    -------\n    cell : `nbformat.NotebookNode`\n        The input cell with the ``source`` member replaced with rendered\n        content.\n\n    Notes\n    -----\n    This function operates on the ``source`` member of the provided ``cell``.\n    Other members of the cell are left unmodified.\n\n    For more information about the format of a cell, see `The Notebook\n    file format <https:\/\/ls.st\/g01>`_ in the nbformat docs.\n    \"\"\"\n    template = jinja_env.from_string(cell.source)\n    cell.source = template.render(**context)\n    return cell","sha1":"9612c122c726f31d9055ac29f7bb891b269f906b","id":395618}
{"content":"def transfer_driver_cookies_to_request(cookies):\n    \"\"\"\n    Extract the cookies to a format that the request library understands\n    :param cookies: This should be a driver cookie. Obtained with the command my_driver.get_cookies()\n    :type cookies: dict\n\n    :return: Cookies dictionary suitable for a requests lib object\n    :rtype: dict\n    \"\"\"\n    return {i['name']: i['value'] for i in cookies}","sha1":"b81df474c731f939421852bf2f719fee73c9d3c5","id":63049}
{"content":"from bs4 import BeautifulSoup\n\n\ndef get_facts_soup(soup:BeautifulSoup):\n    \"\"\" \n        ** Creates a sub-soup object **\n\n        ** Args: **\n\n        * soup (BeautifulSoup): \n            * Beautifulsoup soup Object of the complete page\n\n        ** Returns: **\n\n        * Beautifulsoup: \n            * a sub-soup element of the complete page\n    \"\"\"\n    return soup.find(class_=\"citation\")","sha1":"296f188aa58e841c5de3d70672638e4325ac3b29","id":534215}
{"content":"def parse(stdin):\n    \"\"\"\n    Parse an input data set into a list.\n    \"\"\"\n\n    return [\n        int(line) for line in stdin.read().strip().split(\"\\n\")\n    ]","sha1":"6696c6e5b817ff0cf6ee30f2fce61a8cdabd8153","id":91443}
{"content":"def nightly_calendar(twilight_evening, twilight_morning, time_windows, verbose = False):\n    \"\"\"\n    Sort observation tot_time windows by nightly observing window.\n\n    Parameters\n    ----------\n    twilight_evening : '~astropy.tot_time.core.Time'\n        Evening twilight tot_time for scheduling period (UTC)\n\n    twilight_morning : '~astropy.tot_time.core.Time'\n        Morning twilight tot_time for scheduling period (UTC)\n\n    time_windows : list of lists of '~astropy.tot_time.core.Time' pairs\n        Array of tot_time windows for all observations.\n\n    Returns\n    -------\n    i_obs : int array\n        Indices of observations with a time_window during the night of the provided date.\n\n    obs_windows : array of '~astropy.tot_time.core.Time' pair(s)\n        Observation tot_time windows for current night corresponding to 'i_obs'.\n    \"\"\"\n\n    # define start of current day as local noon\n    night_start = twilight_evening\n    night_end = twilight_morning\n\n    if verbose:\n        print('\\nDate window (start,end): ', night_start.iso, night_end.iso)\n\n    i_obs = []  # list of current night's observations\n    obs_windows = []  # tot_time windows corresponding to i_obs\n    for i in range(len(time_windows)):  # cycle through observations\n\n        if verbose:\n            print('\\tobs i:', i)\n\n        if time_windows[i] is not None:\n\n            obs_wins = []\n\n            for j in range(len(time_windows[i])):  # cycle through tot_time windows\n\n                if verbose:\n                    print('\\t\\ttime_window[' + str(i) + '][' + str(j) + ']:',\n                          time_windows[i][j][0].iso, time_windows[i][j][1].iso)\n\n                # save index if there is overlap with schedule period\n                if time_windows[i][j][1] >= night_start and night_end >= time_windows[i][j][0]:\n                    obs_wins.append(time_windows[i][j])\n                    if verbose:\n                        print('\\t\\t\\tadded window')\n                # else:\n                #     print('\\t\\tnot added')\n\n            # if tot_time window(s) overlapped with night, save obs index and window(s)\n            if len(obs_wins) != 0:\n                i_obs.append(i)\n                obs_windows.append(obs_wins)\n                if verbose:\n                    print('\\t\\tadded obs index'\n                          ' to list')\n\n        else:\n            if verbose:\n                print('\\t\\t\\ttime_window[' + str(i) + ']:', time_windows[i])\n            pass\n\n    # if verbose:\n    #     print('i_obs', i_obs)\n    #     print('obs_windows', obs_windows)\n\n    return i_obs, obs_windows","sha1":"54c59458cbf4816a573e1edfc2b41799f30f0763","id":128331}
{"content":"def fst(ls):\n    \"\"\"returns the 0th element of a list\"\"\"\n    return ls[0]","sha1":"a44d5b883be2117f9dc26be3cedac8f8e4356471","id":670453}
{"content":"def read_gmt(gmt_file: str) -> dict:\n    \"\"\"Parser for gmt files, specifically from ptm-ssGSEA\n\n    Args:\n        gmt_file: Name of gmt file.\n\n    Returns:\n        Dictionary of ptm sets. Keys are labels for each set. Values are dictionaries with\n        structure: {aa sequence: site name}\n\n    \"\"\"\n    result = {}\n    with open(gmt_file, 'r') as fh:\n        for line in fh.readlines():\n            line = line.strip().split('\\t')\n            name = line[0]\n            site_labels = line[1]\n            seqs = line[2:]\n            seq_labels = {seqs[i].split('-')[0]: label for i, label in enumerate(site_labels.split('|')[1:])}\n            result.update({name: seq_labels})\n\n    return result","sha1":"ab5131d038f2a8d7bff78ec848e0490c84db3854","id":512680}
{"content":"def get_metric(metric):\n    \"\"\"Get the current value of a Prometheus metric\"\"\"\n    return metric.collect()[0].samples[0][2]","sha1":"96bf8507a0848ed396134efc8bd27df5576c7860","id":246069}
{"content":"def format_detections(num_images, pre_format_dict):\n    \"\"\"Dict{List} to List[Dict]\"\"\"\n    detections = []\n    keys = pre_format_dict.keys()\n    for v in pre_format_dict.values(): assert len(v) == num_images\n\n    for i in range(num_images):\n        item = dict()\n        for k in keys:\n            item.update({k: pre_format_dict[k][i]})\n        detections.append(item)\n    return detections","sha1":"6d2b21a69d093db8a18550664a0094bf8a59ec2e","id":120981}
{"content":"def downloading_complete(dct,flag=None):\n    \"\"\" Utility function: check if all files in the dictionary have been \n        successefully downloaded.\n        You can se the flag to:\n        * 'op' to only check for final products (forecast are ignored)\n        * 'fc' to only check for forecast products (final are ignored)\n        * None to either check final or forecast, i.e. a file is considered\n          successefully downloaded in either case\n    \"\"\"\n    if not flag:\n        all([dct[x]['fc'] or dct[x]['op'] for x in dct])\n    elif flag == 'fc':\n        return all([dct[x]['fc'] for x in dct])\n    elif flag=='op':\n        return all([dct[x]['op'] for x in dct])\n    else:\n        raise RuntimeError('[ERROR] Invalid argument to downloading_complete function!')","sha1":"368f2ff6aa128d4664250b397e77c0f3a9e4ddb3","id":570614}
{"content":"def get_atts(card):\n    \"\"\"Return dict with name, set, and price of card\"\"\"\n    card_name = card[\"name\"]\n    set_name = card[\"set_name\"]\n    prices = card[\"prices\"]\n    card_atts = {\n        \"name\": card_name,\n        \"set\": set_name,\n        \"price_normal\": prices[\"usd\"],\n        \"price_foil\": prices[\"usd_foil\"]\n    }\n    print(\"Card Name: {}\\nSet: {}\".format(card_name, set_name))\n    print(\"Normal: {}\\nFoil: {}\".format(prices[\"usd\"], prices[\"usd_foil\"]))\n    return(card_atts)","sha1":"75ca5ad53398a6b177023bc8aff5ff91ffff7342","id":106172}
{"content":"import random\nimport string\n\n\ndef random_id(length=5):\n    \"\"\"Generate a random ID of 5 characters to append to qsub job name.\"\"\"\n    return ''.join(random.sample(string.ascii_letters + string.digits, length))","sha1":"c69c5f50e8a913b67eed1f9449b992f87f976552","id":630099}
{"content":"import importlib\n\n\ndef parse_model_objects(s: str):\n    \"\"\"Parses model objects string.\n\n    The string must contain the name of an importable module ''mod''.\n    The module is imported and ''mod.get_model_objects()'' is called to\n    get a list of model objects.\n    \"\"\"\n    mod = importlib.import_module(s)\n    return mod.get_model_objects","sha1":"daa736d34a0dbe7185c45ab971f9706aa2655143","id":283952}
{"content":"import re\n\n\ndef get_select_cols_and_rest(query):\n    \"\"\"\n    Separate the a list of selected columns from\n    the rest of the query\n\n    Returns:\n        1. a list of the selected columns\n        2. a string of the rest of the query after the SELECT\n    \"\"\"\n    from_loc = query.lower().find(\"from\")\n\n    raw_select_clause = query[0:from_loc].rstrip()\n    rest_of_query = query[from_loc:len(query)]\n\n    # remove the SELECT keyword from the query\n    select_pattern = re.compile(\"select\", re.IGNORECASE)\n    raw_select_clause = select_pattern.sub('', raw_select_clause)\n\n    # now create and iterate through a list of of the SELECT'ed columns\n    selected_columns = raw_select_clause.split(',')\n    selected_columns = [c.strip() for c in selected_columns]\n\n    return selected_columns, rest_of_query","sha1":"33b397e7894792c0b354c41f219c609dc4df5927","id":26984}
{"content":"def correct_gravity(sg, temp, cal_temp):\n    \"\"\"Correct a specific gravity reading to the specified calibration temperature\n\n    Args:\n        sg (float): Measured specific gravity\n        temp (float): Measurement temperature in degrees Fahrenheit\n        cal_temp (float): Hydrometer calibration temperature in degrees Fahrenheit\n    \"\"\"\n\n    numerator = 1.00130346 - 0.000134722124 * temp + 0.00000204052596 * temp**2 - 0.00000000232820948 * temp**3\n    denom = 1.00130346 - 0.000134722124 * cal_temp + 0.00000204052596 * cal_temp**2 - 0.00000000232820948 * cal_temp**3\n    corrected_gravity = sg * numerator \/ denom\n    return corrected_gravity","sha1":"c9f70212441af665ba0d2c889285e3a75c3e5423","id":86356}
{"content":"def compute_hyperpars(k, mu, nu, L, mean, S, N):\n    \"\"\"\n    Update hyperparameters for Normal Inverse Gamma\/Wishart (NIG\/NIW).\n    See https:\/\/www.cs.ubc.ca\/~murphyk\/Papers\/bayesGauss.pdf\n    \n    Arguments:\n        :double k:        Normal std parameter (for NIG\/NIW)\n        :np.ndarray mu:   Normal mean parameter (for NIG\/NIW)\n        :int nu:          Gamma df parameter (for NIG\/NIW)\n        :np.ndarray L:    Gamma scale matrix (for NIG\/NIW)\n        :np.ndarray mean: samples mean\n        :np.ndarray S:    samples covariance\n        :int N:           number of samples\n    \n    Returns:\n        :double:     updated Normal std parameter (for NIG\/NIW)\n        :np.ndarray: updated Normal mean parameter (for NIG\/NIW)\n        :int:        updated Gamma df parameter (for NIG\/NIW)\n        :np.ndarray: updated Gamma scale matrix (for NIG\/NIW)\n    \"\"\"\n    k_n  = k + N\n    mu_n = (mu*k + N*mean)\/k_n\n    nu_n = nu + N\n    L_n  = L + S + k*N*((mean - mu).T@(mean - mu))\/k_n\n    return k_n, mu_n, nu_n, L_n","sha1":"dc6cd3acc920473a9e676a77531fb68f8354ffcd","id":488710}
{"content":"def _without_namespace(tagname):\n    \"\"\"Remove the xml namespace from a tag name.\"\"\"\n    if '}' in tagname:\n        return tagname.rsplit('}', 1)[-1]\n    return tagname","sha1":"ad0dca49ee2e018053cb3f8a561a73b236a1e220","id":338866}
{"content":"def hamming(str1, str2):\n    \"\"\"Hamming distance between two strings\"\"\"\n    return sum(a!=b and not( a=='N' or b=='N' ) for a,b in zip(str1, str2))","sha1":"247bde164b51708a504c9aabbf429daeed62a07e","id":314005}
{"content":"def compute_specificity(cm):\n    \"\"\"\n    Computes specificity for binary classification problems\n    :param cm: A Numpy-like matrix that represents a confusion matrix\n    :return: The specificity of the confusion matrix\n    \"\"\"\n    print(cm.shape)\n    print(len(cm.shape))\n    assert len(cm.shape) and cm.shape[0] == cm.shape[1] and cm.shape[0] == 2\n    TP = cm[1, 1]\n    TN = cm[0, 0]\n    FN = cm[1, 0]\n    FP = cm[0, 1]\n    if FP+TN == 0:\n        return 0\n    return float(TN)\/(FP + TN)","sha1":"4815d74db80be0b953dade3692092634ff7c9b7b","id":118602}
{"content":"import math\n\n\ndef _raw_hp(base: int, ev: int, iv: int, level: int) -> int:\n    \"\"\"Converts to raw hp\n    :param base: the base stat\n    :param ev: HP Effort Value (EV)\n    :param iv: HP Individual Value (IV)\n    :param level: pokemon level\n    :return: the raw hp\n    \"\"\"\n    s = math.floor((math.floor(ev \/ 4) + iv + 2 * base) * level \/ 100) + level + 10\n    return int(s)","sha1":"21adf05cc21d20ea4ae9ab8a16cbd3a7c2f7d4ec","id":128179}
{"content":"def parse_logs(files):\n\n    \"\"\"\n    parses through log files to extract marginal\n    likelihood estimates from executing the\n    variational inference algorithm on a dataset.\n\n    Arguments:\n\n        files : list\n            list of .log file names\n\n    \"\"\"\n\n    marginal_likelihood = []\n    for file in files:\n        handle = open(file,'r')\n        for line in handle:\n            if 'Marginal Likelihood' in line:\n                m = float(line.strip().split('=')[1])\n                marginal_likelihood.append(m)\n                break\n        handle.close()\n\n    return marginal_likelihood","sha1":"3820ec79fb4800a66d92583d687e985f07b1bf64","id":466192}
{"content":"def init_task(parts, name='init-pdpart'):\n    \"\"\"Create a doit task to initialize a Partitioned directory.\n\n    Parameters\n    ----------\n    parts : pdpart.Partitioned\n        Partitioned object to be initialized\n    name : str\n        name of task, defaults to 'init-pdpart'\n    \"\"\"\n    def _wrapper():\n        \"\"\"withhold return value for compatibility with doit\"\"\"\n        parts.init_dir()\n\n    return {\n        'name': name,\n        'actions': [(_wrapper, [], {})],\n        'file_dep': [],\n        'targets': [parts.fn_meta],\n        'uptodate': [True],\n    }","sha1":"a8c1cb27461b48b5c9f0a8733b058a24403a5227","id":694554}
{"content":"def check_stmt(stmt, conditions, evid_policy='any'):\n    \"\"\"Decide whether a statement meets the conditions.\n\n    Parameters\n    ----------\n    stmt : indra.statements.Statement\n        INDRA Statement that should be checked for conditions.\n    conditions : dict\n        Conditions represented as key-value pairs that statements'\n        metadata can be compared to. NOTE if there are multiple conditions\n        provided, the function will require that all conditions are met to\n        return True.\n    evid_policy : str\n        Policy for checking statement's evidence objects. If 'all', then the\n        function returns True only if all of statement's evidence objects meet\n        the conditions. If 'any', the function returns True as long as at\n        least one of statement's evidences meets the conditions.\n\n    Return\n    ------\n    meets_conditions : bool\n        Whether the Statement meets the conditions.\n    \"\"\"\n    evid_checks = []\n    for evid in stmt.evidence:\n        emmaa_anns = evid.annotations.get('emmaa')\n        if emmaa_anns:\n            metadata = emmaa_anns.get('metadata')\n            checks = []\n            for key, value in conditions.items():\n                checks.append(metadata[key] == value)\n            evid_checks.append(all(checks))\n            if all(checks) and evid_policy == 'any':\n                break\n    # There are no evidence checks if stmt doesn't have emmaa annotations,\n    # in this case we say it meets conditions by default\n    if not evid_checks:\n        return True\n    # Make decision based on the evidence policy\n    if evid_policy == 'any':\n        return any(evid_checks)\n    elif evid_policy == 'all':\n        return all(evid_checks)","sha1":"045e9c5fec6681f0c90fcb0df2b4b924b72e12f9","id":614245}
{"content":"def ATE(y, treatment):\n    \"\"\"Average Treatment Effect\n    Note! treatment assumed to have values {0, 1}\"\"\"\n    return (y * treatment).sum() \/ (treatment.sum()) - (\n        y * (1 - treatment)\n    ).sum() \/ ((1 - treatment).sum())","sha1":"fa1b69624a1204b3e8a0e32e327c85348bd97a2c","id":527300}
{"content":"import torch\n\n\ndef sem2ins_masks(gt_sem_seg,\n                  num_thing_classes=80):\n    \"\"\"Convert semantic segmentation mask to binary masks\n\n    Args:\n        gt_sem_seg (torch.Tensor): Semantic masks to be converted.\n            [0, num_thing_classes-1] is the classes of things,\n            [num_thing_classes:] is the classes of stuff.\n        num_thing_classes (int, optional): Number of thing classes.\n            Defaults to 80.\n\n    Returns:\n        tuple[torch.Tensor]: (mask_labels, bin_masks).\n            Mask labels and binary masks of stuff classes.\n    \"\"\"\n    # gt_sem_seg is zero-started, where zero indicates the first class\n    # since mmdet>=2.17.0, see more discussion in\n    # https:\/\/mmdetection.readthedocs.io\/en\/latest\/conventions.html#coco-panoptic-dataset  # noqa\n    classes = torch.unique(gt_sem_seg)\n    # classes ranges from 0 - N-1, where the class IDs in\n    # [0, num_thing_classes - 1] are IDs of thing classes\n    masks = []\n    labels = []\n\n    for i in classes:\n        # skip ignore class 255 and \"thing classes\" in semantic seg\n        if i == 255 or i < num_thing_classes:\n            continue\n        labels.append(i)\n        masks.append(gt_sem_seg == i)\n\n    if len(labels) > 0:\n        labels = torch.stack(labels)\n        masks = torch.cat(masks)\n    else:\n        labels = gt_sem_seg.new_zeros(size=[0])\n        masks = gt_sem_seg.new_zeros(\n            size=[0, gt_sem_seg.shape[-2], gt_sem_seg.shape[-1]])\n    return labels.long(), masks.float()","sha1":"c19c31c0fc09868ecc48b8de83b8c761282809a5","id":395766}
{"content":"import torch\n\n\ndef huber(d, delta):\n    \"\"\"\n    See: https:\/\/en.wikipedia.org\/wiki\/Huber_loss#Definition\n    Multiplied by 2 w.r.t Wikipedia version (aligning with Jan's definition)\n    \"\"\"\n    return torch.where(torch.abs(d)<=delta, d**2, 2.*delta*(torch.abs(d)-delta))","sha1":"43a387a2a97da7cada20484a3855f83e1f7630f6","id":483221}
{"content":"from bs4 import BeautifulSoup\n\n\ndef soupify(document):\n    \"\"\"\n    Takes an html-document and returns a BeautifulSoup object.\n    \"\"\"\n    return BeautifulSoup(document)","sha1":"ee637568cae2d457106dc67052142858e634a785","id":468903}
{"content":"import gzip\n\n\ndef open_input(filename):\n    \"\"\"Handle zipped or unzipped input\"\"\"\n    if filename.endswith(\".gz\"):\n        handler = gzip.open( filename, \"rt\")\n    else:\n        handler = open( filename, \"r\")\n    return handler","sha1":"618b7857eb2ca9e71e5474080dcedb4e00f9a21a","id":162674}
{"content":"def get_good_dim(dim):\n    \"\"\"\n    This function calculates the dimension supported by opencl library (i.e. is multiplier of 2, 3, or 5) and is closest to the given starting dimension.\n    If the dimension is not supported the function adds 1 and verifies the new dimension. It iterates until it finds supported value.\n    \n    Parameters\n    ----------\n    dim : int\n        initial dimension\n\n    Returns\n    -------\n    new_dim : int\n        a dimension that is supported by the opencl library, and closest to the original dimension\n    \"\"\"\n\n    def is_correct(x):\n        sub = x\n        if sub % 3 == 0:\n            sub = sub \/ 3\n        if sub % 3 == 0:\n            sub = sub \/ 3\n        if sub % 5 == 0:\n            sub = sub \/ 5\n        while sub % 2 == 0:\n            sub = sub \/ 2\n        return sub == 1\n\n    new_dim = dim\n    if new_dim % 2 == 1:\n        new_dim += 1\n    while not is_correct(new_dim):\n        new_dim += 2\n    return new_dim","sha1":"196e9f7d13f43bd0208258367139debcfd193782","id":450314}
{"content":"def packed_letter_to_number(letter):\n    \"\"\"Unpack a letter to the corresponding number according to MPC.\n\n    See:\n        https:\/\/www.minorplanetcenter.net\/iau\/info\/DesDoc.html\n\n    Args:\n        letter (str): Single character to decode.\n\n    Returns:\n        int: Corresponding number.\n    \"\"\"\n    try:\n        int(letter)\n        return letter.rjust(2, '0')\n    except ValueError:\n        ord_letter = ord(letter)\n        if ord_letter >= 97 and ord_letter <= 122:\n            return str(ord_letter - 61)\n        elif ord_letter >= 65 and ord_letter <= 96:\n            return str(ord_letter - 55)\n        else:\n            raise ValueError(f'Letter \"{letter}\" is invalid')","sha1":"ab83f539363316c09c4b38be93a78f34605774ee","id":285200}
{"content":"def get_object(context):\n    \"\"\"\n    Get an object from the context or view.\n    \"\"\"\n    object = None\n\n    view = context.get(\"view\")\n    if view:\n        # View is more reliable then an 'object' variable in the context.\n        # Works if this is a SingleObjectMixin\n        object = getattr(view, \"object\", None)\n\n    if object is None:\n        object = context.get(\"object\", None)\n\n    return object","sha1":"27f4b346d6e50d9bfe684643133d69511ea9ca22","id":626243}
{"content":"def ParseGTestListTests(raw_list):\n  \"\"\"Parses a raw test list as provided by --gtest_list_tests.\n\n  Args:\n    raw_list: The raw test listing with the following format:\n\n    IPCChannelTest.\n      SendMessageInChannelConnected\n    IPCSyncChannelTest.\n      Simple\n      DISABLED_SendWithTimeoutMixedOKAndTimeout\n\n  Returns:\n    A list of all tests. For the above raw listing:\n\n    [IPCChannelTest.SendMessageInChannelConnected, IPCSyncChannelTest.Simple,\n     IPCSyncChannelTest.DISABLED_SendWithTimeoutMixedOKAndTimeout]\n  \"\"\"\n  ret = []\n  current = ''\n  for test in raw_list:\n    if not test:\n      continue\n    if not test.startswith(' '):\n      test_case = test.split()[0]\n      if test_case.endswith('.'):\n        current = test_case\n    else:\n      test = test.strip()\n      if test and not 'YOU HAVE' in test:\n        test_name = test.split()[0]\n        ret += [current + test_name]\n  return ret","sha1":"ec53e28fbf7b934fcfffa0d028cabb4c3bc120b3","id":600439}
{"content":"def view(path, *, sep='->'):\n  \"\"\" Forms string representation of path. \"\"\"\n  if path is None:\n    return 'None'\n  return sep.join([str(point) for point in path])","sha1":"3c40265f3b50c73e8a62932663f3b96b4516b7ba","id":368836}
{"content":"def remove_signature(message):\n    \"\"\"Remove the 3 Letter signature and the '\/' found at the beginning of a valid message\"\"\"\n    return message[1:]","sha1":"a802dc5abfea09e05fad51936dd76e17719900e7","id":699126}
{"content":"def get_tags(data):\n    \"\"\" Convert the AWS ASG tags into something more useful \"\"\"\n    return dict((x['Key'], x['Value']) for x in data)","sha1":"672cc0c6a9f9bf9484b3dbfd23a6899de8107df5","id":485079}
{"content":"def parse_q_stats(line):\n    \"\"\"\n    Parses a \"stats\" line of a BESS queue log. Line should be of the form:\n        ( \"stats\", src port, enqueued, dequeued, dropped )\n    \"\"\"\n    return (\n        (\"stats\",) +\n        tuple(\n            int(tok, 16) if tok.startswith(\"0x\") else int(tok)\n            for tok in line.split(\":\")[1].split(\",\")))","sha1":"d804e475dd7e1d26c1fd89df3320ffa04aba6f6e","id":549397}
{"content":"def append(listA, listB):\n    \"\"\"\n    Add the linked lists together to form\n    one linked list. If both are None,\n    return None. If either is None, return\n    the other one.\n\n    If both have nodes, this function will\n    append listB to listA and return the\n    head of listA.\n\n    :param listA: Node, head of linked list\n    :param listB: Node, head of linked list\n    :return: Node or None, head of combined\n        list or None if both lists are None\n    \"\"\"\n\n    # if both lists are None, return None\n    if listA is None and listB is None:\n        return None\n\n    # if either list is None, return the other\n    if listA is None or listB is None:\n        return listA if listB is None else listB\n\n    # at this point both lists have nodes.\n    # let's loop through listA until we get to the\n    # last node and append listB to it.\n\n    current_node = listA\n\n    # find the last node in listA\n    while current_node.next is not None:\n        current_node = current_node.next\n\n    # append listB to the last node of listA\n    current_node.next = listB\n\n    # return the combined linked lists\n    return listA","sha1":"bace8277dd33330ee0046b1709e2c592c407a716","id":258190}
{"content":"def filter_publishes(app, sg_data_list):\n    \"\"\"\n    Filters a list of shotgun published files based on the filter_publishes\n    hook.\n\n    :param app:           app that has the hook.\n    :param sg_data_list:  list of shotgun dictionaries, as returned by the\n                          find() call.\n    :returns:             list of filtered shotgun dictionaries, same form as\n                          the input.\n    \"\"\"\n    try:\n        # Constructing a wrapper dictionary so that it's future proof to\n        # support returning additional information from the hook\n        hook_publish_list = [{\"sg_publish\": sg_data}\n                             for sg_data in sg_data_list]\n\n        hook_publish_list = app.execute_hook(\"filter_publishes_hook\",\n                                             publishes=hook_publish_list)\n        if not isinstance(hook_publish_list, list):\n            app.log_error(\n                \"hook_filter_publishes returned an unexpected result type \\\n                '%s' - ignoring!\"\n                % type(hook_publish_list).__name__)\n            hook_publish_list = []\n\n        # split back out publishes:\n        sg_data_list = []\n        for item in hook_publish_list:\n            sg_data = item.get(\"sg_publish\")\n            if sg_data:\n                sg_data_list.append(sg_data)\n\n    except:\n        app.log_exception(\"Failed to execute 'filter_publishes_hook'!\")\n        sg_data_list = []\n\n    return sg_data_list","sha1":"d9d1ed4506c516261a8ed2c4b52655287e86f28a","id":578626}
{"content":"import torch\n\n\ndef create_init_scores(prev_tokens, tensor):\n    \"\"\"\n    Create init scores in search algorithms\n\n    Args:\n        prev_tokens: previous token\n        tensor: a type tensor\n\n    Returns:\n        - initial scores as zeros\n    \"\"\"\n    batch_size = prev_tokens.size(0)\n    prev_scores = torch.zeros(batch_size).type_as(tensor)\n    return prev_scores","sha1":"0da0f693041602ed7e79f4f114fedac317419875","id":572056}
{"content":"def get_grouped_metric(df, group_column, metric_column, operation):\n    \"\"\"Group by a specified column, then perform a mathematical operation on a given\n    metric column and return the value so it can be assigned to the DataFrame.\n    For example, group by ID and sum the value of orders placed by the ID.\n\n    :param df: Pandas DataFrame\n    :param group_column: Column name to use for groupby, i.e. id\n    :param metric_column: Column name for metric column, i.e. visits\n    :param operation: Operation to perform  (sum, count, nunique, min, max, median, mean)\n    \"\"\"\n\n    return df.groupby(group_column)[metric_column].transform(operation)","sha1":"55e431acc33c4c7abb71b25b584edcf827555c7a","id":574328}
{"content":"from typing import List\nimport json\n\n\ndef get_checks_from_appinspect_result(path: str) -> List[str]:\n    \"\"\"\n    Returns manual checks from appinspect json result file\n\n    :param path: path to json result file\n    :return: list of checks in string format\n    \"\"\"\n    manual_checks = []\n    with open(path) as f:\n        appinspect_results = json.load(f)\n        for report in appinspect_results[\"reports\"]:\n            for group in report[\"groups\"]:\n                for check in group[\"checks\"]:\n                    if check[\"result\"] == \"manual_check\":\n                        manual_checks.append(check[\"name\"])\n    return manual_checks","sha1":"4cce76b49e3792c4d40bfe9fc091c5e2681cb970","id":583676}
{"content":"def remove_final_whitespace(string):\n    \"\"\"\n    Return a copy of *string* with final whitespace removed from each line.\n    \"\"\"\n    return '\\n'.join(x.rstrip() for x in string.split('\\n'))","sha1":"136692e32aa14ec607c908e7cd455e7885d6d463","id":98912}
{"content":"def return_unique_apps(app_list):\n    \"\"\"Return a list of unique apps and the number of times that app is installed\"\"\"\n\n    unique_app_list = []\n    install_count = []\n\n    for app in app_list:\n\n        if app not in unique_app_list:\n\n            unique_app_list.append(app)\n            install_count.append(\n                {\n                    \"app_name\": app[\"app_name\"],\n                    \"version\": app[\"version\"],\n                    \"install_count\": app_list.count(app),\n                }\n            )\n\n            print(app, app_list.count(app))\n\n    return install_count","sha1":"d3feb0ac80e21af61339cab23f9e9578e8c96438","id":342095}
{"content":"def find_first_common_ancestor(node1, node2):\n    \"\"\"\n    Find the first common ancestor for two nodes in the same tree.\n\n    Parameters\n    ----------\n    node1 : nltk.tree.ParentedTree\n        The first node.\n    node2 : nltk.tree.ParentedTree\n        The second node.\n\n    Returns\n    -------\n    ancestor_node : nltk.tree.ParentedTree\n        The first common ancestor for two nodes in the same tree.\n    \"\"\"\n    # make sure we are in the same tree\n    assert node1.root() == node2.root()\n\n    # make a set of all ancestors of node1\n    node1_ancestor_treepositions = set()\n    node1_parent = node1.parent()\n    while node1_parent is not None:\n        # note: storing treepositions isn't particularly efficient since\n        # treeposition() walks up the tree; using memory addresses like\n        # id(node1_parent) would be faster, but seems potentially\n        # hazardous\/confusing\n        node1_ancestor_treepositions.add(node1_parent.treeposition())\n        node1_parent = node1_parent.parent()\n\n    # find the first ancestor of node2 that is also an ancestor of node1\n    node2_parent = node2.parent()\n    res = None\n    while node2_parent is not None:\n        if node2_parent.treeposition() in node1_ancestor_treepositions:\n            res = node2_parent\n            break\n        node2_parent = node2_parent.parent()\n\n    assert res is not None\n    return res","sha1":"37748766c76e3fd1d2b1f15202637536147a7a5c","id":665770}
{"content":"import pickle\n\n\ndef load(input_filename):\n    \"\"\"unpickle an object from a file\"\"\"\n    with open(input_filename, \"rb\") as input_file:\n        res = pickle.load(input_file)\n    return res","sha1":"7401114353e671fa52a035159dd564882b771bc3","id":37266}
{"content":"def create_model_name(src):\n    \"\"\"Generate a name for a source object given its spatial\/spectral\n    properties.\n\n    Parameters\n    ----------\n    src : `~fermipy.roi_model.Source`\n          A source object.\n\n    Returns\n    -------\n    name : str\n           A source name.\n    \"\"\"\n    o = ''\n    spatial_type = src['SpatialModel'].lower()\n    o += spatial_type\n\n    if spatial_type == 'gaussian':\n        o += '_s%04.2f' % src['SpatialWidth']\n\n    if src['SpectrumType'] == 'PowerLaw':\n        o += '_powerlaw_%04.2f' % float(src.spectral_pars['Index']['value'])\n    else:\n        o += '_%s' % (src['SpectrumType'].lower())\n\n    return o","sha1":"d305dd26bc6017f3fce5db2a5267fa6e74df3bc6","id":698705}
{"content":"def fahrenheit(celsius):\n    \"\"\"Converts celsius to fahrenheit\"\"\"\n    return (celsius * (9\/5)) + 32","sha1":"faf890a53db4dcef80ec7dca1aaa3d52e6cb8ccb","id":484955}
{"content":"import json\n\n\ndef reset_notebook(fname, dryrun=False):\n    \"\"\"\n    Empties the output fields and resets execution_count in all code cells in the given notebook.\n\n    Also removes any empty code cells. The specified notebook is overwritten.\n\n    Parameters\n    ----------\n    fname : str\n        Name of the notebook file.\n    dryrun : bool\n        If True, don't actually update the file.\n\n    Returns\n    -------\n    bool\n        True if the file was updated or would have been updated if not a dry run.\n    \"\"\"\n    changed = False\n\n    with open(fname) as f:\n        dct = json.load(f)\n\n    newcells = []\n    for cell in dct['cells']:\n        if cell['cell_type'] == 'code':\n            if cell['source']:  # cell is not empty\n                if cell['execution_count'] is not None or len(cell['outputs']) > 0:\n                    cell['execution_count'] = None\n                    cell['outputs'] = []\n                    changed = True\n                newcells.append(cell)\n        else:\n            newcells.append(cell)\n\n    changed |= len(dct['cells']) != len(newcells)\n\n    dct['cells'] = newcells\n\n    if changed and not dryrun:\n        with open(fname, 'w') as f:\n            json.dump(dct, f, indent=1, ensure_ascii=False)\n            print(file=f)  # avoid 'no newline at end of file' message\n\n    return changed","sha1":"82e299bf0f40002bfc2a4f5054a6fbcbbafb29fb","id":530553}
{"content":"def construct_service_uri(provider, service, catalog_id=None):\n    \"\"\"Builds a uri from the given parameters.\n\n    Args:\n        provider (string): A string of the provider.\n        service (string): A string of the service.\n        catalog_id (string): A string of the catalog_id.\n\n    Returns:\n        If there is no catalog_id then the uri will just be the provider\n        and service, else the catalog_id will be appended to the end of the\n        uri.\n    \"\"\"\n    uri = 'svc:\/\/{}:{}'.format(provider, service)\n    if catalog_id is not None:\n        uri = '{}\/{}'.format(uri, catalog_id)\n\n    return uri","sha1":"a6b188bb6ac9243f0fb4c8d86774249f8e760cb6","id":420826}
{"content":"def basis_function(degree, knot_vector, span, t):\n    \"\"\"Computes the non-vanishing basis functions for a single parameter t.\n\n    Implementation of Algorithm A2.2 from The NURBS Book by Piegl & Tiller.\n    Uses recurrence to compute the basis functions, also known as Cox - de\n    Boor recursion formula.\n    \"\"\"\n    left = [0.0 for _ in range(degree + 1)]\n    right = [0.0 for _ in range(degree + 1)]\n    N = [1.0 for _ in range(degree + 1)]\n\n    for j in range(1, degree + 1):\n        left[j] = t - knot_vector[span + 1 - j]\n        right[j] = knot_vector[span + j] - t\n        saved = 0.0\n        for r in range(0, j):\n            temp = N[r] \/ (right[r + 1] + left[j - r])\n            N[r] = saved + right[r + 1] * temp\n            saved = left[j - r] * temp\n        N[j] = saved\n    return N","sha1":"b23549dfa81a87acf2478120eaf9c9e1e84005d5","id":666355}
{"content":"def cubic_rbf(r, kappa):\n    \"\"\"\n    Computes the Cubic Radial Basis Function between two points with distance `r`.\n\n    Parameters\n    ----------\n    r : float\n        Distance between point `x1` and `x2`.\n    kappa : float\n        Shape parameter.\n\n    Returns\n    -------\n    phi : float\n        Radial basis function response.\n    \"\"\"\n\n    return (r * kappa) ** 3.","sha1":"0c5192d2108834a0e82dcbe8a038d9fc720eb318","id":651284}
{"content":"def _KeyValueToDict(pair):\n  \"\"\"Converts an iterable object of key=value pairs to dictionary.\"\"\"\n  d = dict()\n  for kv in pair:\n    (k, v) = kv.split('=', 1)\n    d[k] = v\n  return d","sha1":"d02a3aa321f804dbf16389f4938e5fcad57bafb5","id":275050}
{"content":"from functools import reduce\n\n\ndef deepGetAttr(obj, path):\n    \"\"\"\n    Resolves a dot-delimited path on an object. If path is not found\n    an `AttributeError` will be raised.\n    \"\"\"\n    return reduce(getattr, path.split('.'), obj)","sha1":"9029369c204e78184066032bd2b264d2af758da5","id":676149}
{"content":"from typing import List\n\n\ndef _subset_of(subset_list: List, lst: List) -> bool:\n    \"\"\"\n    check whether the first list is a subset of the second list\n    \"\"\"\n    return all(elem in lst for elem in subset_list)","sha1":"81099c03c35356e1eb47ffa42ee1c0062b0551ec","id":339885}
{"content":"def get_eso_file_version(raw_version):\n    \"\"\"Return eso file version as an integer (i.e.: 860, 890).\"\"\"\n    version = raw_version.strip()\n    start = version.index(\" \")\n    return int(version[(start + 1) : (start + 6)].replace(\".\", \"\"))","sha1":"d456ce65585a8aad0c08d9e53b6aeb77132a0d49","id":49109}
{"content":"def total_cost(J_content, J_style, alpha = 10, beta = 40):\n    \"\"\"\n    Computes the total cost function\n    \n    Arguments:\n    J_content -- content cost coded above\n    J_style -- style cost coded above\n    alpha -- hyperparameter weighting the importance of the content cost\n    beta -- hyperparameter weighting the importance of the style cost\n    \n    Returns:\n    J -- total cost as defined by the formula above.\n    \"\"\"\n    ###\u00a0START CODE HERE\n    \n    #(\u22481 line)\n    J = alpha*J_content + beta*J_style\n    \n    ###\u00a0START CODE HERE\n\n    return J","sha1":"d05b0cdc0b72ae70c31034db86d691a3999dbf02","id":545848}
{"content":"def array_get(array, idx, fallback):\n    \"\"\"\n    Retrieves the item at position idx in array, or fallback if out of bounds\n    \"\"\"\n    try:\n        return array[idx]\n    except IndexError:\n        return fallback","sha1":"76c4347b5961316cf2ca7386a4015cc155c2bbcb","id":234688}
{"content":"def _parse_color_string(colors, n=None, r=False, start=0, stop=1):\n    \"\"\"\n    Parses strings that are formatted like the following:\n\n    'RdBu_r_start=0.8_stop=0.9_n=10'\n    'viridis_start0.2_r_stop.5_n20'\n    'Greens_start0_n15'\n    \"\"\"\n    if isinstance(colors, str):\n        color_settings = colors.split('_')\n        colors = color_settings[0]\n        for setting in color_settings[1:]:\n            setting = setting.replace('=', '')\n            if setting.startswith('n') and setting[1].isdigit():\n                n = int(setting[1:])\n            elif setting == 'r':\n                r = True\n            elif setting.startswith('start'):\n                start = float(setting[5:])\n            elif setting.startswith('stop'):\n                stop = float(setting[4:])\n    return colors, n, r, start, stop","sha1":"c7ca24e7a8d345f94cdbabd623c60756aeeefe34","id":666492}
{"content":"def format_card(cards):\n    \"\"\"\n    Returns a dictionary with the passed cards rank, color, and suit.\n\n    Args:\n        cards: A list of 2-letter strings representing card rank and suit.\n    \"\"\"\n    formated_cards = []\n\n    for card in cards:\n        if card[0] == 'T':\n            card_dict = {'rank': '10'}\n        else:\n            card_dict = {'rank': card[0]}\n        if card[1] == 'C':\n            card_dict['color'] = 'black'\n            card_dict['suit'] = 'clubs'\n        elif card[1] == 'D':\n            card_dict['color'] = 'red'\n            card_dict['suit'] = 'diams'\n        elif card[1] == 'H':\n            card_dict['color'] = 'red'\n            card_dict['suit'] = 'hearts'\n        elif card[1] == 'S':\n            card_dict['color'] = 'black'\n            card_dict['suit'] = 'spades'\n        formated_cards.append(card_dict)\n\n    return formated_cards","sha1":"efe497b9552b77f109d7096132e5fb465e268b8f","id":448426}
{"content":"def temp_output_file(temp_folder, temp_output_filename):\n    \"\"\"Create a file path to for the output file.\"\"\"\n    return temp_folder.join(temp_output_filename)","sha1":"cbfb5bc72ec4e91dec1eb12b83139a274f518597","id":214895}
{"content":"def pad(lst, length, padding=' '):\n    \"\"\"\n    Pad a list up to length with padding\n    \"\"\"\n    return lst+[padding]*(length-len(lst))","sha1":"162479912484623ba159a6cd1a4579bac918ce39","id":175772}
{"content":"from typing import Callable\n\n\ndef memoize(function: Callable) -> Callable:\n    \"\"\"\n    An extremely simple, general-purpose shallow memoizer.\n    \"\"\"\n    cache = {}\n    def wrapper(*args, **kwargs):\n        key = tuple(args)\n        if key not in cache:\n            cache[key] = function(*args, **kwargs)\n        return cache[key]\n    return wrapper","sha1":"b87d76d15c5089f57cd59adbf1892b3649bf3fa5","id":518102}
{"content":"def get_file_content(filepath):\n    \"\"\"\n    Get the file content in binary mode\n\n    :param filepath: The path to the file\n    :type filepath: str\n    :return: The content of the file\n    :rtype: bytes\n    \"\"\"\n    file = open(filepath, \"rb\")\n    content = file.read()\n    file.close()\n    return content","sha1":"4e037d49ff4e5eaa1ff7c1523d72a7b25e641e05","id":361354}
{"content":"def get_prob_from_odds( odds, outcome ):\n    \"\"\" Get the probability of `outcome` given the `odds` \"\"\"\n    oddFor = odds[ outcome   ]\n    oddAgn = odds[ 1-outcome ]\n    return oddFor \/ (oddFor + oddAgn)","sha1":"b4507dde8285c357cb17c1b749657a2c634efa63","id":656305}
{"content":"def _exclusion_spec_to_json(exclusion_spec):\n    \"\"\"\n    Given an artifact exclusion spec, returns the json serialization of the object.\n    \"\"\"\n    return \"{ \\\"group\\\": \\\"\" + exclusion_spec[\"group\"] + \"\\\", \\\"artifact\\\": \\\"\" + exclusion_spec[\"artifact\"] + \"\\\" }\"","sha1":"7797d1198a8755b7ba3f5dc0d32337bee75f0780","id":476907}
{"content":"def lt_log_determinant(L):\n    \"\"\"\n    Log-determinant of a triangular matrix\n\n    Args:\n        L (Variable): Lower-triangular matrix to take log-determinant of.\n    \"\"\"\n    return L.diag().log().sum()","sha1":"dc7e0a75e71eddb2b284c1cb511d4b9902559371","id":643528}
{"content":"def _isint(string):\n    \"\"\"\n    Checks if a string can be converted into an int.\n\n    Parameters\n    ----------\n    value : str\n\n    Returns\n    -------\n    bool:\n        True\/False if the string can\/can not be converted into an int.\n\n    \"\"\"\n    try:\n        int(string)\n        return True\n\n    except ValueError:\n        return False","sha1":"e22db7b199b7c6cd4e2413e19ad10e69c8daf4dd","id":181539}
{"content":"import math\n\n\ndef approx_equal(x, y, tol=1e-12, rel=1e-7):\n    \"\"\"Test whether x is approximately equal to y, using an absolute error\n    of tol and\/or a relative error of rel, whichever is bigger.\n\n    Pass None as either tol or rel to ignore that test; if both are None,\n    the test performed is an exact equality test.\n\n    tol and rel must be either None or a positive, finite number, otherwise\n    the behaviour is undefined.\n    \"\"\"\n    # Note that the relative error is calculated relative to x only.\n    if tol is rel is None:\n        # Fall back on exact equality.\n        return x == y\n    # Infinities and NANs are special.\n    if math.isnan(x) or math.isnan(y):\n        return False\n    delta = abs(x - y)\n    if math.isnan(delta):\n        # Only if both x and y are the same infinity.\n        assert x == y and math.isinf(x)\n        return True\n    if math.isinf(delta):\n        # Either x and y are both infinities with the opposite sign, or\n        # one is an infinity and the other is finite. Either way, they're\n        # not approximately equal.\n        return False\n    tests = []\n    if tol is not None: tests.append(tol)\n    if rel is not None: tests.append(rel*abs(x))\n    assert tests\n    return delta <= max(tests)","sha1":"7ae3574337350092ed37628cb30e922a478cf514","id":361148}
{"content":"import requests\n\n\ndef get_data_dir(server=\"http:\/\/localhost:5279\"):\n    \"\"\"Return the directory where LBRY stores its data.\"\"\"\n    msg = {\"method\": \"settings_get\"}\n    out_set = requests.post(server, json=msg).json()\n    data_dir = out_set[\"result\"][\"data_dir\"]\n    return data_dir","sha1":"f8cd3e3d17e3264ecc7ea85c2f8e5cd648e64938","id":376103}
{"content":"def get_event_id_missing_feature(con, table, missing_feature):\n    \"\"\"\n    Get the a list of event ids in a given table where data is missing in a specific column (=feature\n    :param con:\n    :param table:\n    :param missing_feature:\n    :return:\n    \"\"\"\n\n    query_missing_data = f\"\"\"SELECT event_id FROM {table} WHERE {missing_feature} is NULL\"\"\"\n\n    with con.cursor() as cursor:\n        cursor.execute(query_missing_data)\n        list_event_ids = [dictionary.get('event_id') for dictionary in cursor.fetchall()]\n\n    return list_event_ids","sha1":"53415f79e0a7afe756868220311489c7409c6585","id":304500}
{"content":"def format_for_null(value):\n    \"\"\"If a Python value is None, we want it to convert to null in json.\"\"\"\n    if value is None:\n        return value\n    else:\n        return \"{}\".format(value)","sha1":"004ada8d8496705c2afd42b82feef3c7a6e38079","id":695627}
{"content":"def exact_match(s1: str, s2: str) -> bool:\n    \"\"\"Test if two strings are exact matches.\n\n    Parameters\n    ----------\n    s1: string\n        Left side string of the comparison.\n    s2: string\n        Right side string of the comparison.\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    return s1 == s2","sha1":"d6e3420e6571f24bc32b83c1155af0f6ef0d2b46","id":60426}
{"content":"def bytesPad(text, size=8, pad=0):\n    \"\"\"Convert a string to bytes and add pad value if necessary to make the length up to size.\n       \"\"\"\n    text_as_bytes = text.encode(\"utf-8\")\n    if len(text_as_bytes) >= size:\n        return text_as_bytes\n    else:\n        return text_as_bytes + bytes([pad] * (size - len(text_as_bytes)))","sha1":"dcade27ff442106436c1ed7f453d1ecb51025f8e","id":441591}
{"content":"def stop_filter(record):\n    \"\"\" A filter function to stop iteration.\n\n    \"\"\"\n    if record[\"int\"] == 789:\n        raise StopIteration\n    return record","sha1":"c0da5e80e77d37ca465f2a1afe4c575a4c223e0c","id":679926}
{"content":"def extract_tree(root, ordered_RD):\n    \"\"\"\n    Extract all nodes from the tree below root node using deepth-first search.\n\n    reture:\n        a list has the structure [[node_id, start_idx, end_idx, average_RD, level_id, parent_id],\\\n                                  [...]...]\n    \"\"\"\n    level = 0\n    node_id = 0\n    parent_id = [-1]\n    to_be_processed = [root]\n    next_level = []\n    next_level_parent_id = []\n    tree = []\n    while len(to_be_processed) > 0:\n        current_node = to_be_processed.pop()\n        next_level.extend(current_node.children)\n\n        # keep track of parent for updating parent id\n        current_parent_id = parent_id.pop()\n        next_level_parent_id.extend([node_id]*len(current_node.children))\n\n        start, end = current_node.index_range\n        ave_RD = current_node.average_RD(ordered_RD)\n        tree.append([node_id, start, end, ave_RD, level, current_parent_id])\n        node_id += 1\n        if len(to_be_processed) == 0: # unless current_children_list is also empty, current notde is refilled and loop continue\n            to_be_processed = next_level\n            parent_id = next_level_parent_id\n            next_level_parent_id = []\n            next_level = []\n            level += 1\n    return tree","sha1":"1b9fe3837d086ae5571b801400a1e8d1e7e7012c","id":603219}
{"content":"def open_file_from_row(filename, row=0):\n    \"\"\"\n    open from a row reference, like some csv files come with their headers starting on later lines\n    \"\"\"\n\n    file = open(filename, encoding='utf-8')\n    for i in range(row):  # skip rows\n        next(file)\n\n    return file","sha1":"1712fb2b18921574c06c09ea7cc0eec499544521","id":358845}
{"content":"def cradmin_instance_url(context, appname, viewname, *args, **kwargs):\n    \"\"\"\n    Template tag implementation of :meth:`cradmin_legacy.crinstance.BaseCrAdminInstance.reverse_url`.\n\n    Examples:\n\n        Reverse the view named ``\"edit\"`` within the app named ``\"pages\"``:\n\n        .. code-block:: htmldjango\n\n            {% load cradmin_legacy_tags %}\n\n            <a href='{% cradmin_instance_url appname=\"pages\" viewname=\"edit\" %}'>\n                Edit\n            <\/a>\n\n        Reverse a view with keyword arguments:\n\n        .. code-block:: htmldjango\n\n            {% load cradmin_legacy_tags %}\n\n            <a href='{% cradmin_instance_url appname=\"pages\" viewname=\"list\" mode=\"advanced\" orderby=\"name\" %}'>\n                Show advanced pages listing ordered by name\n            <\/a>\n    \"\"\"\n    request = context['request']\n    return request.cradmin_instance.reverse_url(\n        appname=appname, viewname=viewname, args=args, kwargs=kwargs)","sha1":"9cea000ff75b68d536796cbcc323fef1b0a7059b","id":699375}
{"content":"def to_set(obj):\n    \"\"\"\n    Converts an object to a set if it isn't already\n    \"\"\"\n\n    if obj is None:\n        return set()\n\n    if isinstance(obj, set):\n        return obj\n\n    if not hasattr(obj, '__iter__') or isinstance(obj, str):\n        obj = [obj]\n\n    return set(obj)","sha1":"3fe3efce00d1bce9b9a5a60fae0c371a25e8259c","id":464346}
{"content":"import functools\n\n\ndef methodcache(f):\n    \"\"\"Utility decorator that ensures the passed function is only called once and the result \n    is cached and reused on future calls. This saves the cached values in the method __cache\n    attribute (only works for methods).\n    \"\"\"\n\n    name = f.__name__\n\n    @functools.wraps(f)\n    def cached(self, *args, **kwargs):\n        if not hasattr(self, \"__cache\"):\n            self.__cache = {}\n\n        if name not in self.__cache:\n            self.__cache[name] = {\"called\": False, \"result\": None}\n\n        if self.__cache[name][\"called\"]:\n            return self.__cache[name][\"result\"]\n\n        result = f(self, *args, **kwargs)\n        self.__cache[name] = {\"called\": True, \"result\": result}\n        return result\n\n    return cached","sha1":"8b538245836431a914ca2042b1ae2d502675e99a","id":319781}
{"content":"from typing import Dict\n\n\ndef filter_role(role: Dict[str, str]) -> Dict[str, str]:\n    \"\"\" Remove unwanted keys to save space. \"\"\"\n    unwanted = [\"description\"]\n    filtered: Dict[str, str] = {}\n    for k in role.keys():\n        if k not in unwanted:\n            filtered[k] = role[k]\n    return filtered","sha1":"846c6e12c40be29c9ad2ae094dff16215b4280ed","id":507887}
{"content":"def win_split(cmdline):\n    \"\"\" Minimal implementation of shlex.split for Windows following\n    https:\/\/msdn.microsoft.com\/en-us\/library\/windows\/desktop\/17w5ykft.aspx.\n    \"\"\"\n    def split_iter(cmdline):\n        in_quotes = False\n        backslashes = 0\n        arg = ''\n        for c in cmdline:\n            if c == '\\\\':\n                # MSDN: Backslashes are interpreted literally, unless they\n                # immediately precede a double quotation mark.\n                # Buffer them until we know what comes next.\n                backslashes += 1\n            elif c == '\"':\n                # Quotes can either be an escaped quote or the start of a quoted\n                # string. Paraphrasing MSDN:\n                # Before quotes, place one backslash in the arg for every pair\n                # of leading backslashes. If the number of backslashes is odd,\n                # retain the double quotation mark, otherwise interpret it as a\n                # string delimiter and switch state.\n                arg += '\\\\' * (backslashes \/\/ 2)\n                if backslashes % 2 == 1:\n                    arg += c\n                else:\n                    in_quotes = not in_quotes\n                backslashes = 0\n            elif c in (' ', '\\t') and not in_quotes:\n                # MSDN: Arguments are delimited by white space, which is either\n                # a space or a tab [but only outside of a string].\n                # Flush backslashes and return arg bufferd so far, unless empty.\n                arg += '\\\\' * backslashes\n                if arg:\n                    yield arg\n                    arg = ''\n                backslashes = 0\n            else:\n                # Flush buffered backslashes and append.\n                arg += '\\\\' * backslashes\n                arg += c\n                backslashes = 0\n\n        if arg:\n            arg += '\\\\' * backslashes\n            yield arg\n\n    return list(split_iter(cmdline))","sha1":"d1aa8230c36fa52537c0b6f8d8e2bcf89513a5ea","id":421556}
{"content":"def _extract_license_outliers(license_service_output):\n    \"\"\"Extract license outliers.\n\n    This helper function extracts license outliers from the given output of\n    license analysis REST service.\n\n    :param license_service_output: output of license analysis REST service\n    :return: list of license outlier packages\n    \"\"\"\n    outliers = []\n    if not license_service_output:\n        return outliers\n\n    outlier_packages = license_service_output.get('outlier_packages', {})\n    for pkg in outlier_packages.keys():\n        outliers.append({\n            'package': pkg,\n            'license': outlier_packages.get(pkg, 'Unknown')\n        })\n\n    return outliers","sha1":"101a916fec08a3a5db1a09a2817e82314ca19f6b","id":701032}
{"content":"async def say(message: str):\n    \"\"\"Echos the specified message\"\"\"\n    return {\"message\": message}","sha1":"6eb775a1b84a9df85c0b28e789c5240c421f88bd","id":407667}
{"content":"def extract_state_fips(fips: str) -> str:\n    \"\"\"Extracts the state FIPS code from a county or state FIPS code.\"\"\"\n    return fips[:2]","sha1":"241bab8540c32c4e41597723bd42bc7b35a7d303","id":639092}
{"content":"def add_to_set(entries, new):\n    \"\"\"Adds an entry to a list and makes a set for uniqueness before returning the list.\n\n    Args:\n        entries: `list`.\n        new: (any datatype) The new member to add to the list.\n\n    Returns:\n        `list`: A deduplicated list.\n    \"\"\"\n    entries.append(new)\n    unique_list = list(set(entries))\n    return unique_list","sha1":"7fcff194d67850d0f7ace408e5bbbe908b71886b","id":595668}
{"content":"import torch\n\n\ndef log_sum_exp(value, dim=None, keepdim=False):\n    \"\"\"Numerically stable implementation of the operation\n    value.exp().sum(dim, keepdim).log()\n    \"\"\"\n    if dim is not None:\n        m, _ = torch.max(value, dim=dim, keepdim=True)\n        value0 = value - m\n        if keepdim is False:\n            m = m.squeeze(dim)\n        return m + torch.log(torch.sum(torch.exp(value0), dim=dim, keepdim=keepdim))\n    else:\n        m = torch.max(value)\n        sum_exp = torch.sum(torch.exp(value - m))\n    return m + torch.log(sum_exp)","sha1":"d8b519bb70437b6e710ea81540ce734dd5d8359b","id":609907}
{"content":"def create_html_url_href(url: str) -> str:\n    \"\"\"\n    HTML version of a URL\n    :param url: the URL\n    :return: URL for use in an HTML document\n    \"\"\"\n    return f'<a href=\"{url}\">{url}<\/a>' if url else \"\"","sha1":"5c41f5035a2b549e9c969eccbcc960a100600e7e","id":693824}
{"content":"def all(*args, span=None):\n    \"\"\"Create a new expression of the intersection of all conditions in the\n      arguments\n\n    Parameters\n    ----------\n    args : list\n        List of symbolic boolean expressions\n\n    span : Optional[Span]\n        The location of this operator in the source code.\n\n    Returns\n    -------\n    expr: Expr\n        Expression\n    \"\"\"\n    if not args:\n        raise ValueError(\"Any must take at least 1 argument\")\n    if len(args) == 1:\n        return args[0]\n    val = _ffi_api._OpAnd(args[0], args[1], span)  # type: ignore\n    for i in range(2, len(args)):\n        val = _ffi_api._OpAnd(val, args[i], span)  # type: ignore\n    return val","sha1":"f0cebfb241c10c2d53c58a8b4fb186e9d65a1b7a","id":9430}
{"content":"def _read() -> str:\n    \"\"\"Returns a query term matching messages that are read.\"\"\"\n\n    return 'is:read'","sha1":"8465780f45172c83a1e449bf5d52e60af64e8013","id":238945}
{"content":"def line(p1, p2):\n    \"\"\"Converts line from a 2-point representation into a 3-parameter representation.\"\"\"\n    A = (p1[1] - p2[1])\n    B = (p2[0] - p1[0])\n    C = (p1[0]*p2[1] - p2[0]*p1[1])\n    return A, B, -C","sha1":"141b8f1dc1d78f7fd1606ff8d3d88fd34465ba05","id":425102}
{"content":"import json\n\n\ndef parse_as_list(input_text):\n    \"\"\"Parse given input as JSON or comma-separated list.\"\"\"\n    try:\n        return json.loads(input_text)\n    except json.decoder.JSONDecodeError:\n        return [s.strip() for s in input_text.split(',')]","sha1":"79fa9d43bd144b3ca1689adc8fb94ce30b7f4659","id":70691}
{"content":"def summary_global_quantities(ods, update=True):\n    \"\"\"\n    Calculates global quantities for each time slice and stores them in the summary ods:\n     - Greenwald Fraction\n     - Energy confinement time estimated from the IPB98(y,2) scaling\n     - Integrate power densities to the totals\n     - Generate summary.global_quantities from global_quantities of other IDSs\n\n    :param ods: input ods\n\n    :param update: operate in place\n\n    :return: updated ods\n    \"\"\"\n    ods_n = ods.physics_summary_greenwald(update=update)\n    ods_n.physics_summary_taue(update=True)\n    ods_n.physics_summary_heating_power(update=True)\n    ods_n.physics_summary_consistent_global_quantities(update=True)\n    return ods_n","sha1":"f70e9d233fc457e495e3ebb5ee46faaec38ad98a","id":636865}
{"content":"def mCVTC(molFrac, vTC, vV, fW, eps=1.0):\n    \"\"\"\n    mCVTC(molFrac, vTC, vV, fW, eps=1.0)\n    \n    multi-Component Vapor Thermal Conductivity in W\/m\/K\n\n    Parameters:\n        molFrac, list of mol fractions\n        vTC, list of pure component vapor thermal conductivities (components \n             ordered as in molFrac, fW, and vV) \n        vV, list of pure component vapor viscosities (components ordered as\n            in molFrac, fW, and vTC)\n        fW, list of formula weights for each component in molFrac\n        eps, epsilon - typical value is 1.0\n    \n    Returns:\n        multi-component vapor thermal conductivity in W\/m\/K\n    \"\"\"\n    tCm = 0\n    Aij = []\n    nMF = len(molFrac)\n    \n    for idx in range(nMF):\n        Aij.append([])\n        sumyjAij = 0\n        for jdx in range(nMF):\n            fWR=fW[idx]\/fW[jdx]\n            vR = vV[idx]\/vV[jdx]\n            if idx == jdx:\n                sumyjAij += molFrac[jdx]\n            elif idx < jdx:\n                Aij[idx].append(eps*(1+vR**(1.\/2)*(1\/fWR)**(1.\/4))**2\/(8*(1+fWR))**(1.\/2))\n                sumyjAij += molFrac[jdx]*Aij[idx][jdx-(1+idx)]\n            else:\n                sumyjAij += molFrac[jdx]*Aij[jdx][idx-(1+jdx)]*vR\/fWR\n        tCm += molFrac[idx]*vTC[idx]\/sumyjAij\n    \n    return tCm","sha1":"f01e234d8cda556c11c54b4272ed4e83d6d5fcdb","id":595422}
{"content":"import math\n\n\ndef entropy(probas):\n    \"\"\"\n    Negative of sum of p * lg (p)\n    >>> entropy([.5, .5])\n    1.0\n    >>> entropy([.5, .5, .5])\n    1.5\n    >>> entropy([1., 0.])\n    -0.0\n    >>> entropy([.5, .5, 0.])\n    1.0\n    >>> entropy([.25, .25, .25, .25])\n    2.0\n    \"\"\"\n    return -sum(p * math.log(p, 2.) for p in probas if p != 0.)","sha1":"0a65bb69cfeed76132e6fd2db1803793e56d946f","id":296954}
{"content":"def mode_tags_equivalent(l, r):\n    \"\"\"Returns whether two modes are equivalent for Go build tags. For example,\n    goos and goarch must match, but static doesn't matter.\"\"\"\n    return (l.goos == r.goos and\n            l.goarch == r.goarch and\n            l.race == r.race and\n            l.msan == r.msan)","sha1":"56a732e53b26be343d3397ab497b67a0d23ed880","id":296604}
{"content":"def bytes_to_list(input_bytes, block_len):\n    \"\"\"\u5c06\u5b57\u8282\u4e32\u5207\u5206\u6210\u6307\u5b9a\u957f\u5ea6\u7684\u5b50\u4e32\n\n    Args:\n        input_bytes (bytes): \u8f93\u5165\u5b57\u8282\u4e32\n        block_len (int): \u5b50\u4e32\u957f\u5ea6\n\n    Returns:\n        list: \u5207\u5206\u540e\u7684\u5b57\u8282\u4e32\u6570\u7ec4\n    \"\"\"\n    block_len = int(block_len)\n    return [\n        input_bytes[i:i + block_len]\n        for i in range(0, len(input_bytes), block_len)\n    ]","sha1":"e89d3de3bca21571068e3dae767144565c21cb85","id":476948}
{"content":"def create_huc_ids_list(np_array_huc_ids):\n    \"\"\"\n    Create a Python list of HUC_IDs from the SuperPRZM output numpy array of HUC_IDs\n    :param np_array_huc_ids: numpy array, [['0', '5', '0', '0', '1', '1', '0', '5', '4', '2', '1', '1'], [...]]\n    :return: list\n    \"\"\"\n    out_list = []\n    for char_array in np_array_huc_ids:\n        out_list.append(\"\".join(list(char_array)))\n\n    return out_list","sha1":"dacd186bf53b91421cb8041a3ed1d89e2b01a2b1","id":126405}
{"content":"def dotted_ip_to_bytes(ip: str) -> bytes:\n    \"\"\"\n    Convert a dotted IPv4 address string into four bytes, with\n    some sanity checks\n    \"\"\"\n    ip_ints = [int(i) for i in ip.split(\".\")]\n    if len(ip_ints) != 4 or any(i < 0 or i > 255 for i in ip_ints):\n        raise ValueError\n    return bytes(ip_ints)","sha1":"2108b329fb8b7d95d7b5eb2a5ba7a4647a29129f","id":399466}
{"content":"def merge(job_settings, default_settings):\n    \"\"\" Merge two dicts\n\n    :param job_settings:\n    :param default_settings:\n    :return:\n    \"\"\"\n    settings = default_settings\n    if settings is None:\n        settings = {}\n    settings.update(job_settings)\n    return settings","sha1":"6afd35bc0fadfddf32baf13f8b07b56e7bd64b68","id":326979}
{"content":"def is_expected_event(event_to_check, events_list):\n    \"\"\" check whether event is present in the event list\n\n    Args:\n        event_to_check: event to be checked.\n        events_list: list of events\n    Returns:\n        result: True if event present in the list. False if not.\n    \"\"\"\n    for event in events_list:\n        if event in event_to_check['name']:\n            return True\n    return False","sha1":"72d59b1e8a1f7fd8030b156e7521b051576cc4ef","id":448709}
{"content":"def safe_run(func):\n    \"\"\" Decorator to run a method wrapped in a try\/except -> returns None upon exception \"\"\"\n\n    def func_wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            return None\n\n    return func_wrapper","sha1":"3c16cc97ddc62d4ffd9adcd11f39173340c0cd3d","id":260659}
{"content":"from datetime import datetime\n\n\ndef _update_spec(additional_spec):\n    \"\"\"\n    Update spec to overwrite default settings.\n\n    Args:\n        additional_spec (dict): user spec settings.\n            \"is_optimized\" (bool): If True, the given structures are assumed to be optimized.\n                            Otherwise relaxation will be applied to the given structures.\n                            Default is False.\n            \"interpolation_type\" (str): The approach to generate images between the two endpoints.\n                            Default approach is \"IDPP\" (image dependent pair potential approach),\n                            otherwise \"linear\" (conventional linear interpolation approach).\n            \"idpp_species\" ([str]): Species used in IDPP method.\n            \"sort_tol\" (float): Distance tolerance (in Angstrom) used to match the atomic indices\n                            between start and end structures. If it is set 0, no sorting will be\n                            performed.\n            \"d_img\" (float): The distance between two adjacent images, in Angstrom. If \"IMAGES\" is\n                            not provided in user_incar_settings, this will be used to compute the\n                            number of images. Default is 0.7 Angstrom.\n            \"wf_name\" (str): An appropriate and unique name for the workflow. The workflow result\n                            will be transferred to \">>run_dest_root<<\/wf_name\".\n            \"neb_walltime (str)\": Additional time limit setting for NEB calculations. For example,\n                            {\"neb_walltime\": \"10:00:00\"} sets 10 hours walltime for all NEB\n                            calculations. Default is None, which uses fireworks configuration.\n\n    Returns:\n        spec dict\n    \"\"\"\n    additional_spec = additional_spec or {}\n    default_spec = {\n        \"is_optimized\": False,\n        \"interpolation_type\": \"IDPP\",\n        \"idpp_species\": None,\n        \"sort_tol\": 0,\n        \"d_img\": 0.7,\n        \"wf_name\": datetime.utcnow().strftime(\"%Y-%m-%d-%H-%M-%S-%f\"),\n        \"neb_walltime\": None,\n    }\n    default_spec.update(additional_spec)\n    return default_spec","sha1":"d957e5d53120f452a4b0f54d8df31f539c11e9e6","id":448567}
{"content":"def unique_subjects_sessions_to_subjects_sessions(\n    unique_subject_list, per_subject_session_list\n):\n    \"\"\"Do reverse operation of get_unique_subjects function.\n\n    Example:\n        >>> from clinicaml.utils.participant import unique_subjects_sessions_to_subjects_sessions\n        >>> unique_subjects_sessions_to_subjects_sessions(['sub-01', 'sub-02'], [['ses-M00', 'ses-M18'], ['ses-M00']])\n        (['sub-CLNC01', 'sub-01', 'sub-02'], ['ses-M00', 'ses-M18', 'ses-M00'])\n\n    \"\"\"\n    list_participants = []\n    list_sessions = []\n    for idx, participant_id in enumerate(unique_subject_list):\n        for session_id in per_subject_session_list[idx]:\n            list_participants.append(participant_id)\n            list_sessions.append(session_id)\n\n    return list_participants, list_sessions","sha1":"862a9b81e0e342a629722808c77baab60444200c","id":434764}
{"content":"from typing import Union\nimport inspect\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import Any\n\n\ndef add_named_kwargs_to_signature(\n    func_or_signature: Union[inspect.Signature, Callable],\n    kwargs: Dict[str, Any]\n) -> inspect.Signature:\n    \"\"\"\n    Add named keyword arguments with default values to a function signature.\n\n    Parameters\n    ----------\n    func_or_signature : inspect.Signature or callable\n        The function or signature.\n\n    kwargs : dict\n        The dictionary of kwarg_name to default_value.\n\n    Returns\n    -------\n    modified_signature : inspect.Signature\n        The modified signature with additional keyword arguments.\n    \"\"\"\n\n    if isinstance(func_or_signature, inspect.Signature):\n        sig = func_or_signature\n    else:\n        sig = inspect.signature(func_or_signature)\n\n    params = list(sig.parameters.values())\n    keyword_only_indices = [\n        idx for idx, param in enumerate(params)\n        if param.kind == inspect.Parameter.KEYWORD_ONLY\n    ]\n    if not keyword_only_indices:\n        start_params, end_params = params, []\n    else:\n        insert_at = keyword_only_indices[0]\n        start_params, end_params = params[:insert_at], params[insert_at:]\n\n    wrapper_params = list(\n        inspect.Parameter(\n            name, kind=inspect.Parameter.KEYWORD_ONLY, default=value\n        )\n        for name, value in kwargs.items()\n        if name not in sig.parameters\n    )\n\n    return sig.replace(parameters=start_params + wrapper_params + end_params)","sha1":"50975f39fc9d39225ec4cab2882ea8c50309973d","id":162934}
{"content":"def ABCDFrequencyList_to_HFrequencyList(ABCD_frequency_list):\n    \"\"\" Converts ABCD parameters into h-parameters. ABCD-parameters should be in the form [[f,A,B,C,D],...]\n    Returns data in the form\n    [[f,h11,h12,h21,h22],...]\n    \"\"\"\n    h_frequency_list=[]\n    for row in ABCD_frequency_list[:]:\n        [frequency,A,B,C,D]=row\n        h11=B\/D\n        h12=(A*D-B*C)\/D\n        h21=-1\/D\n        h22=C\/D\n        h_frequency_list.append([frequency,h11,h12,h21,h22])\n    return h_frequency_list","sha1":"d4f54e6864a34b8b24b1afe599a9338be52f29fd","id":31250}
{"content":"def _add_text_axes(axes, text):\n    \"\"\"Use a given axes to place given text.\"\"\"\n\n    txt = axes.text(0.5, 0.5, text, ha='center', va='center')\n    axes.axis('off')\n\n    return txt","sha1":"b8370a92a3c589044e45a5d295015e3afd5f35e2","id":370534}
{"content":"def undifference(first, series):\n    \"\"\"\n    Undifferences a series\n    :param first: the first actual value in the series\n    :param series: the series of differences\n    :return: undifferenced series\n    \"\"\"\n    undifferenced = [first]\n    for num in series:\n        lastItem = undifferenced[-1]\n        undifferenced.append(lastItem + num)\n    return undifferenced[1:]","sha1":"8cdec9fd2cead16c2bdb2924109d7f52fd9a89ec","id":278436}
{"content":"def lobes(file):\n    \"\"\"\n    Reads LOBES calibration text files and returns variables.\n    \n    Parameters\n    ----------\n    file: str\n        path\/to\/lobes\/calibration\/file.txt\n\n    Returns\n    -------\n    d: dict\n        Contains data variables from the calibration LOBES file.\n\n    \"\"\"\n    \n    # open LOBES calibration file and extract variables, line by line\n    d = {}\n    f = open(file, 'r')    \n    while 1:\n        l = f.readline()\n        if not ('#' in l[0]):\n            break\n        \n        if '#  Calibration  Version' in l:\n            d['calibration_version'] = l.strip('#  Calibration  Version').strip()\n            \n        if '#  Date:' in l:\n            d['calibration_date'] = l.strip('#  Date:').strip()\n        \n        if '#  Comments:' in l:\n            l = f.readline()\n            d['calibration_coments'] = l.strip('#').strip()\n            \n        if '#  Reference Target:' in l:\n            l = f.readline()\n            d['reference_TS']           = float(l.split()[2])\n            d['reference_min_distance'] = float(l.split()[6])\n            l = f.readline()\n            d['reference_TS_deviation'] = float(l.split()[3])\n            d['reference_max_distance'] = float(l.split()[7])\n                    \n        if '#  Transducer:' in l:\n            d['instrument_transducer_model']                        = l.split()[2]\n            d['instrument_transducer_serial']                       = l.split()[5]        \n            l = f.readline()\n            d['instrument_transducer_frequency']                    = int(l.split()[2])\/1e3\n            d['instrument_transducer_beam_type']                    = l.split()[5]        \n            l = f.readline()\n            d['instrument_transducer_gain']                         = float(l.split()[2])\n            d['instrument_transducer_psi']                          = float(l.split()[8])        \n            l = f.readline()\n            d['instrument_transducer_beam_angle_major_sensitivity'] = float(l.split()[4])\n            d['instrument_transducer_beam_angle_minor_sensitivity'] = float(l.split()[8])                \n            l = f.readline()\n            d['instrument_transducer_beam_angle_major']             = float(l.split()[4])\n            d['instrument_transducer_beam_angle_minor']             = float(l.split()[9])        \n            l = f.readline()\n            d['instrument_transducer_beam_angle_major_offset']      = float(l.split()[4])\n            d['instrument_transducer_beam_angle_minor_offset']      = float(l.split()[9])\n            \n        if'#  Transceiver:' in l:\n            d['instrument_transceiver_model']          = l.split()[-1]\n            d['instrument_transceiver_serial']         = l.split()[5]        \n            l = f.readline()\n            d['data_processing_transmit_pulse_length'] = float(l.split()[3])\n            d['data_range_axis_interval_value']        = float(l.split()[7])\n            d['data_range_axis_interval_type']         = 'Range (metres)'\n            l = f.readline()\n            d['data_processing_transceiver_power']     = float(l.split()[2])\n            d['data_processing_bandwidth']             = float(l.split()[6])\n            \n        if '#  Sounder Type:' in l:\n            l = f.readline()\n            d['data_processing_software_version'] = l.split('#')[1].strip()\n            \n        if '#  TS Detection:' in l:\n            l = f.readline()\n            d['target_detection_minimum_value']           = float(l.split()[3])\n            d['target_detection_minimum_spacing']         = float(l.split()[7])\n            l = f.readline()\n            d['target_detection_beam_compensation']       = float(l.split()[4])\n            d['target_detection_minimum_ecolength']       = float(l.split()[8])\n            l = f.readline()\n            d['target_detection_maximum_phase_deviation'] = float(l.split()[4])\n            d['target_detection_maximum_echolength']      = float(l.split()[7])\n            \n        if '#  Environment:' in l:\n            l                       = f.readline()\n            d['calibration_absorption']  = float(l.split()[3])\/1000\n            d['calibration_sound_speed'] = float(l.split()[7])\n            \n        if '#  Beam Model results:' in l:\n            l = f.readline()\n            d['data_processing_on_axis_gain']           = float(l.split()[4])\n            d['data_processing_on_axis_gain_units']     = 'dB'\n            d['data_processing_Sacorrection']           = float(l.split()[8])\n            l = f.readline()\n            d['data_procesing_beam_angle_major']        = float(l.split()[5])\n            d['data_procesing_beam_angle_minor']        = float(l.split()[11])\n            l = f.readline()\n            d['data_procesing_beam_angle_major_offset'] = float(l.split()[5])\n            d['data_procesing_beam_angle_minor_offset'] = float(l.split()[11])\n            \n    # return data dictionary    \n    return d","sha1":"f1ead3d1005bfa7d67bdbbe777e1c01dd1d40f57","id":441059}
{"content":"import torch\n\n\ndef init_center_c(train_loader, dataset_name, net, device, eps=0.1,):\n    \"\"\"Initialize hypersphere center c as the mean from an initial forward pass on the data.\"\"\"\n    n_samples = 0\n    \n    if dataset_name == \"mnist\":\n        tnum = 32\n    elif dataset_name == \"cifar10\":\n        tnum = 96*2*2\n    elif dataset_name == \"ivc-filter\" or dataset_name == \"rsna\":\n        tnum = 512\n    else:\n        tnum = 0\n    c = torch.zeros(tnum, device=device)\n\n    net.eval()\n    with torch.no_grad():\n        for idx, inputs in enumerate(train_loader):\n            # get the inputs of the batch\n            data, _ = inputs\n            org_imgs, _ = data\n            org_imgs = org_imgs.to(device)\n            outputs = net.get_embedding(org_imgs)\n            n_samples += outputs.shape[0]\n            c += torch.sum(outputs, dim=0)\n\n    c \/= n_samples\n\n    # If c_i is too close to 0, set to +-eps. Reason: a zero unit can be trivially matched with zero weights.\n    c[(abs(c) < eps) & (c < 0)] = -eps\n    c[(abs(c) < eps) & (c > 0)] = eps\n\n    return c","sha1":"720a4379caa8bc814da5d636372d2f2c2db6db71","id":159454}
{"content":"import torch\n\n\ndef mask_dt_loss(proj_verts, dist_transf):\n    \"\"\"\n    proj_verts: B x N x 2\n    (In normalized coordinate [-1, 1])\n    dist_transf: B x 1 x N x N\n\n    Computes the distance transform at the points where vertices land.\n    \"\"\"\n    # Reshape into B x 1 x N x 2\n    sample_grid = proj_verts.unsqueeze(1)\n    # B x 1 x 1 x N\n    dist_transf = torch.nn.functional.grid_sample(dist_transf, sample_grid, padding_mode='border')\n    return dist_transf.mean()","sha1":"210ac2ba39a52d6dc0dcca991844aefce46a9c19","id":265063}
{"content":"def filesafe(str_):\n    \"\"\"Convert a string to something safe for filenames.\"\"\"\n    return \"\".join(c for c in str_ if c.isalnum() or c in (' ', '.', '_', '-')).rstrip()","sha1":"925e163ef6e56e313360a3fd6e57bb1f84f38cb6","id":205955}
{"content":"import re\n\n\ndef rm_markers_ann(text):\n    \"\"\"Remove page annotation and replace footnotesmarker with #.\n\n    Args:\n        text (str): diff applied text\n\n    Returns:\n        str: diff applied text without page annotation and replaced footnotesmarker with #\n    \"\"\"\n    result = \"\"\n    lines = text.splitlines()\n    for line in lines:\n        line = re.sub(\"<p.+?>\", \"\", line)\n        line = re.sub(\"<.+?>\", \"#\", line)\n        result += line + \"\\n\"\n    return result","sha1":"00e06177118b64d1b86969631ecf402bf7455b5c","id":164336}
{"content":"def average_saccades_time(saccades_times):\n    \"\"\"\n\n    :param saccades_times:  a list of tuples with (start_time_inclusive, end_time_exclusive)\n    :return:                returns the average time of saccades\n    \"\"\"\n\n    return sum([saccade_time[1] - saccade_time[0] for saccade_time in saccades_times]) \/ len(saccades_times)","sha1":"a22a5d89ddd4317fa10ed6f5d920f17560028514","id":1625}
{"content":"import unicodedata\n\n\ndef normalize(grapheme: str) -> str:\n    \"\"\"\n    Normalize the string representation of a grapheme.\n\n    Currently, normalization involves NFD Unicode normalization and stripping any leading\n    and trailing white spaces. As these opeations might change in the future, it is\n    suggested to always use this function instead of reimplementing it in code\n    each time.\n\n    @param grapheme: The grapheme to be normalized.\n    @return: The normalized version of the grapheme.\n    \"\"\"\n\n    return unicodedata.normalize(\"NFD\", grapheme).strip()","sha1":"8e5345a20cef83890ca8042d8ec418399fdb38e4","id":443552}
{"content":"def read_file_lines(filename):\n    \"\"\"\n    opens and read file lines from file system\n    \n    returns file lines as list if ok\n    returns None is error while reading file\n    \"\"\"\n    try:\n        fhnd = open(filename)\n        lines = fhnd.readlines()\n        fhnd.close()\n        return lines\n    except: \n        return None","sha1":"49714dfa25a45db3739135e79d81f4492a2d83cf","id":361456}
{"content":"def pop_id(attrs):\n    \"\"\"\n    Removes and returns 'id' from the passed-in dictionary, if present.\n    \"\"\"\n    if 'id' in attrs:\n        return attrs.pop('id')","sha1":"3acde358769c4ca60b0c882c25d31baaa2ce037f","id":367666}
{"content":"def from_string(val):\n    \"\"\"escape a python string\"\"\"\n    escape = val.translate(\n        str.maketrans({\n            \"'\": \"''\",\n            '\\\\': '\\\\\\\\',\n            '\\0': '\\\\0',\n            '\\b': '\\\\b',\n            '\\n': '\\\\n',\n            '\\r': '\\\\r',\n            '\\t': '\\\\t',\n            # ctrl z: windows end-of-file; escaped z deprecated in python\n            '\\x1a': '\\\\x1a',\n        }))\n    return f\"'{escape}'\"","sha1":"539e7ab6cb1ba9f05a4ae40162166989936bc45c","id":178408}
{"content":"def is_array(signature):\n    \"\"\"Return True if this argument is an array. A dictionary is considered an array.\"\"\"\n    return signature[0] == \"a\"","sha1":"f75a29b8beea875ba30c635caabe7cc16b85611f","id":219902}
{"content":"import inspect\n\n\ndef keyword(name=None, tags=(), types=()):\n    \"\"\"Decorator to set custom name, tags and argument types to keywords.\n\n    This decorator creates ``robot_name``, ``robot_tags`` and ``robot_types``\n    attributes on the decorated keyword function or method based on the\n    provided arguments. Robot Framework checks them to determine the keyword's\n    name, tags, and argument types, respectively.\n\n    Name must be given as a string, tags as a list of strings, and types\n    either as a dictionary mapping argument names to types or as a list\n    of types mapped to arguments based on position. It is OK to specify types\n    only to some arguments, and setting ``types`` to ``None`` disables type\n    conversion altogether.\n\n    If the automatic keyword discovery has been disabled with the\n    :func:`library` decorator or by setting the ``ROBOT_AUTO_KEYWORDS``\n    attribute to a false value, this decorator is needed to mark functions\n    or methods keywords.\n\n    Examples::\n\n        @keyword\n        def example():\n            # ...\n\n        @keyword('Login as user \"${user}\" with password \"${password}\"',\n                 tags=['custom name', 'embedded arguments', 'tags'])\n        def login(user, password):\n            # ...\n\n        @keyword(types={'length': int, 'case_insensitive': bool})\n        def types_as_dict(length, case_insensitive):\n            # ...\n\n        @keyword(types=[int, bool])\n        def types_as_list(length, case_insensitive):\n            # ...\n\n        @keyword(types=None])\n        def no_conversion(length, case_insensitive=False):\n            # ...\n    \"\"\"\n    if inspect.isroutine(name):\n        return keyword()(name)\n\n    def decorator(func):\n        func.robot_name = name\n        func.robot_tags = tags\n        func.robot_types = types\n        return func\n\n    return decorator","sha1":"90bd90f4de9adafd657dba60c56cf4ffe74de097","id":664344}
{"content":"def bytes_in_context(data, index):\n    \"\"\"Helper method to display a useful extract from a buffer.\"\"\"\n    start = max(0, index - 10)\n    end = min(len(data), index + 15)\n    return data[start:end]","sha1":"b20b35c4a3aaaa8fb81a90ecae775d758f6d6114","id":124407}
{"content":"def checksum(command):\n    \"\"\"\n    Calculate the UBX checksum for a given command\n    \"\"\"\n    ck_a = 0\n    ck_b = 0\n    for b in command[2:]:\n        ck_a += b\n        ck_b += ck_a\n    return [ ck_a % 256, ck_b % 256  ]","sha1":"63e0e7d82c27a168966816d1032690991123d700","id":405935}
{"content":"import struct\n\n\ndef parse_boolean(s):\n  \"\"\"Parses a single boolean value from a single byte\"\"\"\n  return struct.unpack('<?', s)[0]","sha1":"09322cbca1ab9cb04fce03ee3ca976bbef2fa53b","id":103734}
{"content":"def get_word_spacer(word_limit):\n    \"\"\"\n    Return a word spacer as long as the word limit exists and the user\n    doesn't want to quit.\n    \"\"\"\n    while word_limit:\n        spacer = input('Enter a symbol to separate the words in your passphrase'\n                       '\\n(Hit space bar and enter for no symbols): ')\n        if spacer == 'q':\n            word_limit = False\n        else:\n            return spacer","sha1":"3cfff8c5fb7551f588955b63e0cdb4796c98b607","id":334595}
{"content":"def url_path_param_quoting(param):\n    \"\"\"Quote URL path parameters\n\n    Convert '\/' to _FORWARDSLASH_ - otherwise is interpreted as additional path parameter\n        gunicorn processes the path prior to Falcon and interprets the\n        correct quoting of %2F into a slash\n    \"\"\"\n    return param.replace(\"\/\", \"_FORWARDSLASH_\")","sha1":"9bb82b8756307f5d630c8535904791a5d1d81f97","id":590940}
{"content":"def Top5Criterion(x,y, model):\n    \"\"\"Returns True if model prediction is in top5\"\"\"\n    return (model(x).topk(5)[1]==y.view(-1,1)).any(dim=-1)","sha1":"2442e70360a22ecd538aa378baeadf44b431213f","id":397751}
{"content":"def _to_list(obj):\n    \"\"\"Put obj in list if it is not a list.\"\"\"\n    if not isinstance(obj, list):\n        return [obj]\n    else:\n        return obj","sha1":"3b6888428f8f55a627e52bb13c9a5ea44528669f","id":40709}
{"content":"def profit_filler(df, max_length):\n    \"\"\"Takes a dataframe and ensures all the profit rows are the same length for\n    adding together profits over the number of games, shorter rows will be\n    filled with their final value.\n    example: row 0 = [1,2,3,4], row 1 = [1,2,3] -> row 0 = [1,2,3,4],\n    row 1 = [1,2,3,3]\n\n    Parameters:\n    df (pandas.core.frame.DataFrame): data frame containing the simulation\n    results\n    max_length (int): The longest set of results in the simulation\n\n    Returns:\n    pandas.core.frame.DataFrame: The simulation results converted to be used\n    for calculating profit. All runs being the same length with the shorter\n    rows maintaining its final value.\n    \"\"\"\n    spin = [i for i in range(1, max_length + 1)]\n    data = df\n    for i in range(len(data['profit'])):\n        size = max_length - len(data['profit'][i])\n        data['profit'][i].extend([data['profit'][i][-1]]*size)\n        data['spin'][i] = spin\n\n    return data","sha1":"0cc6e787f46fffdbc9890da408ead248d49e1ac6","id":668011}
{"content":"def createId(df1, dest_col, cols):\n    \"\"\"\n\n    Append flight IDs to the input table (usually the FSU table).\n\n    Parameters:\n\n       df1 (pandas.DataFrame) : input dataframe to enhance with flight identifiers\n       dest_col (string) base name of the identifier column\n       cols (list(string)) : columns from which to form the identifier\n\n    Return:\n       A dataframe with one additional identifier column.\n\n    The original input remains intact. The number of rows in the returned dataframe\n    is identical to the number of rows in the input dataframe.\n\n    \"\"\"\n    df = df1.copy()\n    df[dest_col] = df[cols[0]]\n    for i in range(1,len(cols)):\n        df[dest_col] += df[cols[i]]\n    return df","sha1":"87777a104d531d5e3482e72fd6085be010662d16","id":49672}
{"content":"def _get_loadings(params, info, dimensions):\n    \"\"\"Create the array of factor loadings.\"\"\"\n    return params[info[\"loadings\"]].reshape(-1, dimensions[\"n_states\"])","sha1":"27ab3a77660681bfb1da69c94658c831dd9d4f9c","id":231855}
{"content":"def totald(sol, d):\n    \"\"\"A quick procedure for calclulating the total demand of a solution\n    (or a route).\"\"\"\n    if not d: return 0\n    return sum( d[n] for n in sol )","sha1":"44fdb78898cc35a6ce844285c1c24a8005c4b78d","id":247470}
{"content":"def inverse_image(img):\n    \"\"\"\n    Inverse the image\n    Black becomes white\n    White becomes black\n    \"\"\"\n    for i in range(img.shape[0]):\n        for j in range(img.shape[1]):\n            if img[i, j] > 255\/2.0:\n                img[i, j] = 0\n            else:\n                img[i, j] = 255\n\n    return img","sha1":"ee1b7f13213d0baa0fcff82faab917f0353268e2","id":51764}
{"content":"def recombine_named_key_value(name_val, obj_val, name_key='name'):\n    \"\"\"\n    Take a name-keyed dict for readability purposes and put the name back inside the dict.\n    :param name_val: The value of the 'name' field\n    :param obj_val: Rest of the object\n    :param name_key: key label for name object.\n    :return: recombined Dict.\n    \"\"\"\n    recombined_dict = {}\n    recombined_dict.update(obj_val)\n    recombined_dict[name_key] = name_val\n\n    return recombined_dict","sha1":"2592b6b5c6a1cac78576091b4d83461d7e982a80","id":384283}
{"content":"from pathlib import Path\n\n\ndef _create_file(file_path: str) -> Path:\n    \"\"\"Create the report directory if it does not exists.\n\n    Args:\n        file_path (str): the path of the file to be created.\n\n    Returns:\n        (pathlib.Path): the created file path.\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    return path","sha1":"3e5c1f2bebdac04d452de83d0271913404e37b21","id":262991}
{"content":"from typing import OrderedDict\n\n\ndef dictify(od):\n    \"\"\"Recursively replace OrderedDict with dict\"\"\"\n    if isinstance(od, OrderedDict):\n        return dict((k, dictify(v)) for k, v in od.items())\n    else:\n        return od","sha1":"886fc727dba02f9029945d5efa947d7573c3df19","id":624913}
{"content":"import itertools\n\n\ndef get_group_i(cubes: list) -> list:\n    \"\"\"Given a list of CCDIDs, return a list of lists where each list contains\n    continuous CCD number range indexes that have good statistics.\"\"\"\n    cubes.sort()\n    good_indexes = list()\n    for i, c in enumerate(cubes):\n        if c.rstats is not None:\n            if i + 1 < len(cubes):\n                if cubes[i + 1].lstats is not None:\n                    good_indexes.append(int(c.ccdnumber))\n            else:\n                good_indexes.append(int(c.ccdnumber))\n\n    good_indexes.sort()\n    # print(f'good indexes: {good_indexes}')\n    pairs = list()\n    for offset, group in itertools.groupby(\n        good_indexes, lambda x, c=itertools.count(): next(c) - x\n    ):\n        pairs.append((offset, list(group)))\n    return pairs","sha1":"42743acb33ddbea0d878076e34db5373f22fceff","id":150222}
{"content":"def quadratic_ease_out(p):\n    \"\"\"Modeled after the parabola y = -x^2 + 2x\"\"\"\n    return -(p * (p - 2))","sha1":"2ae9e45516659e0f24207247dc5aea57788c1bf1","id":561879}
{"content":"import hashlib\n\n\ndef get_subscriber_hash(member_email):\n    \"\"\"\n    The MD5 hash of the lowercase version\n    of the list member's email.\n    Uses as memeber_id\n    \"\"\"\n    member_email = member_email.lower()\n    m = hashlib.md5(member_email)\n    return m.hexdigest()","sha1":"c2d7c7f4d0da58ec3990cccad865cb53cf2ebe15","id":698945}
{"content":"def make_stat_flag_data(cls_dt):\n    \"\"\"\n    \u5982\u679c\u9500\u6237\u65e5\u671f\u4e3a\u771f\uff0c\u8bbe\u7f6e\u5ba2\u6237\u72b6\u6001\u4e3a\u5173\u95ed\uff0c\n    n-\u6b63\u5e38\uff0c c-\u5173\u95ed\n    :param cls_dt: \u9500\u6237\u65e5\u671f\n    :return:\u5ba2\u6237\u72b6\u6001\u6807\u8bc6\n    \"\"\"\n\n    if cls_dt:\n        stat_flag = \"c\"\n    else:\n        stat_flag = \"n\"\n    return stat_flag\n\n    # return 'n'  # \u9ed8\u8ba4\u6b63\u5e38","sha1":"891fe5b340488cb8ef147647df8ecf44f8f7a498","id":118157}
{"content":"from typing import List\n\n\ndef group_consecutives(vals: List[float], step=1):\n    \"\"\"\n    Function to group all consecutive values in a list together. The primary purpose of this\n    is to split concatenated spectra that are given in a single list of frequencies\n    into individual windows.\n    \n    Parameters\n    ----------\n    vals : list\n        List of floats to be split\n    step : int, optional\n        [description], by default 1\n    \n    Returns\n    -------\n    [type]\n        [description]\n    \"\"\"\n    run = []\n    result = [run]\n    expect = None\n    for v in vals:\n        if (v == expect) or (expect is None):\n            run.append(v)\n        else:\n            run = [v]\n            result.append(run)\n        expect = v + step\n    return result","sha1":"6b308dffa2600f288e3f41473955f41ee14b182c","id":223152}
{"content":"def get_comma_separated_values(values):\n    \"\"\"Return the values as a comma-separated string\"\"\"\n\n    # Make sure values is a list or tuple\n    if not isinstance(values, list) and not isinstance(values, tuple):\n        values = [values]\n\n    return ','.join(values)","sha1":"a2c7148f4cd67636bf5f918a3bcbd5a1e95d8942","id":566609}
{"content":"import ipaddress\nimport logging\n\n\ndef validate_network_cidr(network_cidr):\n    \"\"\"\n    validate ip address with cidr format. defaults to \/24 if only IP is given\n\n    \"\"\"\n    compressed_network_cidr = ''\n    while True:\n        try:\n            compressed_network_cidr = ipaddress.ip_network(network_cidr)\n            break\n        except ValueError:\n            logging.warn('input should be in format x.x.x.x\/x')\n            network_cidr = input('enter the network cidr: ')\n\n    return compressed_network_cidr.compressed","sha1":"6e81f1cd55ca837966e052ed18d169649c83f8e4","id":335148}
{"content":"def width(geom):\n    \"\"\"This functions computes the width of a geometry.\n\n    :param geom: Polygon geometry\n    :type geom: shapely.geometry.Polygon\n\n    :returns width: Polygon width.\n    \"\"\"\n\n    minx, miny, maxx, maxy = geom.bounds\n\n    return maxx - minx","sha1":"981fd4a50321c143c92761e7c5dbf0fd7b3632c3","id":529283}
{"content":"def convert(item, cls, default=None):\n    \"\"\"\n    Convert an argument to a new type unless it is `None`.\n    \"\"\"\n    return default if item is None else cls(item)","sha1":"447eb82558c4a4135322633d8e6ec35afb7dbe64","id":506167}
{"content":"def snowmelt_temperature_index(temperatures,\n                               temperature_cutoff=32.0,\n                               melt_rate_coeff=0.06):\n    \"\"\"Calculate the amount of snowmelt using a temperature index method,\n    also called degree-day method. This method has its limitations as noted\n    in the reference.\n\n    :param temperatures: Temperatures, in degrees Fahrenheit\n    :type temperatures: numpy.ndarray\n    :param temperature_cutoff: Temperature when melt begins,\n                               in degrees Fahrenheit\n    :type temperature_cutoff: float\n    :param melt_rate_coeff: Snowmelt rate coefficient (often variable),\n                            in inches per degree Fahrenheit\n    :type melt_rate_coeff: float\n    :return snowmelt: Snowmelt values, in inches per day\n    :rtype snowmelt: numpy.ndarray\n\n    .. note::\n\n    Equation:\n\n    M = C_m * (T_a - T_b)\n\n    where\n\n    M           snowmelt, inches per period\n    C_m         melt-rate coefficient, inches\/(degree Fahrenheit\/period)\n    T_a         air temperature, degrees Fahrenheit\n    T_b         base temperature, degrees Fahrenheit\n\n    Reference:\n\n    Engineering and Design - Runoff from Snowmelt\n    U.S. Army Corps of Engineers\n    Engineering Manual 1110-2-1406\n    Chapter 6-1. Generalized Equations, Rain-on-Snow Situations, Equation 6-1\n    https:\/\/www.wcc.nrcs.usda.gov\/ftpref\/wntsc\/H&H\/snow\/COEemSnowmeltRunoff.pdf\n    \"\"\"\n    return melt_rate_coeff * (temperatures - temperature_cutoff)","sha1":"f283487fc54700c8b3da592f0b488bdc42856e93","id":607244}
{"content":"def check_diameters(vessel, new_ID, new_OD):\n    \"\"\" Change the vessel's ID and OD and return the resulting safety factor.\n    \"\"\"\n    if new_OD <= new_ID:\n        # OD must be larger than ID by any finite amount\n        # Otherwise, don't try calculating, may cause error\n        return 0\n    else:\n        vessel.modify_parameters(ID=new_ID, OD=new_OD)\n        return vessel.SF","sha1":"733d82cfc18d35ff5467351bab4297575da988b5","id":482718}
{"content":"def in_order_path(tsp):\n    \"\"\"Return the tour [1,2,...,n,1] where n is the dimension of the TSP\"\"\"\n    dim = tsp[\"DIMENSION\"]\n    return list(range(1, dim + 1))","sha1":"43f5be498cb23b9717bb55476b0317412cd63bec","id":458787}
{"content":"def expand_contractions(text, contractions):\n    \"\"\"\n    Method used to expand the contractions from the text\"\n\n    Parameters:\n    -----------------\n        text (string): Text to clean\n        contractions (dict): Dictionary of contractions with expansion\n                             for each contraction\n\n    Returns:\n    -----------------\n        text (string): Text after transforming the contractions.\n\n    \"\"\"\n\n    # Tokenizing text into tokens.\n    tokens = text.split(\" \")\n\n    for token in tokens:\n\n        # Checking whether token is in contractions as a key\n        if token in contractions:\n\n            # Token is replace if is in dictionary and tokens\n            tokens = [item.replace(token, contractions[token])\n                      for item in tokens]\n\n    # Transforming from list to string\n    text = \" \".join(str(token) for token in tokens)\n\n    return text","sha1":"36d0737c3688c1226e577f661ae9cc09db4ebfa0","id":323545}
{"content":"def get_boolean(prompt):\n    \"\"\"Returns a boolean for a yes or no question.\"\"\"\n    response = input(prompt)\n    response = response.lower()\n    if response in ['yes', 'y', 'yep', 'sure', 'of course']:\n        boolean = True\n    elif response in ['no', 'nope', 'n', 'absolutely not']:\n        boolean = False\n    else:\n        prompt = \"Do you mean yes? \"\n        boolean = get_boolean(prompt)\n    return boolean","sha1":"532614fcf2e576392e09a52eb3f4b063283472cb","id":171279}
{"content":"def _decoder(mocker):\n    \"\"\"Fixture providing decoder mock.\"\"\"\n    return mocker.Mock()","sha1":"fbd2f178f0851705e2b85be9c2ac54ef316cbb87","id":145904}
{"content":"def set_model_permissions(context, token):\n    \"\"\"\n    Assigns a permissions dict to the given model, much like Django\n    does with its dashboard app list.\n\n    Used within the change list for pages, to implement permission\n    checks for the navigation tree.\n    \"\"\"\n    model = context[token.split_contents()[1]]\n    opts = model._meta\n    perm_name = opts.app_label + \".%s_\" + opts.object_name.lower()\n    request = context[\"request\"]\n    setattr(model, \"perms\", {})\n    for perm_type in (\"add\", \"change\", \"delete\"):\n        model.perms[perm_type] = request.user.has_perm(perm_name % perm_type)\n    return \"\"","sha1":"255e603ecbe51c0076f07e399ba07415cd5ad3d2","id":507153}
{"content":"def none_if_na(s):\n    \"\"\"Return None if s == 'N\/A'. Return s otherwise.\"\"\"\n    return None if s == 'N\/A' else s","sha1":"b0788485d3f836c8a1f4c73595b098a6d87860ab","id":617280}
{"content":"def reset_labels(dataset):\n    \"\"\"Set labels to the original labels stored in originalLabel\"\"\"\n    dataset['label'] = dataset['originalLabel']\n    return dataset","sha1":"978678ac1ba52d17a6ef596efe3c1e499d08f0ab","id":410637}
{"content":"import re\n\n\ndef strip_quotations(source_code):\n    \"\"\"\n    Strips quotations from code to prevent false positives due to keywords\n    inside strings.\n\n    First the function matches groups of quotations by looking for text within\n    quotations (not escaped quotations). Then it replaces every group with an\n    empty string.\n\n    :param source_code: the string that contains source code\n    :return: same source code as source_code but without any quoted strings\n    \"\"\"\n\n    # based on regex from\n    # http:\/\/stackoverflow.com\/questions\/171480\/regex-grabbing-values-between-quotation-marks\n    quotation_regex = \"([\\\"'])(?:(?=(\\\\\\\\?))\\\\2.)*?\\\\1\"\n    return re.sub(quotation_regex, \"\", source_code)","sha1":"8ea256a72f95bb588b968eff1f5d44ce6826efb1","id":284312}
{"content":"def relabel_df(df):\n    \"\"\"\n    Modify the column labels to fit into Jekyll. Default to lowercase.\n    \"\"\"\n    df = df.rename(index={'Title': 'title',\n                          'Teaser text': 'excerpt',\n                          'Description': 'description',\n                          'Example projects': 'projects',\n                          'Tags': 'tags',\n                          'Mentors': 'mentors',\n                          'Resources': 'resources',\n                          'Badge': 'badge',\n                          'Level': 'level'\n                          })\n    return df","sha1":"8325c86b13021539fe468fe045018f813721d101","id":382992}
{"content":"def graph6n(data):\n\t\"\"\"Read initial one or four-unit value from graph6 sequence.  Return value, rest of seq.\"\"\"\n\tif data[0] <= 62:\n\t\treturn data[0], data[1:]\n\treturn (data[1]<<12) + (data[2]<<6) + data[3], data[4:]","sha1":"799a08bd4728674247c26031dee13e77fa9f4e9c","id":690234}
{"content":"import struct\n\n\ndef get_dword_at(li, offset):\n    \"\"\"\n    Get dword at specified offset\n\n    :param li: Loader input\n    :param offset: Offset\n    \"\"\"\n\n    li.seek(offset)\n    return struct.unpack('>I', li.read(4))[0]","sha1":"4e7a8fe947088f133521197cc4b4820afc608a00","id":225573}
{"content":"from typing import Type\n\n\ndef is_dict_specifier(value):\n    # type: (object) -> bool\n    \"\"\" Check if value is a supported dictionary.\n    Check if a parameter of the task decorator is a dictionary that specifies\n    at least Type (and therefore can include things like Prefix, see binary\n    decorator test for some examples).\n\n    :param value: Decorator value to check.\n    :return: True if value is a dictionary that specifies at least the Type of\n             the key.\n    \"\"\"\n    return isinstance(value, dict) and Type in value","sha1":"e18ad83a1b79a8150dfda1c65f4ab7e72cc8c8c8","id":709169}
{"content":"from pathlib import Path\n\n\ndef requirements_path() -> Path:\n    \"\"\" Return the absolute Path to 'tests\/requirements' \"\"\"\n    return Path(__file__).parent \/ Path('requirements')","sha1":"d20bf1c50fe0881238435641f782a3975a82c1f7","id":366614}
{"content":"def convert_index_2_bool(index):\n    \"\"\"\n    Convert integer style outlier \/ inlier index to boolean.\n\n    Inputs:\n        index: (list) -1 for outliers and 1 for inliers.\n\n    Returns:\n        (list) False for outliers and True for inliers.\n    \"\"\"\n    return [True if i == 1 else False for i in index]","sha1":"bcc1e6bed141ae9141b601a04210abb518e020ab","id":227360}
{"content":"import re\n\n\ndef split_atom_label(label):\n    \"\"\"\n    Split atomic label used in ref or cif formats into computer-readable parts.\n    :param label: Short atomic label such as \"C12A\" or \"Fe1\".\n    :type label: str\n    :return: 3-element tuple with element, number, and suffix, as str.\n    :rtype: tuple\n    \"\"\"\n    label_regex = re.compile(r'(\\d+|\\s+)')\n    label_fragments = label_regex.split(label)\n    element = label_fragments[0]\n    number = label_fragments[1]\n    suffix = ''.join(label_fragments[2:])\n    return element, number, suffix","sha1":"49b1e127c5ca84405be4783a207611774fcceef1","id":199942}
{"content":"def closed_neighborhood(graph, v):\n    \"\"\"The closed neighborhood of a vertex in a graph\n\n    Args:\n      graph (networkx.classes.graph.Graph): graph\n      v: vertex in a graph\n\n    Returns:\n      The neighbors of v in graph, as a set.\n\n    Example:\n      >>> import networkx as nx\n      >>> from pycliques.dominated import closed_neighborhood\n      >>> closed_neighborhood(nx.path_graph(4), 0)\n      {0, 1}\n\n    \"\"\"\n    return set(graph[v]) | {v}","sha1":"451b7c57d339bb9826bb417ee064dc3fb62c0df3","id":664805}
{"content":"def load_poly(path):\n    \"\"\"\n    Loads the poly.txt file\n\n    :param path: The path of the file to open\n\n    :returns : A list of every line in the file\n    \"\"\"\n    with open(path, \"r\") as f:\n        data = f.readlines()\n    return [x.strip() for x in data]","sha1":"80ee49f54d74fc1c6779736fa4df6a20d1059675","id":361238}
{"content":"def get_file_dir(location):\n    \"\"\"Returns the directory of the file with the file name\n    \n    Keyword arguments:\n        location -- A file path.\n    \"\"\"\n    return location.rpartition('\\\\')","sha1":"7697fd7bc0b033c83dda1fe721fe8711bd407058","id":357302}
{"content":"def get_report(analytics, VIEW_ID):\n    \"\"\"Queries the Analytics Reporting API V4.\n    Args: analytics: An authorized Analytics Reporting API V4 service object.\n    Returns: The Analytics Reporting API V4 response.\n    \"\"\"\n    return analytics.reports().batchGet(\n        body={\n            'reportRequests': [\n                {\n                    'viewId': VIEW_ID,\n                    'dateRanges': [{'startDate': '7daysAgo', 'endDate': 'today'}],\n                    'metrics': [{'expression': 'ga:sessions'}],\n                    'dimensions': [{'name': 'ga:country'}]\n                }]\n        }\n    ).execute()","sha1":"817d28e1a9fa842a254d544e6554613ecab517dd","id":611240}
{"content":"def serialize_period(period, end_amounts=None):\n    \"\"\"Serialize a Period object for JSON encoding.\n\n    end_amounts is an optional dict of {'circ', 'surplus'} containing\n    the computed end_circ and end_surplus values. If end_amounts is not\n    provided and the end amounts are stored as null, the end amounts\n    will be None.\n    \"\"\"\n    end_circ = period.end_circ\n    if end_circ is None:\n        if end_amounts:\n            end_circ = end_amounts.get('circ')\n\n    end_surplus = period.end_surplus\n    if end_surplus is None:\n        if end_amounts:\n            end_surplus = end_amounts.get('surplus')\n\n    if end_circ is not None and end_surplus is not None:\n        end_combined = end_circ + end_surplus\n    else:\n        end_combined = None\n\n    return {\n        'id': str(period.id),\n        'owner_id': period.owner_id,\n        'file_id': str(period.file_id),\n        'start_date': period.start_date,\n        'end_date': period.end_date,\n        'start_circ': period.start_circ,\n        'end_circ': end_circ,\n        'start_surplus': period.start_surplus,\n        'end_surplus': end_surplus,\n        'start_combined': period.start_circ + period.start_surplus,\n        'end_combined': end_combined,\n        'closed': period.closed,\n    }","sha1":"c95d7399c690639bdd06bf00dbdc171801bd7d90","id":270312}
{"content":"from typing import List\n\n\ndef count_total_keywords(keywords: List[str]) -> int:\n    \"\"\"\n    Returns the count of total keywords of a list of keywords as a integer.\n        Parameters:\n            keywords (List[str]): list with all keywords as strings.\n    \"\"\"\n    return len(keywords)","sha1":"ba4fff81fddd82487aca121f458d68d3d793af9a","id":667295}
{"content":"def doprefix(site_url):\n    \"\"\"\n    Returns protocol prefix for url if needed.\n    \"\"\"\n    if site_url.startswith(\"http:\/\/\") or site_url.startswith(\"https:\/\/\"):\n        return \"\"\n    return \"http:\/\/\"","sha1":"fc53d4f9f113be79b68becec8b7332c96d779127","id":123624}
{"content":"def _get_camera_translation(camera):\n  \"\"\"Computes the extrinsic translation of the camera.\"\"\"\n  rot_mat = camera.orientation\n  return -camera.position.dot(rot_mat.T)","sha1":"675ab1b39ecf550f8aa9d377872f91526695bd2b","id":557641}
{"content":"import gzip\n\n\ndef open_file_helper(filename, compressed, mode=\"rt\"):\n    \"\"\"\n    Supports reading of gzip compressed or uncompressed file.\n\n    Parameters:\n    ----------\n    filename : str\n        Name of the file to open.\n    compressed : bool\n        If true, treat filename as gzip compressed.\n    mode : str\n        Reading mode.\n\n    Returns:\n    --------\n    file handle to the opened file.\n    \"\"\"\n    return gzip.open(filename, mode=mode) if compressed else open(filename, mode)","sha1":"a682924f68158f88552b96197c43735bb596afaa","id":604130}
{"content":"from typing import List\n\n\ndef change_partition(partition: List[tuple], t: int) -> List[tuple]:\n    \"\"\"\n    Used in algorithm 2.\n    Create a partition from original partition where each 2 ^ t pieces are united.\n    :param partition: The original partition.\n    :param t: Defines the size of the new partition.\n    :return: A partition with pieces with 2 ^ t size.\n\n    >>> change_partition([(0.0, 1.0), (1.0, 2.0)], 1)\n    [(0.0, 2.0)]\n\n    \"\"\"\n\n    ret = []\n    # Go over all the original partitions with 2^t jumps\n    for start in range(0, len(partition) - 2 ** t + 1, 2 ** t):\n        end = start + 2 ** t - 1\n        # Add the new joined partition to the list\n        ret.append((partition[start][0], partition[end][1]))\n    return ret","sha1":"58311f285c445dd09132ef3e660279e91ce94550","id":142037}
{"content":"def get_data_file_args(args, language):\n    \"\"\"\n    For a interface, return the language-specific set of data file arguments\n\n    Args:\n        args (dict): Dictionary of data file arguments for an interface\n        language (str): Language of the testbench\n\n    Returns:\n        dict: Language-specific data file arguments\n    \"\"\"\n    if language in args:\n        return args[language]\n    return args[\"generic\"]","sha1":"11e30b92316bad9a46b87bd9188f97d5e8860377","id":708879}
{"content":"from typing import Any\n\n\ndef get_class_fully_qualified_name(t: Any) -> str:\n    \"\"\"Returns the fully qualified class name of the given class as a string.\"\"\"\n    module = t.__module__\n    if module is None or module == str.__class__.__module__:\n        return t.__qualname__  # type: ignore\n    else:\n        return module + '.' + t.__qualname__","sha1":"8931eab8a05764f294a5b256213dc1e863a02109","id":388626}
{"content":"from typing import Iterable\nfrom typing import Any\nfrom typing import List\n\n\ndef init(it: Iterable[Any]) -> List[Any]:\n    \"\"\"\n    Get all but end of iterable object\n\n    Args:\n        it: Iterable object\n\n    Examples:\n        >>> fpsm.init([1, 2, 3])\n        [1, 2]\n        >>> fpsm,init(range(5))\n        [0, 1, 2, 3]\n    \"\"\"\n    *i, _ = it\n    return i","sha1":"f8fe0a1de2f23e4bcd8f02e81d7449f36c2452bd","id":601852}
{"content":"def topic_exists(client, topic_name):\n    \"\"\"Checks if the given topic exists\"\"\"\n    topic_metadata = client.list_topics(timeout=5)\n    return topic_name in set(t.topic for t in iter(topic_metadata.topics.values()))","sha1":"986c7405c6e6390b8c8af900b03657901b4be90c","id":326997}
{"content":"import requests\n\n\ndef get_latest_ensembl_release() -> str:\n    \"\"\"Return the latest Ensembl release as provided by bioversions.\"\"\"\n\n    url = \"https:\/\/github.com\/biopragmatics\/bioversions\/raw\/main\/src\/bioversions\/resources\/versions.json\"\n    results = requests.get(url).json()\n    versions = {\n        entry[\"prefix\"]: entry[\"releases\"][-1][\"version\"]\n        for entry in results[\"database\"]\n        if \"prefix\" in entry\n    }\n    ensembl_release = versions[\"ensembl\"]\n    assert isinstance(ensembl_release, str)\n    return ensembl_release","sha1":"6963a294fe81968a26272f8eed2b90f625997305","id":112989}
{"content":"def substr_surrounded_by_chars(full_str, char_pair, offset=0):\n    \"\"\"\n    Extract substring surrounded by open & close chars.\n    For example, 'value: { k1: v1, k2: v2 }'\n    :param full_str: target string.\n    :param char_pair: a tuple contains open & close tag.\n    :param offset: Start point to search substring.\n    :return: Substring which include open & close char.\n    \"\"\"\n    open_tag, close_tag = char_pair\n    if len(full_str) <= offset or len(full_str) < 2:\n        raise ValueError('Invalid offset value.')\n    if len(open_tag) != 1 and len(close_tag) != 1:\n        raise ValueError('Invalid close tag characters, open & close tag length must be 1.')\n    stack = []\n    open_index = -1\n    close_index = -1\n    for i in range(offset, len(full_str)):\n        ch = full_str[i]\n        if ch == open_tag:\n            if open_index == -1:\n                open_index = i\n            stack.append(ch)\n        elif ch == close_tag:\n            if len(stack) < 1:\n                raise ValueError(\"Invalid str value, close tag before open tag\")\n            stack.pop()\n            if len(stack) == 0:\n                close_index = i\n                break\n    if len(stack) > 0:\n        raise ValueError('Invalid str value, open tag and close tag are not paired.')\n    return full_str[open_index:close_index + 1]","sha1":"0c204bc5d0fa25536856f2e96b3bf846172d7c13","id":299305}
{"content":"def summate(a,b):\n\t\"\"\"sums together a and b and retuns the answer\"\"\"\n\n\tc=a+b\n\n\treturn c","sha1":"797e16de25121aec4c5bd2e4e2618254bc4eadb0","id":412673}
{"content":"def extract_sequence_accessions_from_seed(seed_file):\n    \"\"\"\n    Parses a seed MSA and extracts all sequence\n    accessions in the form of a dictionary\n\n    seed_file: An Rfam seed alignment\n\n    return: A dictionary of seed accessions\n    \"\"\"\n\n    accessions = {}\n    fp = open(seed_file, 'r')\n\n    for line in fp:\n        line = line.strip()\n        if len(line) > 1 and line[0] != '#' and line != '':\n            line = line.split(' ')\n            accession = line[0].partition('\/')[0]\n            if accession != '':\n                accessions[accession] = \"\"\n\n    fp.close()\n\n    return accessions","sha1":"24cb304921dd1fc1d36c7e283b558fbc5cc55bda","id":379052}
{"content":"def _get_partners(frag_index, part_nodes):\n    \"\"\"\n    Returns the left and right partners of the node whose index is\n    equal to the given frag_index.\n\n    :param frag_index: a fragment index\n    :param part_nodes: a list of primary nodes\n    :returns: [<node-to-left>, <node-to-right>]\n    \"\"\"\n    return [\n        part_nodes[(frag_index - 1) % len(part_nodes)],\n        part_nodes[(frag_index + 1) % len(part_nodes)],\n    ]","sha1":"479df3a30222fa16cd9bf0c99f8d88e05f943560","id":166466}
{"content":"def num_switches(df):\n    \"\"\"Total number of track switches.\"\"\"\n    return df.Type.isin(['SWITCH']).sum()","sha1":"0b3f9864a3d46df615cdecae1ff8251803a7f807","id":399907}
{"content":"def reverse_bits(x):\n    \"\"\"\n    Question 5.3: Reverse the bits in a number\n    \"\"\"\n    MAX_LEN = 64\n    bin_str = '{:08b}'.format(x)\n    remaining = MAX_LEN - len(bin_str)\n    full_str = bin_str[::-1] + ('0' * remaining)\n    return int(full_str, 2)","sha1":"16543e3ce3242953edc44e6ffc6dba50a29e6c6d","id":319077}
{"content":"from typing import Optional\n\n\ndef corner_matcher(entrances_exits: list[bool]) -> Optional[dict]:\n    \"\"\"Check existence and rotation of a corner\"\"\"\n    if entrances_exits.count(True) != 2:\n        return None\n\n    sequence = [True] * 2 + [False] * 2\n    for index in range(4):\n        if sequence == entrances_exits:\n            return {\n                \"type\": \"corner\",\n                \"rotation\": 90 * index\n            }\n        sequence = sequence[-1:] + sequence[:-1]\n\n    return None","sha1":"6d7d205a6285c0ef38dd4f49ccd3e07efc86a5c2","id":169995}
{"content":"def get_stage_suffix(stage):\n    \"\"\"\n    Which suffix to use (i.e. are we packaging a checkpoint?) - maps a stage to a suffix.\n    :param stage:\n    :return:\n    \"\"\"\n    if stage == 'final':\n        return ''\n    else:\n        return \".%s\" % stage","sha1":"e8a8da5ddad06002e4f150388bd7d0269903c13f","id":297290}
{"content":"def construct_absolute_url(url):\n    \"\"\"Turn a relative URL into an absolute URL\"\"\"\n    return \"https:\/\/www.consumerfinance.gov\" + url","sha1":"7bffc3906a6106c24bd93a260ad71d73321ad7ff","id":481247}
{"content":"def process_max_frames_arg(max_frames_arg):\n    \"\"\"Handle maxFrames arg in vidstab.__main__\n\n    Convert negative values to inf\n\n    :param max_frames_arg: maxFrames arg in vidstab.__main__\n    :return: max_frames as is or inf\n\n    >>> process_max_frames_arg(-1)\n    inf\n    >>> process_max_frames_arg(1)\n    1\n    \"\"\"\n    if max_frames_arg > 0:\n        return max_frames_arg\n    return float('inf')","sha1":"8b4756804688516b828fbd082fb2b9be7b6d9f52","id":30998}
{"content":"import random\n\n\ndef successfulStarts(eventProb, numTrials):\n    \"\"\"Assumes eventProb is a float representing a probability\n    of a single attempt being successful. numTrials a positive int\n    Returns a list of the number of attempts needed before a\n    success for each trial.\"\"\"\n    triesBeforeSuccess = []\n    for t in range(numTrials):\n        consecFailures = 0\n        while random.random() > eventProb:\n            consecFailures += 1\n        triesBeforeSuccess.append(consecFailures)\n    return triesBeforeSuccess","sha1":"486dc50d4f9ebf69902cee25d157b3e9a5fc5f0c","id":633309}
{"content":"def I_box(m,w,l):\n    \"\"\"Moment of a box with mass m, width w and length l.\"\"\"\n    return m * (w**2 + l**2) \/ 12","sha1":"60287d37634c185b2e2632e5ce295d1127f3dd21","id":52439}
{"content":"def agent_texts(agents):\n    \"\"\"Return a list of all agent texts from a list of agents.\n\n    None values are associated to agents without agent texts\n\n    Parameters\n    ----------\n    agents : list of :py:class:`indra.statements.Agent`\n\n    Returns\n    -------\n    list of str\/None\n        agent texts from input list of agents\n    \"\"\"\n    return [ag.db_refs.get('TEXT') for ag in agents]","sha1":"e8d791b72a8b64ca43cfa2d5a026104d73cc5ee5","id":536678}
{"content":"def dequantize_output(data, output_binding_info):\n    \"\"\"Dequantize the (u)int8 output to float\"\"\"\n\n    if output_binding_info[1].IsQuantized():\n        if data.ndim != 2:\n            raise RuntimeError(\"Data must have 2 dimensions for quantization\")\n\n        quant_scale = output_binding_info[1].GetQuantizationScale()\n        quant_offset = output_binding_info[1].GetQuantizationOffset()\n\n        data = data.astype(float)\n        for row in range(data.shape[0]):\n            for col in range(data.shape[1]):\n                data[row, col] = (data[row, col] - quant_offset)*quant_scale\n    return data","sha1":"3a1d6308a007a5f53f864263c316740ab84ab1ab","id":532498}
{"content":"import six\n\n\ndef merge_dict(dict1, dict2):\n  \"\"\"Merges :obj:`dict2` into :obj:`dict1`.\n\n  Args:\n    dict1: The base dictionary.\n    dict2: The dictionary to merge.\n\n  Returns:\n    The merged dictionary :obj:`dict1`.\n  \"\"\"\n  for key, value in six.iteritems(dict2):\n    if isinstance(value, dict):\n      dict1[key] = merge_dict(dict1.get(key, {}), value)\n    else:\n      dict1[key] = value\n  return dict1","sha1":"f587e3690f557967460cd8cf8fc1fa06a0159479","id":584010}
{"content":"import string\n\n\ndef build_abbreviation(agency_name):\n    \"\"\" Given an agency name, guess at an abbrevation. \"\"\"\n    abbreviation = ''\n    for ch in agency_name:\n        if ch in string.ascii_uppercase:\n            abbreviation += ch\n    return abbreviation","sha1":"570a31f25543e3657c3b5afc007458b4d23b8edc","id":421641}
{"content":"def encode_boolean_setting(value):\n\t\"\"\"\n\tEncodes a boolean to the  string \"True\" or \"False\".\n\t\"\"\"\n\tif value:\n\t\treturn \"True\"\n\telse:\n\t\treturn \"False\"","sha1":"a71b5f23cd6d337a9f6e609c5c884b9cc9295b50","id":137229}
{"content":"def _find_union(lcs_list):\n  \"\"\"Finds union LCS given a list of LCS.\"\"\"\n  return sorted(list(set().union(*lcs_list)))","sha1":"f6c2df0092afc0c427fa1de70c032764f4692394","id":431221}
{"content":"from typing import Optional\nimport logging\n\n\ndef check_termination_conditions(\n        delta: float,\n        theta: Optional[float],\n        iterations_finished: int,\n        num_iterations: Optional[int]\n) -> bool:\n    \"\"\"\n    Check for termination.\n\n    :param delta: Delta.\n    :param theta: Theta.\n    :param iterations_finished: Number of iterations that have been finished.\n    :param num_iterations: Maximum number of iterations.\n    :return: True for termination.\n    \"\"\"\n\n    below_theta = theta is not None and delta < theta\n\n    completed_num_iterations = False\n    if num_iterations is not None:\n        completed_num_iterations = iterations_finished >= num_iterations\n        num_iterations_per_print = int(num_iterations * 0.05)\n        if num_iterations_per_print > 0 and iterations_finished % num_iterations_per_print == 0:\n            logging.info(f'Finished {iterations_finished} iterations:  delta={delta}')\n\n    if below_theta or completed_num_iterations:\n        logging.info(f'Evaluation completed:  iterations={iterations_finished}, delta={delta}')\n        return True\n    else:\n        return False","sha1":"b0d16b37e240369bacb0f4f2592e607ba8e3cd6c","id":437352}
{"content":"def unwrap_if_scalar(obj):\n    \"\"\"\n    Unwraps obj if it is a sequence with a single item. Returns obj[0] if\n    len(obj) == 1 and obj otherwise.\n    \"\"\"\n    if len(obj) == 1:\n        return obj[0]\n    else:\n        return obj","sha1":"749fd6efd77030ff32ccc32056eb95f0bb717cd3","id":274342}
{"content":"import pickle\n\n\ndef open_pickle_file(fname):\n    \"\"\"Return the contents of a pickle file.\"\"\"\n    with open(fname, 'rb') as fh:\n        return pickle.load(fh)","sha1":"b52b5cea794af6c4836f3f8fda097a06829310ea","id":270942}
{"content":"def get_file_paths(release, taskid, cbs, tabs):\n    \"\"\"\n    Construct path to files in ALTA\n\n    :param str release: Data release (SVC or internal)\n    :param str taskid: Observation taskid\n    :param iterator cbs: compound beams to download\n    :param iterator tabs: tied-array beams to download of each compound beam\n    :return: list of file paths\n    \"\"\"\n    # set prefix\n    if release == \"internal\":\n        prefix = \"\/altaZone\/archive\/arts_main\/arts_sc4\"\n    elif release == \"SVC\":\n        prefix = \"https:\/\/alta.astron.nl\/webdav\/SVC_2019_TimeDomain\"\n    else:\n        raise ValueError(f\"Unknown release: {release}; valid options are SVC, internal\")\n\n    # construct full path for each CB and TAB\n    files = [f\"{prefix}\/{taskid}\/CB{cb:02d}\/ARTS{taskid}_CB{cb:02d}_TAB{tab:02d}.fits\" for cb in cbs for tab in tabs]\n    return files","sha1":"ce2463f662158a00c8db382a577bdbdd011e1429","id":219113}
{"content":"def read_file(filename):\n    \"\"\"\n\n    read the text of a file\n\n    :param filename: name of the file to read\n    :return: text of the file\n    \"\"\"\n\n    with open(filename, encoding=\"utf-8\") as file:\n        return file.read()","sha1":"0df9e7f7df0cbf8896b62502efb0023f8c30f2b0","id":428966}
{"content":"def reduce_loss(loss, reduction):\n    \"\"\"Reduce loss as specified.\n\n    Args:\n        loss (Tensor): Elementwise loss tensor.\n        reduction (str): Options are \"none\", \"mean\", \"valid_mean\" and \"sum\".\n\n    Return:\n        Tensor: Reduced loss tensor.\n    \"\"\"\n\n    if reduction == 'mean':\n        return loss.mean()\n    elif reduction == 'valid_mean':\n        valid_mask = loss > 0.0\n        num_valid = valid_mask.sum().float().clamp_min(1.0)\n        return loss.sum() \/ num_valid\n    elif reduction == 'sum':\n        return loss.sum()\n    else:\n        return loss","sha1":"8468ad46c2e9d89dc01e672473a48dfcd5a19f9c","id":594442}
{"content":"def decryptFast(msg, cryptor):\n    \"\"\"Decrypts message with an existing `.pyelliptic.ECC` object\"\"\"\n    return cryptor.decrypt(msg)","sha1":"3ac5620495ee9c2c6ee37cf96aabbf36a611084a","id":241448}
{"content":"import requests\n\n\ndef query(data_type, uid):\n    \"\"\"\n    :param data_type: the Pharos data type (e.g. ligand, target ..)\n    :param uid: Pharos concept identifier\n    :return: JSON containing the query results\n    \"\"\"\n    url = \"https:\/\/pharos.nih.gov\/idg\/api\/v1\/{}({})?view=full\".format(data_type, uid)\n    try:\n        resp = requests.get(url).json()\n    except:\n        resp = None\n    return resp","sha1":"3ef9bb23e4f330e4dc2c348ebfeb9d59e21d7005","id":183900}
{"content":"def add_prefix(dict_like, prefix):\n    \"\"\"\n    takes a dict (or dict-like object, e.g. etree._Attrib) and adds the\n    given prefix to each key. Always returns a dict (via a typecast).\n\n    Parameters\n    ----------\n    dict_like : dict (or similar)\n        a dictionary or a container that implements .items()\n    prefix : str\n        the prefix string to be prepended to each key in the input dict\n\n    Returns\n    -------\n    prefixed_dict : dict\n        A dict, in which each key begins with the given prefix.\n    \"\"\"\n    if not isinstance(dict_like, dict):\n        try:\n            dict_like = dict(dict_like)\n        except Exception as e:\n            raise ValueError(\"{0}\\nCan't convert container to dict: \"\n                             \"{1}\".format(e, dict_like))\n    return {prefix + k: v for (k, v) in dict_like.items()}","sha1":"634fc88346fe7da94090d16f4f01265a248ff56c","id":102209}
{"content":"from typing import List\nimport math\n\n\ndef get_minimum_subintervals_set_math(intvl_upper_limit: int, range_length: int = 1) -> List[int]:\n    \"\"\"\n    Method that returns the minimal set of placements that cover the full interval. This method uses\n    the mathematical expresion that minimizes the number of points, however it can go out of the range,\n    so the last point is set to the upper bound when this situation happens.\n\n    :param intvl_upper_limit: upper bound of the interval\n    :param range_length: range of the sub-interval from the placement points\n    :return: list with the position\n    \"\"\"\n    if intvl_upper_limit == 0:\n        raise ValueError('The upper limit of the interval has to be bigger than 0')\n    number_of_subintervals = math.ceil(intvl_upper_limit \/ (2 * range_length))\n    positions = []\n    for i in range(number_of_subintervals):\n        if i == 0:\n            positions.append(range_length)\n        else:\n            position = range_length + 2 ** i * range_length\n            positions.append(position if position <= intvl_upper_limit else intvl_upper_limit)\n    return positions","sha1":"e4329c35eb93281e14b082f99ffb9d5ce71e6cdb","id":634946}
{"content":"def sort_by_absolute_val(df, column):\n    \"\"\"Sort df column by descending order in terms of the absolute value.\"\"\"\n    df = df.reindex(df[column]\n                    .abs()\n                    .sort_values(ascending=False)\n                    .index)\n    return df","sha1":"bf9fdba373cd93c3b385154710b9b310e2a26ac8","id":15166}
{"content":"def any_heterozygous(genotypes):\n    \"\"\"Determine if any of the genotypes are heterozygous\n    \n    Parameters\n    ----------\n    genoytypes : container\n        Genotype for each sample of the variant being considered\n    \n    Returns\n    -------\n    bool\n        True if at least one sample is heterozygous at the variant being\n        considered, otherwise False\n    \"\"\"\n    \n    return bool(\n        {'0|1', '1|0', '0\/1'}\n        & {genotype.split(':')[0] for genotype in genotypes}\n    )","sha1":"1ed57835f84ebf8f609a6addba5b922003a5588e","id":673594}
{"content":"def create_choice(name, value) -> dict:\n    \"\"\"A function that will create a choice for a :class:`~SlashOption`\n    \n    Parameters\n    ----------\n        name: :class:`str`\n            The name of the choice\n        value: :class:`Any`\n            The value that will be received when the user selected this choice\n    \n    Returns\n    -------\n        :returns: The created choice\n        :type: :class:`dict`\n    \n    \"\"\"\n    return {\"name\": name, \"value\": value}","sha1":"26deff75b094d4d8add261bbac957b7486f3a93c","id":419934}
{"content":"def gcd(x: int, y: int) -> int:\n    \"\"\"Returns the largest positive integer that divides non-zero integers x and y.\"\"\"\n    assert x != 0 and y != 0 and type(x) == int and type(y) == int, \"Non-zero integers only.\"\n    if x < 0:\n        x *= -1\n    if y < 0:\n        y *= -1\n    # Euclid's algorithm\n    # assign the larger number to x, so can do larger % smaller\n    if y > x:\n        x, y = y, x\n    remainder = x % y\n    if not remainder:\n        return y\n    return gcd(y, remainder)","sha1":"c9529cddd4b6a88cd4d03c105496318c90481511","id":134430}
{"content":"def is_ipv4_address(addr):\n    \"\"\"Return True if addr is a valid IPv4 address, False otherwise.\"\"\"\n    if (not isinstance(addr, str)) or (not addr) or addr.isspace():\n        return False\n    octets = addr.split('.')\n    if len(octets) != 4:\n        return False\n    for octet in octets:\n        if not octet.isdigit():\n            return False\n        octet_value = int(octet)\n        if octet_value < 0 or octet_value > 255:\n            return False\n    return True","sha1":"2f7127019bcc069462ef85e5d15e5fb3a44add8b","id":653356}
{"content":"def blend_union(da, db, r):\n    \"\"\" Blend union of the distances da, db with blend radius r. \"\"\"\n    e = max(r - abs(da - db), 0)\n    return min(da, db) - e * e * 0.25 \/ r","sha1":"7128b3a78295da0a744046a0b76e466afbcf322d","id":625894}
{"content":"def nonempty(str):\n    \"\"\"Any text. Returns '(none)' if the string is empty.\"\"\"\n    return str or \"(none)\"","sha1":"1eea2751c58422ba14eee6ee6105e85a3b499fb2","id":462921}
{"content":"def get_neighbours(cell, h, w):\n    \"\"\"\n    Get the adjacent cells\n    @param cell: the cell for which to retrieve the neighbours\n    @type cell: list\n    @param h: the height of the map\n    @type h: int\n    @param w: the width of the map\n    @type w: int\n    @return: list of the neighbours' coordinates\n    @rtype: list\n    \"\"\"\n    x = cell[0]\n    y = cell[1]\n    north = south = east = west = None\n    if y + 1 < h:\n        north = (x, y + 1)\n    if y - 1 > 0:\n        south = (x, y - 1)\n    if x + 1 < w:\n        east = (x + 1, y)\n    if x - 1 > 0:\n        west = (x - 1, y)\n\n    return [north, south, east, west]","sha1":"96b1080c2207e8be449739c66d53ce1b6978eebe","id":494122}
{"content":"def _find_depth(node, current_depth):\n    \"\"\"\n    Recursive function traversing a tree and returning the maximum depth.\n    \"\"\"\n    if node.left == -1 and node.right == -1:\n        return current_depth + 1\n    elif node.left != -1 and node.right == -1:\n        return _find_depth(node.l, current_depth + 1)\n    elif node.right != -1 and node.left == -1:\n        return _find_depth(node.r, current_depth + 1)\n    elif node.right != -1 and node.left != -1:\n        return max(_find_depth(node.left, current_depth + 1), _find_depth(node.right, current_depth + 1))","sha1":"1635223fafb69961185adcfc606ae4e16813263e","id":388298}
{"content":"def is_video_path(path):\n    \"\"\"Checks if path has a video extension and is a file.\"\"\"\n    vid_exts = (\".mp4\", \".avi\")\n    return path.suffix.casefold() in vid_exts and path.is_file()","sha1":"915e25c0f37025c7e80f255f9254b89023757d7f","id":566356}
{"content":"def on(id, event):\n    \"\"\"\n    Sets the decorated method to handle indicated event::\n\n        @plugin\n        class Hosts (SectionPlugin):\n            def init(self):\n                self.append(self.ui.inflate('hosts:main'))\n                ...\n\n            @on('save', 'click')\n            def save(self):\n                self.config.save()\n\n    :param id: element ID\n    :type  id: str\n    :param event: event name\n    :type  event: str\n    :rtype: function\n    \"\"\"\n\n    def decorator(fx):\n        fx._event_id = id\n        fx._event_name = event\n        return fx\n    return decorator","sha1":"bf1253864907eaf4b4cfb2b45c0353e7e1cdb5fa","id":473350}
{"content":"def never(_):\n  \"\"\"Predicate function that returns ``False`` always.\n\n  :param _:\n      Argument\n  :returns:\n      ``False``.\n  \"\"\"\n  return False","sha1":"38a949525bb45b556eb5131bd9ff3d11b83a616e","id":214375}
{"content":"def make_ordinal(num):\n    \"\"\"\n    Create an ordinal (1st, 2nd, etc.) from a number.\n    \"\"\"\n    base = num % 10\n    if base in [0,4,5,6,7,8,9] or num in [11,12,13]:\n        ext = \"th\"\n    elif base == 1:\n        ext = \"st\"\n    elif base == 2:\n        ext = \"nd\"\n    else:\n        ext = \"rd\"\n    return str(num) + ext","sha1":"d92069f2d1a88adeb1c72e7551adc75af84beb31","id":89183}
{"content":"def combo_to_int(value, replace_string='None', replace_with=0):\n    \"\"\"\n    Used for the values of combo quants with numerical values and one string value.\n    The default use case is to replace the 'None' option with a 0.\n    \"\"\"\n    if value == replace_string:\n        return int(replace_with)\n    else:\n        return int(value)","sha1":"7a8b49062a015689c67e83bc00ea9da6da0eeffb","id":602625}
